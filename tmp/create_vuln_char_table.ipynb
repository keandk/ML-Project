{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Table Vulnerablity Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LANGUAGE = \"cpp\"\n",
    "ROOT_DIR = f\"data_{LANGUAGE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nodes_by_line(dot_path, target_line):\n",
    "    nodes = []\n",
    "    with open(dot_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if f'LINE_NUMBER=\"{target_line}\"' in line:\n",
    "                # Extract node information here\n",
    "                label_code_match = re.search(r'\"(\\d+)\" \\[label=\"(.*?)\".*?CODE=\"(.*?)\"', line)\n",
    "                if label_code_match:\n",
    "                    node_id, label, code = label_code_match.groups()\n",
    "                    method_match = re.search(r'NAME=\"(.*?)\"', line)\n",
    "                    if method_match:\n",
    "                        method_name = method_match.group(1).split(\".\")[-1]\n",
    "                    else:\n",
    "                        method_match = re.search(r'METHOD_FULL_NAME=\"(.*?)\"', line) \n",
    "                        if method_match:\n",
    "                            method_name = method_match.group(1).split(\".\")[-1]\n",
    "                        else:\n",
    "                            method_name = None\n",
    "                    \n",
    "                    nodes.append({\n",
    "                        'id': node_id,\n",
    "                        'label': label,\n",
    "                        'code': code,\n",
    "                        'method_name': method_name\n",
    "                    })\n",
    "    return nodes\n",
    "\n",
    "def load_vulnerable_lines(sarif_path):\n",
    "    with open(sarif_path, 'r', encoding='utf-8') as f:\n",
    "        sarif = json.load(f)\n",
    "\n",
    "    vulnerable_lines = set()\n",
    "    for run in sarif.get('runs', []):\n",
    "        for result in run.get('results', []):\n",
    "            for loc in result.get('locations', []):\n",
    "                region = loc.get('physicalLocation', {}).get('region', {})\n",
    "                start_line = region.get('startLine')\n",
    "                end_line = region.get('endLine', start_line)  # if no endLine, only 1 line\n",
    "                if start_line:\n",
    "                    for line in range(start_line, end_line + 1):\n",
    "                        vulnerable_lines.add(line)\n",
    "    return list(vulnerable_lines)\n",
    "\n",
    "def analyze_project(project_name):\n",
    "    # print(f\"Analyzing {project_name}...\")\n",
    "    dot_path = os.path.join(ROOT_DIR, \"cpg-output\", project_name, \"export.dot\")\n",
    "    sarif_path = os.path.join(ROOT_DIR, \"unzips\", project_name, \"manifest.sarif\")\n",
    "\n",
    "    if not os.path.exists(dot_path) or not os.path.exists(sarif_path):\n",
    "        print(f\"Missing files for {project_name}\")\n",
    "        return []\n",
    "\n",
    "    vulnerable_lines = load_vulnerable_lines(sarif_path)\n",
    "\n",
    "    results = []\n",
    "    for line_num in vulnerable_lines:\n",
    "        nodes = find_nodes_by_line(dot_path, line_num)\n",
    "        for node in nodes:\n",
    "            results.append({\n",
    "                # \"project\": project_name,\n",
    "                \"line\": line_num,\n",
    "                \"node_label\": node['label'],\n",
    "                \"node_code\": node['code'],\n",
    "                \"method_name\": node['method_name']\n",
    "            })\n",
    "    # print(results)\n",
    "    # exit()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table():\n",
    "    all_nodes = []\n",
    "\n",
    "    project_names = [p for p in os.listdir(os.path.join(ROOT_DIR, \"unzips\")) if p.endswith(\"-mixed\")] or p.endswith(\"-bad\")\n",
    "    # project_names = project_names[:100]  # Test nhỏ nếu muốn\n",
    "\n",
    "    # Use ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Submit all tasks first\n",
    "        future_to_project = {executor.submit(analyze_project, project_name): project_name \n",
    "            for project_name in project_names}\n",
    "        \n",
    "        # Process completed tasks with progress bar\n",
    "        for future in tqdm(as_completed(future_to_project), total=len(project_names), desc=\"Processing projects\"):\n",
    "            project_name = future_to_project[future]\n",
    "            try:\n",
    "                results = future.result()\n",
    "                if results:\n",
    "                    all_nodes.extend(results)\n",
    "            except Exception as e:\n",
    "                print(f\"Project {project_name} generated an exception: {e}\")\n",
    "\n",
    "    print(\"DONE analyzing all projects\")\n",
    "    # Stats by label and method\n",
    "    label_stats = {}\n",
    "    method_stats = {}\n",
    "    code_by_label = {}\n",
    "    code_by_method = {}\n",
    "    exclude = [\"BLOCK\", \"TYPE_REF\", \"LITERAL\", \"<empty>\", \"RET\", \"try\", \"c\", \"e\", \"i\", \"cfg\", \"Tracer\", \"class\", \"reader\", \"line\", \"ioe\", \"ie\"]\n",
    "    for node in all_nodes:\n",
    "        label = node['node_label']\n",
    "        method = node['method_name']\n",
    "        code = node['node_code']\n",
    "        if code in exclude or label in exclude or method in exclude:\n",
    "            continue\n",
    "        if label not in label_stats:\n",
    "            label_stats[label] = 0\n",
    "            code_by_label[label] = []\n",
    "        label_stats[label] += 1\n",
    "        code_by_label[label].append(code)\n",
    "        if method is None:\n",
    "            continue\n",
    "        if method not in method_stats:\n",
    "            method_stats[method] = 0\n",
    "            code_by_method[method] = []\n",
    "        method_stats[method] += 1\n",
    "        code_by_method[method].append(code)\n",
    "\n",
    "    # Create data for DataFrame\n",
    "    data = []\n",
    "    \n",
    "    # Add method stats\n",
    "    for method, count in method_stats.items():\n",
    "        data.append({\n",
    "            'Criterion': 'method',\n",
    "            'Node Type': method,\n",
    "            'Count': count,\n",
    "            'Code Examples': '; '.join(code_by_method[method])\n",
    "        })\n",
    "    \n",
    "    # Add label stats  \n",
    "    for label, count in label_stats.items():\n",
    "        data.append({\n",
    "            'Criterion': 'label',\n",
    "            'Node Type': label, \n",
    "            'Count': count,\n",
    "            'Code Examples': '; '.join(code_by_label[label])\n",
    "        })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing projects: 100%|██████████| 3493/3493 [00:06<00:00, 521.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE analyzing all projects\n"
     ]
    }
   ],
   "source": [
    "data = create_table()\n",
    "df = pd.DataFrame(data)\n",
    "# Sort by Count in descending order\n",
    "df = df.sort_values('Count', ascending=False)\n",
    "# Save to CSV\n",
    "df.to_csv(f\"{ROOT_DIR}/vuln-char.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Criterion</th>\n",
       "      <th>Node Type</th>\n",
       "      <th>Count</th>\n",
       "      <th>Code Examples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>CALL</td>\n",
       "      <td>6234</td>\n",
       "      <td>classTwo-&gt;intTwo = 10; classTwo-&gt;intTwo; class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>label</td>\n",
       "      <td>IDENTIFIER</td>\n",
       "      <td>5648</td>\n",
       "      <td>classTwo; classTwo; classTwo; classTwo; classT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>method</td>\n",
       "      <td>data</td>\n",
       "      <td>2931</td>\n",
       "      <td>data; data; data; data; data; data; data; data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>label</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>1662</td>\n",
       "      <td>wchar_t; NULL; NULL; char; char; char; char; c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>method</td>\n",
       "      <td>delete</td>\n",
       "      <td>1350</td>\n",
       "      <td>delete name; delete [] data; delete [] name; d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>method</td>\n",
       "      <td>exit:&lt;unresolvedSignature&gt;(1)</td>\n",
       "      <td>2</td>\n",
       "      <td>exit(-1); exit(-1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>method</td>\n",
       "      <td>printWLine:&lt;unresolvedSignature&gt;(1)</td>\n",
       "      <td>2</td>\n",
       "      <td>printWLine(data); printWLine(data)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>method</td>\n",
       "      <td>cpp:33:33:SNPRINTF:0</td>\n",
       "      <td>2</td>\n",
       "      <td>SNPRINTF(dest, 100-1, data); SNPRINTF(dest, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>method</td>\n",
       "      <td>swprintf:&lt;unresolvedSignature&gt;(4)</td>\n",
       "      <td>1</td>\n",
       "      <td>swprintf(dest, wcslen(data), L\\</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>method</td>\n",
       "      <td>cpp:25:25:SNPRINTF:0</td>\n",
       "      <td>1</td>\n",
       "      <td>SNPRINTF(dest, wcslen(data), L\\</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Criterion                            Node Type  Count  \\\n",
       "0       label                                 CALL   6234   \n",
       "1       label                           IDENTIFIER   5648   \n",
       "2      method                                 data   2931   \n",
       "3       label                                LOCAL   1662   \n",
       "4      method                               delete   1350   \n",
       "..        ...                                  ...    ...   \n",
       "119    method        exit:<unresolvedSignature>(1)      2   \n",
       "120    method  printWLine:<unresolvedSignature>(1)      2   \n",
       "121    method                 cpp:33:33:SNPRINTF:0      2   \n",
       "122    method    swprintf:<unresolvedSignature>(4)      1   \n",
       "123    method                 cpp:25:25:SNPRINTF:0      1   \n",
       "\n",
       "                                         Code Examples  \n",
       "0    classTwo->intTwo = 10; classTwo->intTwo; class...  \n",
       "1    classTwo; classTwo; classTwo; classTwo; classT...  \n",
       "2    data; data; data; data; data; data; data; data...  \n",
       "3    wchar_t; NULL; NULL; char; char; char; char; c...  \n",
       "4    delete name; delete [] data; delete [] name; d...  \n",
       "..                                                 ...  \n",
       "119                                 exit(-1); exit(-1)  \n",
       "120                 printWLine(data); printWLine(data)  \n",
       "121  SNPRINTF(dest, 100-1, data); SNPRINTF(dest, 10...  \n",
       "122                    swprintf(dest, wcslen(data), L\\  \n",
       "123                    SNPRINTF(dest, wcslen(data), L\\  \n",
       "\n",
       "[124 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read vulnerability characteristics data\n",
    "df = pd.read_csv(f\"{ROOT_DIR}/vuln-char.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created '/vuln-char-table.csv' with total counts for each characteristic!\n"
     ]
    }
   ],
   "source": [
    "# Load data from vuln-char.csv\n",
    "vuln_char_df = pd.read_csv(ROOT_DIR + '/vuln-char.csv')\n",
    "\n",
    "# Filter nodes with Count > 100\n",
    "filtered_df = vuln_char_df[vuln_char_df['Count'] > 50]\n",
    "\n",
    "# Define function to map nodes to vulnerability characteristics\n",
    "def map_node_to_characteristic(node_type, code_example):\n",
    "    node_type = str(node_type).lower()\n",
    "    code_example = str(code_example).lower()\n",
    "\n",
    "    if \"call\" in node_type:\n",
    "        return \"Function calls\"\n",
    "    if \"field\" in node_type or \"access\" in node_type:\n",
    "        return \"Access a field of an object of aggregate type\"\n",
    "    if \"identifier\" in node_type:\n",
    "        return \"Decide the type of the variable\"\n",
    "    if \"assign\" in node_type:\n",
    "        return \"Assign values to variables\"\n",
    "    if \"array\" in node_type:\n",
    "        return \"Use an array\"\n",
    "    if \"alloc\" in node_type or \"free\" in node_type:\n",
    "        return \"Open or discard a memory space\"\n",
    "    if \"cast\" in node_type or \"instanceof\" in code_example:\n",
    "        return \"Type casting and downcasting\"\n",
    "    if any(eq in node_type for eq in [\"assignment\", \"assignmentPlus\", \"assignmentMinus\"]):\n",
    "        return \"Assign values to variables\"\n",
    "\n",
    "    if \"control_structure\" in node_type:\n",
    "        return \"Relate to control flow and code structure of the project\"\n",
    "    if \"logical\" in node_type:\n",
    "        return \"Conduct a boolean/logical/comparison operation\"\n",
    "    \n",
    "    # logical_ops = [\"&&\", \"||\", \"!\", \"==\", \"!=\", \"<\", \">\", \"<=\", \">=\"]\n",
    "    # comparison_ops = [\"equals\", \"notEquals\", \"greaterEqualsThan\", \"lessEqualsThan\", \"greaterThan\", \"lessThan\"]\n",
    "    # if any(boolop in code_example for boolop in logical_ops) or any(boolop in node_type for boolop in comparison_ops):\n",
    "    #     return \"Conduct a boolean/logical/comparison operation\"\n",
    "    \n",
    "    if any(op in node_type for op in [\"addition\", \"subtraction\", \"multiplication\", \"division\"]):\n",
    "        return \"Conduct an arithmetic calculation\"\n",
    " # or any(i in code_example for i in try_catch)\n",
    "    try_catch = [\"try\", \"catch\", \"throw\"]\n",
    "    if any(i in node_type for i in try_catch) :\n",
    "        return \"Exception handling\"\n",
    "    return None\n",
    "\n",
    "# Assign vulnerability characteristics\n",
    "filtered_df = filtered_df.copy()\n",
    "filtered_df['Vulnerability Characteristics'] = filtered_df.apply(\n",
    "    lambda row: map_node_to_characteristic(row['Node Type'], row['Code Examples']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Remove rows with no characteristics\n",
    "filtered_df = filtered_df[filtered_df['Vulnerability Characteristics'].notna()]\n",
    "\n",
    "# Group by Vulnerability Characteristic and sum the counts\n",
    "final_table = []\n",
    "\n",
    "for characteristic in filtered_df['Vulnerability Characteristics'].unique():\n",
    "    group = filtered_df[filtered_df['Vulnerability Characteristics'] == characteristic]\n",
    "    total_count = group['Count'].sum()\n",
    "    node_types = group['Node Type'].tolist()\n",
    "    example_codes = group['Code Examples'].tolist()\n",
    "\n",
    "    # Join node types and example codes with \"/\"\n",
    "    node_type_str = \" / \".join(node_types)\n",
    "    example_code_str = \" / \".join(example_codes)\n",
    "\n",
    "    final_table.append({\n",
    "        \"Vulnerability Characteristics\": characteristic,\n",
    "        \"Total Count\": total_count,\n",
    "        \"Node Type\": node_type_str,\n",
    "        \"Example Code\": example_code_str\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "final_df = pd.DataFrame(final_table)\n",
    "\n",
    "# Sort by Total Count in descending order\n",
    "final_df = final_df.sort_values('Total Count', ascending=False)\n",
    "\n",
    "# Save to CSV\n",
    "final_df.to_csv(ROOT_DIR + '/vuln-char-table-final.csv', index=False)\n",
    "print(\"✅ Created '/vuln-char-table.csv' with total counts for each characteristic!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CALL',\n",
       " 'IDENTIFIER',\n",
       " 'free:<unresolvedSignature>(1) / malloc:<unresolvedSignature>(1)',\n",
       " 'indirectIndexAccess / FIELD_IDENTIFIER / fieldAccess / indirectFieldAccess',\n",
       " 'assignment',\n",
       " 'multiplication',\n",
       " 'cast',\n",
       " 'CONTROL_STRUCTURE']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{ROOT_DIR}/vuln-char-table-final.csv\")\n",
    "# Read vulnerability characteristics data\n",
    "list(df[\"Node Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: file export.dot\n",
    "# Output: {projectname, list[nodesID]}\n",
    "# chọn ra các nnodes có đặc tính thuộc bảng, đếm số lượng thuộc tính thuọc bảng\n",
    "# sắp xết giảm dần các nodes có số lượng thuộc tính thuộc bảng \n",
    "# Kiểm tra các nodes có được có kết nối vớis nhau hay không\n",
    "# TH1: Có kết nối một phần -> coi các nodes có kết nối là 1 center nodes \n",
    "#   -> từng nodes thành phần lấy neibor 1 thành phần\n",
    "# TH2: không có kết nối -> nodes nào độc lập thì nodes đó là 1 center node\n",
    "import pydot\n",
    "import networkx as nx\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample parsed valid_node_types: {'IDENTIFIER', 'CALL', 'CatchClause', 'alloc', 'logicalNot', 'addition', 'stonesoup_array', 'assignment', 'CONTROL_STRUCTURE', 'cast', 'logicalAnd', 'fieldAccess', 'FIELD_IDENTIFIER', 'indexAccess', 'arrayInitializer'}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load vulnerability characteristics\n",
    "df_char = pd.read_csv(\"vuln-char-table-final.csv\")\n",
    "valid_node_types = set()\n",
    "for item in df_char['Node Type'].dropna().unique():\n",
    "    # Assume item is a string like \"['TYPE1', 'TYPE2']\" or \"['TYPE1']\"\n",
    "    try:\n",
    "        # Use regex to find all quoted strings within the brackets\n",
    "        types = re.findall(r\"'(.*?)'\", item)\n",
    "        # Handle cases like ['TYPE'] without quotes or just TYPE\n",
    "        if not types:\n",
    "             # Try removing brackets and splitting if it looks like a list\n",
    "             if isinstance(item, str) and item.startswith('[') and item.endswith(']'):\n",
    "                 cleaned_item = item.strip('[]')\n",
    "                 # Split by comma, strip whitespace from each part\n",
    "                 types = [t.strip() for t in cleaned_item.split(',') if t.strip()]\n",
    "             elif isinstance(item, str): # Assume it's a single type if not bracketed\n",
    "                 types = [item.strip()]\n",
    "\n",
    "        if types:\n",
    "           valid_node_types.update(types)\n",
    "        elif isinstance(item, str): # Fallback if regex and list parsing fail\n",
    "            single_type = item.strip()\n",
    "            if single_type: valid_node_types.add(single_type)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not parse node type string: {item} - {e}\")\n",
    "        # Optional: Add raw string on failure?\n",
    "        # if isinstance(item, str): valid_node_types.add(item.strip())\n",
    "\n",
    "# print(valid_node_types) # Print the full set if needed for debugging\n",
    "# Print a sample to confirm parsing looks reasonable\n",
    "print(f\"Sample parsed valid_node_types: {set(list(valid_node_types)[:20]) if valid_node_types else 'Set is empty'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Helper functions\n",
    "def parse_dot_file(dot_path):\n",
    "    nodes = {}\n",
    "    with open(dot_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if 'LABEL=' in line or 'NAME=' in line or 'METHOD_FULL_NAME=' in line:\n",
    "                node_match = re.search(r'\"(\\d+)\" \\[', line)\n",
    "                # print(node_match)\n",
    "                if not node_match:\n",
    "                    continue\n",
    "                node_id = node_match.group(1)\n",
    "                label_match = re.search(r'label=\"(.*?)\"', line)\n",
    "                name_match = re.search(r'NAME=\"(.*?)\"', line)\n",
    "                method_full_name_match = re.search(r'METHOD_FULL_NAME=\"(.*?)\"', line)\n",
    "                # nếu có label\n",
    "                label = label_match.group(1) if label_match else ''\n",
    "                # nếu co NAME\n",
    "                method = name_match.group(1) if name_match else None\n",
    "                # không có NAME thì lấy METHOD_FULL_NAME\n",
    "                if method is None and method_full_name_match:\n",
    "                    method = method_full_name_match.group(1).split('.')[-1]\n",
    "                # print(f\"Node ID: {node_id}, Label: {label}, Method: {method}\")\n",
    "                # nếu label nằm trong valid_node_types thì thêm vào dict\n",
    "                if label in valid_node_types:\n",
    "                    # nếu có method và method nằm trong valid_node_types\n",
    "                    if (method and method in valid_node_types):\n",
    "                        nodes[node_id] = {\n",
    "                            'label': label,\n",
    "                            'method': method\n",
    "                        }\n",
    "                    # elif method is None:\n",
    "                    else:\n",
    "                        nodes[node_id] = {\n",
    "                            'label': label\n",
    "                        }\n",
    "                        \n",
    "    dot_graph = pydot.graph_from_dot_file(dot_path)[0]\n",
    "    G = nx.drawing.nx_pydot.from_pydot(dot_graph)\n",
    "    # print(G)\n",
    "    return nodes, G\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def render_final_centers_clustered(G, final_groups, output_path=\"graph_clusters.png\"): # Renamed final_centers -> final_groups\n",
    "    if not G: # Add check for None graph\n",
    "        print(\"Error: Cannot render graph because the graph object is None.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    pos = {}\n",
    "    cluster_spacing = 5\n",
    "\n",
    "    # Filter groups to only include nodes present in G\n",
    "    valid_groups = []\n",
    "    nodes_in_G = set(G.nodes())\n",
    "    for group in final_groups:\n",
    "        # Ensure all elements in group are strings before checking presence in nodes_in_G\n",
    "        # Networkx nodes are typically strings when read from dot files\n",
    "        valid_group = [str(node) for node in group if str(node) in nodes_in_G]\n",
    "        if valid_group: # Only keep groups that have at least one valid node\n",
    "            valid_groups.append(valid_group)\n",
    "        #else:\n",
    "             # Optionally print a warning about discarded empty groups\n",
    "             # print(f\"Warning: Discarding group {group} as none of its nodes exist in the graph G.\")\n",
    "\n",
    "    if not valid_groups: # Check if any valid groups remain\n",
    "         print(\"Warning: No valid groups found containing nodes present in the graph G. Cannot render.\")\n",
    "         plt.close() # Close the empty figure\n",
    "         return\n",
    "\n",
    "    # Generate colors based on the number of valid groups\n",
    "    try:\n",
    "        # Handle case where len(valid_groups) might be 0, although checked above\n",
    "        num_colors = max(1, len(valid_groups))\n",
    "        colors = plt.cm.get_cmap('tab20', num_colors)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating colormap: {e}. Using default color.\")\n",
    "        # Fallback color or handle differently\n",
    "        colors = lambda idx: 'blue' # Simple fallback\n",
    "\n",
    "\n",
    "    for idx, group in enumerate(valid_groups): # Iterate through valid groups\n",
    "        # Create subgraph for the current valid group\n",
    "        try:\n",
    "            # Ensure nodes in group are suitable for subgraph (should be strings)\n",
    "            subG = G.subgraph(group) # This should succeed now\n",
    "\n",
    "            # Layout within the small group\n",
    "            if subG.number_of_nodes() > 0:\n",
    "                 # Add try-except around layout calculation\n",
    "                 try:\n",
    "                    # Use a layout that handles disconnected components if necessary,\n",
    "                    # though spring_layout usually works.\n",
    "                    sub_pos = nx.spring_layout(subG, seed=idx)\n",
    "                 except Exception as layout_err:\n",
    "                     print(f\"Error calculating layout for group {idx+1}: {layout_err}. Skipping group.\")\n",
    "                     continue # Skip to next group if layout fails\n",
    "            else:\n",
    "                 # print(f\"Skipping empty subgraph for group {idx+1}\") # Debugging line\n",
    "                 continue # Skip this group if subgraph is unexpectedly empty\n",
    "\n",
    "            # Shift the entire group to a distinct region\n",
    "            shift_x = (idx % 5) * cluster_spacing\n",
    "            shift_y = -(idx // 5) * cluster_spacing\n",
    "\n",
    "            for node, (x, y) in sub_pos.items():\n",
    "                pos[str(node)] = (x + shift_x, y + shift_y) # Ensure pos keys are strings\n",
    "\n",
    "            # Draw nodes for the group - Ensure nodelist=group only contains nodes in pos\n",
    "            drawable_nodes = [node for node in group if node in pos]\n",
    "            if drawable_nodes:\n",
    "                 nx.draw_networkx_nodes(G, pos,\n",
    "                                       nodelist=drawable_nodes,\n",
    "                                       node_color=[colors(idx)], # Use node_color instead of color\n",
    "                                      #  label=f'Group {idx+1}', # Labeling nodes directly might clutter legend\n",
    "                                       node_size=300)\n",
    "            # else:\n",
    "                 # Optionally print warning if no nodes could be positioned/drawn\n",
    "                 # print(f\"Warning: No drawable nodes for group {idx+1}\")\n",
    "\n",
    "        except Exception as e:\n",
    "             # Catch potential key errors if a node ID format mismatch occurs despite filtering\n",
    "             print(f\"Error processing/drawing group {idx+1} ({group}): {e}\")\n",
    "             continue # Skip to the next group on error\n",
    "\n",
    "    # Draw edges and labels after all positions are calculated\n",
    "    try:\n",
    "         # Draw edges only between nodes that have positions calculated\n",
    "         drawable_edges = [(u, v) for u, v in G.edges() if str(u) in pos and str(v) in pos]\n",
    "         nx.draw_networkx_edges(G, pos, edgelist=drawable_edges, alpha=0.3)\n",
    "         # Draw labels only for nodes that have positions\n",
    "         nx.draw_networkx_labels(G, pos, font_size=8) # pos keys are guaranteed to be strings\n",
    "    except Exception as e:\n",
    "         print(f\"Error drawing edges or labels: {e}\")\n",
    "\n",
    "    plt.axis('off')\n",
    "    # Handle legend warning if no labels were successfully generated\n",
    "    # The node drawing doesn't add handles/labels suitable for plt.legend() by default this way\n",
    "    # If a legend per group is desired, it needs a different approach (e.g., plotting dummy points)\n",
    "    # try:\n",
    "    #      handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    #      if handles:\n",
    "    #          plt.legend()\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error creating legend: {e}\")\n",
    "\n",
    "    plt.title(\"Clustered Graph Visualization\")\n",
    "    # Use tight_layout cautiously, can sometimes cause issues\n",
    "    try:\n",
    "        plt.tight_layout()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: tight_layout failed: {e}\")\n",
    "\n",
    "    try:\n",
    "         plt.savefig(output_path, dpi=300)\n",
    "         print(f\"Graph saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "         print(f\"Error saving graph image to {output_path}: {e}\")\n",
    "\n",
    "    plt.close() # Ensure figure is closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_project(dot_path):\n",
    "    nodes, G = parse_dot_file(dot_path) # G contains all nodes and edges\n",
    "\n",
    "    # Chia thành strong và weak nodes\n",
    "    strong_nodes = {nid: info for nid, info in nodes.items() if len(info) == 2}\n",
    "    weak_nodes = {nid: info for nid, info in nodes.items() if len(info) < 2}\n",
    "    print(f\"Number of strong nodes: {len(strong_nodes)}\")\n",
    "    print(f\"Number of weak nodes: {len(weak_nodes)}\")\n",
    "\n",
    "    final_groups = [] # Store groups (lists of nodes) instead of just centers\n",
    "\n",
    "    # --- Handle STRONG nodes ---\n",
    "    # Mỗi strong node tìm weak node nối trực tiếp\n",
    "    center_to_children = {}\n",
    "    for center_id in strong_nodes.keys():\n",
    "        neighbors = set(G.neighbors(center_id))\n",
    "        linked_weak_nodes = neighbors.intersection(weak_nodes.keys())\n",
    "        if linked_weak_nodes:\n",
    "            center_to_children[center_id] = linked_weak_nodes\n",
    "\n",
    "    assigned_weak_nodes = set()\n",
    "    for linked in center_to_children.values():\n",
    "        assigned_weak_nodes.update(linked)\n",
    "\n",
    "    # Add strong centers and their children\n",
    "    for center_id in strong_nodes.keys():\n",
    "        if center_id in center_to_children:\n",
    "            group = [center_id] + list(center_to_children[center_id])\n",
    "            final_groups.append(group)\n",
    "        else:\n",
    "            final_groups.append([center_id])\n",
    "\n",
    "    # --- Handle WEAK nodes ---\n",
    "    # We need the subgraph of weak nodes to find connections among them\n",
    "    # Ensure weak_nodes keys are actually in G before creating subgraph\n",
    "    valid_weak_nodes = set(weak_nodes.keys()) & set(G.nodes())\n",
    "    weak_subG_undirected = None\n",
    "    if not valid_weak_nodes:\n",
    "        print(\"No valid weak nodes found in the graph.\")\n",
    "        remaining_weak_nodes = set()\n",
    "    else:\n",
    "        weak_subG = G.subgraph(valid_weak_nodes)\n",
    "        weak_subG_undirected = weak_subG.to_undirected() # Use undirected for neighbor finding\n",
    "        remaining_weak_nodes = set(weak_subG_undirected.nodes()) - assigned_weak_nodes\n",
    "\n",
    "    weak_groups_count = 0\n",
    "    while remaining_weak_nodes:\n",
    "        # Đếm số neighbor trong remaining_weak_nodes for each node\n",
    "        neighbor_counts = {}\n",
    "        for node in remaining_weak_nodes:\n",
    "            # Neighbors within the weak subgraph AND still remaining\n",
    "            # Ensure neighbor calculation handles potential disconnections if weak_subG_undirected is empty/invalid\n",
    "            if weak_subG_undirected and node in weak_subG_undirected:\n",
    "                neighbors = set(weak_subG_undirected.neighbors(node)) & remaining_weak_nodes\n",
    "                neighbor_counts[node] = len(neighbors)\n",
    "            else: # Node might be isolated or graph might be empty\n",
    "                neighbor_counts[node] = 0\n",
    "\n",
    "        # Identify isolated nodes within the remaining set\n",
    "        isolated_nodes = {node for node in remaining_weak_nodes if neighbor_counts.get(node, 0) == 0}\n",
    "        non_isolated_nodes = remaining_weak_nodes - isolated_nodes\n",
    "\n",
    "        # Process isolated nodes first\n",
    "        for node in isolated_nodes:\n",
    "             final_groups.append([node]) # Each isolated node is its own group\n",
    "             weak_groups_count += 1\n",
    "\n",
    "        remaining_weak_nodes -= isolated_nodes # Remove processed isolated nodes\n",
    "\n",
    "        if not remaining_weak_nodes: # Exit if only isolated nodes were left or the set is now empty\n",
    "            break\n",
    "\n",
    "        # Chọn node nhiều neighbor nhất làm center (within the remaining non-isolated weak nodes)\n",
    "        center_node = max(non_isolated_nodes, key=lambda x: neighbor_counts.get(x, -1)) # Use get with default\n",
    "\n",
    "        # This center's group includes itself and its immediate neighbors within the remaining weak nodes\n",
    "        # Need to ensure weak_subG_undirected is valid before calling neighbors\n",
    "        if weak_subG_undirected and center_node in weak_subG_undirected:\n",
    "             neighbors = set(weak_subG_undirected.neighbors(center_node)) & remaining_weak_nodes # Check against remaining\n",
    "             group = set([center_node]) | neighbors\n",
    "        else: # Should not happen if non_isolated_nodes is correct, but safety check\n",
    "             group = set([center_node])\n",
    "\n",
    "        final_groups.append(list(group)) # Add the identified group\n",
    "        weak_groups_count += 1\n",
    "\n",
    "        # Xoá the entire group (center and its neighbors) ra khỏi remaining_nodes\n",
    "        remaining_weak_nodes -= group\n",
    "\n",
    "    final_centers = []\n",
    "    for group in final_groups:\n",
    "        center = group[0]\n",
    "        final_centers.append(center)\n",
    "    # Return the list of groups and the full graph G for visualization\n",
    "    return final_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_project_hybrid(dot_path):\n",
    "    \"\"\"\n",
    "    Improved center node selection using a hybrid approach that respects connected components\n",
    "    and uses centrality measures to select the most appropriate node from each component.\n",
    "    \n",
    "    Args:\n",
    "        dot_path: Path to the dot file\n",
    "        \n",
    "    Returns:\n",
    "        List of center node IDs\n",
    "    \"\"\"\n",
    "    # Step 1: Parse the dot file to get nodes and graph\n",
    "    nodes, G = parse_dot_file(dot_path)\n",
    "    \n",
    "    # Step 2: Identify strong and weak nodes\n",
    "    strong_nodes = {nid: info for nid, info in nodes.items() if len(info) == 2}\n",
    "    weak_nodes = {nid: info for nid, info in nodes.items() if len(info) < 2}\n",
    "    \n",
    "    print(f\"Number of strong nodes: {len(strong_nodes)}\")\n",
    "    print(f\"Number of weak nodes: {len(weak_nodes)}\")\n",
    "    \n",
    "    # Step 3: Strong nodes are always centers\n",
    "    final_centers = list(strong_nodes.keys())\n",
    "    \n",
    "    # Step 4: Identify weak nodes directly connected to strong nodes\n",
    "    connected_to_strong = set()\n",
    "    for strong_id in strong_nodes:\n",
    "        neighbors = set(G.neighbors(strong_id)) & set(weak_nodes.keys())\n",
    "        connected_to_strong.update(neighbors)\n",
    "    \n",
    "    print(f\"Weak nodes directly connected to strong nodes: {len(connected_to_strong)}\")\n",
    "    \n",
    "    # Step 5: Handle remaining weak nodes not connected to strong nodes\n",
    "    remaining_weak = set(weak_nodes.keys()) - connected_to_strong\n",
    "    print(f\"Remaining weak nodes to process: {len(remaining_weak)}\")\n",
    "    \n",
    "    # Step 6: Create subgraph of remaining weak nodes\n",
    "    weak_subG = G.subgraph(remaining_weak)\n",
    "    weak_undirected = weak_subG.to_undirected()\n",
    "    \n",
    "    # Step 7: Find connected components (these are our natural groups)\n",
    "    components = list(nx.connected_components(weak_undirected))\n",
    "    print(f\"Found {len(components)} connected components in weak nodes subgraph\")\n",
    "    \n",
    "    # Step 8: Process each component to select a center\n",
    "    component_centers = []\n",
    "    for i, component in enumerate(components):\n",
    "        if not component:  # Skip empty components\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing component {i+1} with {len(component)} nodes\")\n",
    "        component_subG = weak_undirected.subgraph(component)\n",
    "        \n",
    "        if len(component) <= 3:  # For small components, just pick any node\n",
    "            center = next(iter(component))\n",
    "            print(f\"  Small component: selecting {center} as center\")\n",
    "        else:\n",
    "            # For larger components, use centrality measures\n",
    "            try:\n",
    "                # Try betweenness centrality first (identifies bridge nodes)\n",
    "                centrality = nx.betweenness_centrality(component_subG)\n",
    "                center_type = \"betweenness centrality\"\n",
    "            except:\n",
    "                # Fallback to degree centrality\n",
    "                centrality = nx.degree_centrality(component_subG)\n",
    "                center_type = \"degree centrality\"\n",
    "            \n",
    "            center = max(centrality, key=centrality.get)\n",
    "            print(f\"  Large component: selecting {center} as center based on {center_type}\")\n",
    "        \n",
    "        component_centers.append(center)\n",
    "    \n",
    "    # Add component centers to final centers\n",
    "    final_centers.extend(component_centers)\n",
    "    \n",
    "    print(f\"Total centers: {len(final_centers)} (Strong: {len(strong_nodes)}, Weak: {len(final_centers) - len(strong_nodes)})\")\n",
    "    return final_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all projects and collect results using multiprocessing\n",
    "from multiprocessing import Pool, Manager\n",
    "\n",
    "def process_project_wrapper(project):\n",
    "    project_path = os.path.join(dot_folder, project)\n",
    "    project_results = []\n",
    "    project_stats = []\n",
    "    \n",
    "    for file in os.listdir(project_path):\n",
    "        if file.endswith(\"export.dot\"):\n",
    "            dot_path = os.path.join(project_path, file)\n",
    "            \n",
    "            try:\n",
    "                center_nodes = process_project(dot_path)\n",
    "                project_results.append({\n",
    "                    'project': project,\n",
    "                    'center_nodes': center_nodes\n",
    "                })\n",
    "                project_stats.append({\n",
    "                    'project': project,\n",
    "                    'num_center_nodes': len(center_nodes)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {dot_path}: {e}\")\n",
    "                continue\n",
    "                \n",
    "    return project_results, project_stats\n",
    "\n",
    "dot_folder = ROOT_DIR + \"/cpg-output\"\n",
    "projects = [p for p in os.listdir(dot_folder)][:5]\n",
    "\n",
    "# Use multiprocessing to process projects in parallel\n",
    "with Pool() as pool:\n",
    "    all_results = list(tqdm(pool.imap(process_project_wrapper, projects), \n",
    "                          total=len(projects),\n",
    "                          desc=\"Processing projects\"))\n",
    "\n",
    "# Combine results from all processes\n",
    "results = []\n",
    "stats = []\n",
    "for project_results, project_stats in all_results:\n",
    "    results.extend(project_results)\n",
    "    stats.extend(project_stats)\n",
    "\n",
    "# Save stats to CSV\n",
    "csv_path = ROOT_DIR + \"/center_nodes_stats.csv\"\n",
    "with open(csv_path, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"project\", \"num_center_nodes\"])\n",
    "    writer.writeheader()\n",
    "    for row in stats:\n",
    "        writer.writerow(row)\n",
    "\n",
    "# Save full results to JSON  \n",
    "with open(ROOT_DIR + \"/center_nodes_result.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"✅ Done! Saved to center_nodes_result.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
