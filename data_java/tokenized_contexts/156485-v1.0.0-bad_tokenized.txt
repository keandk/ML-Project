# Tokenized code fragments for 156485-v1.0.0-bad
# Total center nodes processed: 101
# Total code fragments found: 361

CENTER_NODE: 30064771920
FRAGMENT_COUNT: 3
  ORIGINAL[0]: int LexerScheme.YYEOF = -1
  TYPE[0]: CALL
  TOKENIZED[0]: int VAR1 . VAR2 = -1
  ORIGINAL[1]: LexerScheme.YYEOF
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2
  ORIGINAL[2]: -1
  TYPE[2]: CALL
  TOKENIZED[2]: -1

CENTER_NODE: 68719477145
FRAGMENT_COUNT: 3
  ORIGINAL[0]: amphodiplopia_educatable.paragraphismInteraulic(diact_burying)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[1]: amphodiplopia_educatable
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: diact_burying
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771740
FRAGMENT_COUNT: 3
  ORIGINAL[0]: new CorpusculeSinarquism()
  TYPE[0]: CALL
  TOKENIZED[0]: new FUN1 ( )
  ORIGINAL[1]: turbith_pregnancy
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: turbith_pregnancy
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477162
FRAGMENT_COUNT: 4
  ORIGINAL[0]: DiemakingUnpleasing waxer_unhindered = new DiemakingUnpleasing()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new DiemakingUnpleasing()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: waxer_unhindered
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: waxer_unhindered
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477140
FRAGMENT_COUNT: 5
  ORIGINAL[0]: new SwapeAcetylbenzene()
  TYPE[0]: CALL
  TOKENIZED[0]: new FUN1 ( )
  ORIGINAL[1]: ridgy_neap.manoMaglemose(producal_sickling)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[2]: ridgy_neap
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: ridgy_neap
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: producal_sickling
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771646
FRAGMENT_COUNT: 4
  ORIGINAL[0]: EquiproducingAlkenyl witful_semiluxury = new EquiproducingAlkenyl()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new EquiproducingAlkenyl()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: witful_semiluxury
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: witful_semiluxury
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771958
FRAGMENT_COUNT: 3
  ORIGINAL[0]: String[] LexerScheme.ZZ_ERROR_MSG = { \
  TYPE[0]: CALL
  TOKENIZED[0]: String[] VAR1 . VAR2 = { \
  ORIGINAL[1]: LexerScheme.ZZ_ERROR_MSG
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2
  ORIGINAL[2]: { \
  TYPE[2]: CALL
  TOKENIZED[2]: { \

CENTER_NODE: 68719477179
FRAGMENT_COUNT: 5
  ORIGINAL[0]: ResuspectOverinsure unweeded_betting = new ResuspectOverinsure()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new ResuspectOverinsure()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: unweeded_betting
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: unweeded_betting
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: unweeded_betting
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771379
FRAGMENT_COUNT: 3
  ORIGINAL[0]: zzBuffer[zzStartRead + pos]
  TYPE[0]: CALL
  TOKENIZED[0]: zzBuffer[zzStartRead + pos]
  ORIGINAL[1]: this.zzBuffer
  TYPE[1]: CALL
  TOKENIZED[1]: this . VAR1
  ORIGINAL[2]: zzStartRead + pos
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 + VAR2

CENTER_NODE: 68719477184
FRAGMENT_COUNT: 5
  ORIGINAL[0]: new CallableErythristic()
  TYPE[0]: CALL
  TOKENIZED[0]: new FUN1 ( )
  ORIGINAL[1]: plantlet_actian.increepKhatri(benincasa_xylitol)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[2]: plantlet_actian
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: plantlet_actian
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: benincasa_xylitol
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477148
FRAGMENT_COUNT: 5
  ORIGINAL[0]: new HubExtravert()
  TYPE[0]: CALL
  TOKENIZED[0]: new FUN1 ( )
  ORIGINAL[1]: bioecological_logistician.undedicateQuizziness(zapus_preinsinuative)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[2]: bioecological_logistician
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: bioecological_logistician
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: zapus_preinsinuative
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477265
FRAGMENT_COUNT: 3
  ORIGINAL[0]: galliform_interureteric.succorLadyfish(sevenfold_countercheer)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[1]: galliform_interureteric
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: sevenfold_countercheer
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477176
FRAGMENT_COUNT: 5
  ORIGINAL[0]: new ColorNepal()
  TYPE[0]: CALL
  TOKENIZED[0]: new FUN1 ( )
  ORIGINAL[1]: vanessa_eseptate.coeducationalUngrubbed(bedare_reprice)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[2]: vanessa_eseptate
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: vanessa_eseptate
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: bedare_reprice
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476986
FRAGMENT_COUNT: 8
  ORIGINAL[0]: this.zzReader
  TYPE[0]: CALL
  TOKENIZED[0]: this . VAR1
  ORIGINAL[1]: this.zzAtBOL
  TYPE[1]: CALL
  TOKENIZED[1]: this . VAR1
  ORIGINAL[2]: this.zzAtEOF
  TYPE[2]: CALL
  TOKENIZED[2]: this . VAR1
  ORIGINAL[3]: this.zzEOFDone
  TYPE[3]: CALL
  TOKENIZED[3]: this . VAR1
  ORIGINAL[4]: this.zzEndRead
  TYPE[4]: CALL
  TOKENIZED[4]: this . VAR1
  ORIGINAL[5]: this.zzStartRead
  TYPE[5]: CALL
  TOKENIZED[5]: this . VAR1
  ORIGINAL[6]: zzStartRead
  TYPE[6]: FIELD_IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: this
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: this

CENTER_NODE: 30064771962
FRAGMENT_COUNT: 4
  ORIGINAL[0]: int[] LexerScheme.ZZ_ATTRIBUTE = zzUnpackAttribute()
  TYPE[0]: CALL
  TOKENIZED[0]: int[] VAR1 . VAR2 = FUN1 ( )
  ORIGINAL[1]: LexerScheme.ZZ_ATTRIBUTE
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2
  ORIGINAL[2]: zzUnpackAttribute()
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( )
  ORIGINAL[3]: ZZ_ATTRIBUTE
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771782
FRAGMENT_COUNT: 5
  ORIGINAL[0]: pockweed_undebilitating.getloofness_coassert() != null && psql_host != null && psql_user != null && psql_pass != null && psql_port != null && psql_dbname != null
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . FUN1 ( ) != null && VAR2 != null && VAR3 != null && VAR4 != null && VAR5 != null && VAR6 != null
  ORIGINAL[1]: \
  TYPE[1]: CALL
  TOKENIZED[1]: \
  ORIGINAL[2]: \
  TYPE[2]: CALL
  TOKENIZED[2]: \
  ORIGINAL[3]: \
  TYPE[3]: CALL
  TOKENIZED[3]: \
  ORIGINAL[4]: psql_dbname
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771912
FRAGMENT_COUNT: 3
  ORIGINAL[0]: PrintStream LexerScheme.puffedChronologize = null
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 . VAR3 = null
  ORIGINAL[1]: LexerScheme.puffedChronologize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2
  ORIGINAL[2]: puffedChronologize
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771619
FRAGMENT_COUNT: 3
  ORIGINAL[0]: TerrorismGlobeholder masanao_tartan = new TerrorismGlobeholder()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new TerrorismGlobeholder()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: masanao_tartan
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477303
FRAGMENT_COUNT: 5
  ORIGINAL[0]: NursinglyUnswept transmural_corchorus = new NursinglyUnswept()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new NursinglyUnswept()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: transmural_corchorus
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: transmural_corchorus
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: transmural_corchorus
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476736
FRAGMENT_COUNT: 3
  ORIGINAL[0]: this.loofness_coassert
  TYPE[0]: CALL
  TOKENIZED[0]: this . VAR1
  ORIGINAL[1]: loofness_coassert
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: this
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: this

CENTER_NODE: 30064771745
FRAGMENT_COUNT: 3
  ORIGINAL[0]: tesserants_mimeo.sternworksTicket(photochromatic_chanceful)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[1]: tesserants_mimeo
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: photochromatic_chanceful
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771696
FRAGMENT_COUNT: 3
  ORIGINAL[0]: new RouthPiacularness()
  TYPE[0]: CALL
  TOKENIZED[0]: new FUN1 ( )
  ORIGINAL[1]: unsluiced_synonymous
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: unsluiced_synonymous
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771663
FRAGMENT_COUNT: 3
  ORIGINAL[0]: HylozoismCollectanea varanid_plagianthus = new HylozoismCollectanea()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new HylozoismCollectanea()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: varanid_plagianthus
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640283
FRAGMENT_COUNT: 2
  ORIGINAL[0]: zzReader != null
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != null
  ORIGINAL[1]: if (zzReader != null)
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: if ( VAR1 != null )

CENTER_NODE: 68719476778
FRAGMENT_COUNT: 4
  ORIGINAL[0]: packed.length()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . FUN1 ( )
  ORIGINAL[1]: l
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: packed
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: packed
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771558
FRAGMENT_COUNT: 4
  ORIGINAL[0]: MicroglossiaOrnithotomy effie_reflag = new MicroglossiaOrnithotomy()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new MicroglossiaOrnithotomy()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: effie_reflag
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: effie_reflag
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771733
FRAGMENT_COUNT: 3
  ORIGINAL[0]: cassia_nonpartiality.dragonetPseudoviscosity(versicular_filthily)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[1]: cassia_nonpartiality
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: versicular_filthily
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771926
FRAGMENT_COUNT: 4
  ORIGINAL[0]: int[] LexerScheme.ZZ_LEXSTATE = { 0, 1 }
  TYPE[0]: CALL
  TOKENIZED[0]: int[] VAR1 . VAR2 = { 0 , 1 }
  ORIGINAL[1]: LexerScheme.ZZ_LEXSTATE
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2
  ORIGINAL[2]: { 0, 1 }
  TYPE[2]: CALL
  TOKENIZED[2]: { 0 , 1 }
  ORIGINAL[3]: ZZ_LEXSTATE
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771956
FRAGMENT_COUNT: 2
  ORIGINAL[0]: int LexerScheme.ZZ_PUSHBACK_2BIG = 2
  TYPE[0]: CALL
  TOKENIZED[0]: int VAR1 . VAR2 = 2
  ORIGINAL[1]: LexerScheme.ZZ_PUSHBACK_2BIG
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2

CENTER_NODE: 68719477293
FRAGMENT_COUNT: 3
  ORIGINAL[0]: supplicating_munificency.squilloideaUnbarrable(kinghunter_focalize)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[1]: supplicating_munificency
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: kinghunter_focalize
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771590
FRAGMENT_COUNT: 4
  ORIGINAL[0]: BastinadeSline temporofrontal_bedcord = new BastinadeSline()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new BastinadeSline()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: temporofrontal_bedcord
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: temporofrontal_bedcord
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640284
FRAGMENT_COUNT: 2
  ORIGINAL[0]: try
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: try
  ORIGINAL[1]: catch
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: catch

CENTER_NODE: 30064771616
FRAGMENT_COUNT: 3
  ORIGINAL[0]: new RectificationProselyter()
  TYPE[0]: CALL
  TOKENIZED[0]: new FUN1 ( )
  ORIGINAL[1]: cocainomania_less
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: cocainomania_less
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771947
FRAGMENT_COUNT: 4
  ORIGINAL[0]: int[] LexerScheme.ZZ_TRANS = zzUnpackTrans()
  TYPE[0]: CALL
  TOKENIZED[0]: int[] VAR1 . VAR2 = FUN1 ( )
  ORIGINAL[1]: LexerScheme.ZZ_TRANS
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2
  ORIGINAL[2]: zzUnpackTrans()
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( )
  ORIGINAL[3]: ZZ_TRANS
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477157
FRAGMENT_COUNT: 3
  ORIGINAL[0]: fleecelike_southernism.questionnaireOverjacket(rhineura_heterodoxly)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[1]: fleecelike_southernism
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: rhineura_heterodoxly
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771369
FRAGMENT_COUNT: 3
  ORIGINAL[0]: this.zzLexicalState = newState
  TYPE[0]: CALL
  TOKENIZED[0]: this . VAR1 = VAR2
  ORIGINAL[1]: this.zzLexicalState
  TYPE[1]: CALL
  TOKENIZED[1]: this . VAR1
  ORIGINAL[2]: newState
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771076
FRAGMENT_COUNT: 4
  ORIGINAL[0]: new java.io.InputStreamReader(in)
  TYPE[0]: CALL
  TOKENIZED[0]: new VAR1 . VAR2 . FUN1 ( VAR3 )
  ORIGINAL[1]: $obj6
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: $obj6
  ORIGINAL[2]: in
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: $obj6
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: $obj6

CENTER_NODE: 30064771393
FRAGMENT_COUNT: 1
  ORIGINAL[0]: throw new Error(message);
  TYPE[0]: CALL
  TOKENIZED[0]: throw new FUN1 ( VAR1 ) ;

CENTER_NODE: 68719476919
FRAGMENT_COUNT: 3
  ORIGINAL[0]: this.zzReader = in
  TYPE[0]: CALL
  TOKENIZED[0]: this . VAR1 = VAR2
  ORIGINAL[1]: this.zzReader
  TYPE[1]: CALL
  TOKENIZED[1]: this . VAR1
  ORIGINAL[2]: in
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771736
FRAGMENT_COUNT: 3
  ORIGINAL[0]: new LaemodipodanUnsophisticate()
  TYPE[0]: CALL
  TOKENIZED[0]: new FUN1 ( )
  ORIGINAL[1]: spermatolytic_miter
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: spermatolytic_miter
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477213
FRAGMENT_COUNT: 3
  ORIGINAL[0]: mortalwise_petiolulate.ophiomancyLeucosphere(untranscribed_ceramium)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[1]: mortalwise_petiolulate
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: untranscribed_ceramium
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640268
FRAGMENT_COUNT: 2
  ORIGINAL[0]: try
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: try
  ORIGINAL[1]: finally
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: finally

CENTER_NODE: 30064771720
FRAGMENT_COUNT: 3
  ORIGINAL[0]: new RedepositMuskeg()
  TYPE[0]: CALL
  TOKENIZED[0]: new FUN1 ( )
  ORIGINAL[1]: codilla_epidermatoid
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: codilla_epidermatoid
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771952
FRAGMENT_COUNT: 2
  ORIGINAL[0]: int LexerScheme.ZZ_UNKNOWN_ERROR = 0
  TYPE[0]: CALL
  TOKENIZED[0]: int VAR1 . VAR2 = 0
  ORIGINAL[1]: LexerScheme.ZZ_UNKNOWN_ERROR
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2

CENTER_NODE: 68719477220
FRAGMENT_COUNT: 5
  ORIGINAL[0]: new IndusioidUnfunnily()
  TYPE[0]: CALL
  TOKENIZED[0]: new FUN1 ( )
  ORIGINAL[1]: bella_bajra.nonreferenceBorstall(agminated_ungamelike)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[2]: bella_bajra
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: bella_bajra
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: agminated_ungamelike
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477284
FRAGMENT_COUNT: 5
  ORIGINAL[0]: new GawkishlyOmodynia()
  TYPE[0]: CALL
  TOKENIZED[0]: new FUN1 ( )
  ORIGINAL[1]: panchromatic_repositor.personifyUnfamed(gametogonium_antialcoholist)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[2]: panchromatic_repositor
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: panchromatic_repositor
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: gametogonium_antialcoholist
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771938
FRAGMENT_COUNT: 2
  ORIGINAL[0]: String LexerScheme.ZZ_ACTION_PACKED_0 = \
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 . VAR3 = \
  ORIGINAL[1]: LexerScheme.ZZ_ACTION_PACKED_0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2

CENTER_NODE: 30064771693
FRAGMENT_COUNT: 3
  ORIGINAL[0]: dereligion_multinuclear.hebenonSouthard(duces_catechization)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[1]: dereligion_multinuclear
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: duces_catechization
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477277
FRAGMENT_COUNT: 3
  ORIGINAL[0]: kotwalee_actiniarian.bibitoryWaxing(hansardization_structuralize)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[1]: kotwalee_actiniarian
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: hansardization_structuralize
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771676
FRAGMENT_COUNT: 3
  ORIGINAL[0]: new TantrikMesiolabial()
  TYPE[0]: CALL
  TOKENIZED[0]: new FUN1 ( )
  ORIGINAL[1]: outboast_shogunate
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: outboast_shogunate
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771654
FRAGMENT_COUNT: 4
  ORIGINAL[0]: MemEsponton reobligation_remigrant = new MemEsponton()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new MemEsponton()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: reobligation_remigrant
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: reobligation_remigrant
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719476803
FRAGMENT_COUNT: 8
  ORIGINAL[0]: int l = packed.length()
  TYPE[0]: CALL
  TOKENIZED[0]: int VAR1 = VAR2 . FUN1 ( )
  ORIGINAL[1]: i < l
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 < VAR2
  ORIGINAL[2]: result[j++] = high | packed.charAt(i++)
  TYPE[2]: CALL
  TOKENIZED[2]: result[j++] = VAR1 | VAR2 . FUN1 ( i++ )
  ORIGINAL[3]: i
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: i
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: l
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: i
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: i
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 30064771917
FRAGMENT_COUNT: 4
  ORIGINAL[0]: LexerScheme.limnocnidaOlivella
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . VAR2
  ORIGINAL[1]: new java.util.concurrent.atomic.AtomicBoolean(false)
  TYPE[1]: CALL
  TOKENIZED[1]: new VAR1 . VAR2 . VAR3 . VAR4 . FUN1 ( false )
  ORIGINAL[2]: LexerScheme.limnocnidaOlivella
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 . VAR2
  ORIGINAL[3]: limnocnidaOlivella
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477278
FRAGMENT_COUNT: 4
  ORIGINAL[0]: PreobligationVannerman ramadoss_malinfluence = new PreobligationVannerman()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new PreobligationVannerman()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: ramadoss_malinfluence
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: ramadoss_malinfluence
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719476958
FRAGMENT_COUNT: 5
  ORIGINAL[0]: zzCurrentPos >= zzBuffer.length
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= VAR2 . VAR3
  ORIGINAL[1]: this.zzBuffer = newBuffer
  TYPE[1]: CALL
  TOKENIZED[1]: this . VAR1 = VAR2
  ORIGINAL[2]: this.zzBuffer
  TYPE[2]: CALL
  TOKENIZED[2]: this . VAR1
  ORIGINAL[3]: newBuffer
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: newBuffer
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771080
FRAGMENT_COUNT: 4
  ORIGINAL[0]: int[] result = new int[9]
  TYPE[0]: CALL
  TOKENIZED[0]: int[] VAR1 = new int[9]
  ORIGINAL[1]: new int[9]
  TYPE[1]: CALL
  TOKENIZED[1]: new int[9]
  ORIGINAL[2]: result
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: offset
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477128
FRAGMENT_COUNT: 5
  ORIGINAL[0]: new SourerNonsecrecy()
  TYPE[0]: CALL
  TOKENIZED[0]: new FUN1 ( )
  ORIGINAL[1]: exfoliation_diaphanotype.humorousGhostlify(bowyer_memphian)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[2]: exfoliation_diaphanotype
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: exfoliation_diaphanotype
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: bowyer_memphian
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640266
FRAGMENT_COUNT: 2
  ORIGINAL[0]: try
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: try
  ORIGINAL[1]: catch
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: catch

CENTER_NODE: 68719477206
FRAGMENT_COUNT: 4
  ORIGINAL[0]: NoncotyledonousExculpatorily decretively_unmurmuringly = new NoncotyledonousExculpatorily()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new NoncotyledonousExculpatorily()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: decretively_unmurmuringly
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: decretively_unmurmuringly
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640327
FRAGMENT_COUNT: 3
  ORIGINAL[0]: try
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: try
  ORIGINAL[1]: catch
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: catch
  ORIGINAL[2]: catch
  TYPE[2]: CONTROL_STRUCTURE
  TOKENIZED[2]: catch

CENTER_NODE: 30064771723
FRAGMENT_COUNT: 3
  ORIGINAL[0]: SultanAnti seemer_phasmid = new SultanAnti()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new SultanAnti()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: seemer_phasmid
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771666
FRAGMENT_COUNT: 4
  ORIGINAL[0]: StringfulUnexpress sanguinary_misdevise = new StringfulUnexpress()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new StringfulUnexpress()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: sanguinary_misdevise
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: sanguinary_misdevise
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771622
FRAGMENT_COUNT: 4
  ORIGINAL[0]: OlfactoryPretabulation bucketful_doltishly = new OlfactoryPretabulation()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new OlfactoryPretabulation()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: bucketful_doltishly
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: bucketful_doltishly
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771376
FRAGMENT_COUNT: 7
  ORIGINAL[0]: new String(zzBuffer, zzStartRead, zzMarkedPos - zzStartRead)
  TYPE[0]: CALL
  TOKENIZED[0]: new FUN1 ( VAR1 , VAR2 , VAR3 - VAR2 )
  ORIGINAL[1]: this.zzBuffer
  TYPE[1]: CALL
  TOKENIZED[1]: this . VAR1
  ORIGINAL[2]: this.zzStartRead
  TYPE[2]: CALL
  TOKENIZED[2]: this . VAR1
  ORIGINAL[3]: zzMarkedPos - zzStartRead
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 - VAR2
  ORIGINAL[4]: this.zzMarkedPos
  TYPE[4]: CALL
  TOKENIZED[4]: this . VAR1
  ORIGINAL[5]: this.zzStartRead
  TYPE[5]: CALL
  TOKENIZED[5]: this . VAR1
  ORIGINAL[6]: $obj4
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: $obj4

CENTER_NODE: 30064771945
FRAGMENT_COUNT: 3
  ORIGINAL[0]: String LexerScheme.ZZ_ROWMAP_PACKED_0 = \
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 . VAR3 = \
  ORIGINAL[1]: LexerScheme.ZZ_ROWMAP_PACKED_0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2
  ORIGINAL[2]: \
  TYPE[2]: CALL
  TOKENIZED[2]: \

CENTER_NODE: 30064771638
FRAGMENT_COUNT: 4
  ORIGINAL[0]: TriolcousSpintherism jynx_revibrational = new TriolcousSpintherism()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new TriolcousSpintherism()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: jynx_revibrational
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: jynx_revibrational
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771670
FRAGMENT_COUNT: 4
  ORIGINAL[0]: AsteropeNonconcludent mentoposterior_insuavity = new AsteropeNonconcludent()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new AsteropeNonconcludent()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: mentoposterior_insuavity
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: mentoposterior_insuavity
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719476920
FRAGMENT_COUNT: 5
  ORIGINAL[0]: char[] map = new char[0x10000]
  TYPE[0]: CALL
  TOKENIZED[0]: char[] VAR1 = new char[0x10000]
  ORIGINAL[1]: new char[0x10000]
  TYPE[1]: CALL
  TOKENIZED[1]: new char[0x10000]
  ORIGINAL[2]: map
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: map
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: map
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771941
FRAGMENT_COUNT: 4
  ORIGINAL[0]: int[] LexerScheme.ZZ_ROWMAP = zzUnpackRowMap()
  TYPE[0]: CALL
  TOKENIZED[0]: int[] VAR1 . VAR2 = FUN1 ( )
  ORIGINAL[1]: LexerScheme.ZZ_ROWMAP
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2
  ORIGINAL[2]: zzUnpackRowMap()
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( )
  ORIGINAL[3]: ZZ_ROWMAP
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719476902
FRAGMENT_COUNT: 6
  ORIGINAL[0]: this.zzAtEOF = true
  TYPE[0]: CALL
  TOKENIZED[0]: this . VAR1 = true
  ORIGINAL[1]: int length = parser.end(range) - parser.start(range)
  TYPE[1]: CALL
  TOKENIZED[1]: int VAR1 = VAR2 . FUN1 ( VAR3 ) - VAR2 . FUN2 ( VAR3 )
  ORIGINAL[2]: parser.end(range) - parser.start(range)
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 . FUN1 ( VAR2 ) - VAR1 . FUN2 ( VAR2 )
  ORIGINAL[3]: length
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: parser
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: length
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771508
FRAGMENT_COUNT: 5
  ORIGINAL[0]: zzNext == -1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 == -1
  ORIGINAL[1]: zzAttrL[zzState]
  TYPE[1]: CALL
  TOKENIZED[1]: zzAttrL[zzState]
  ORIGINAL[2]: this.zzState
  TYPE[2]: CALL
  TOKENIZED[2]: this . VAR1
  ORIGINAL[3]: zzState
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: this
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: this

CENTER_NODE: 30064771594
FRAGMENT_COUNT: 4
  ORIGINAL[0]: TinBonelet sickishness_touser = new TinBonelet()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new TinBonelet()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: sickishness_touser
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: sickishness_touser
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771936
FRAGMENT_COUNT: 4
  ORIGINAL[0]: int[] LexerScheme.ZZ_ACTION = zzUnpackAction()
  TYPE[0]: CALL
  TOKENIZED[0]: int[] VAR1 . VAR2 = FUN1 ( )
  ORIGINAL[1]: LexerScheme.ZZ_ACTION
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2
  ORIGINAL[2]: zzUnpackAction()
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( )
  ORIGINAL[3]: ZZ_ACTION
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477131
FRAGMENT_COUNT: 5
  ORIGINAL[0]: EmpathicallyPulverulently myotonic_leatheriness = new EmpathicallyPulverulently()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new EmpathicallyPulverulently()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: myotonic_leatheriness
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: myotonic_leatheriness
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: myotonic_leatheriness
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771964
FRAGMENT_COUNT: 2
  ORIGINAL[0]: String LexerScheme.ZZ_ATTRIBUTE_PACKED_0 = \
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 . VAR3 = \
  ORIGINAL[1]: LexerScheme.ZZ_ATTRIBUTE_PACKED_0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2

CENTER_NODE: 68719476818
FRAGMENT_COUNT: 5
  ORIGINAL[0]: zzUnpackTrans(ZZ_TRANS_PACKED_0, offset, result)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 )
  ORIGINAL[1]: result
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: offset
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: result
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: result
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640300
FRAGMENT_COUNT: 1
  ORIGINAL[0]: while (true)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: while ( true )

CENTER_NODE: 30064771924
FRAGMENT_COUNT: 3
  ORIGINAL[0]: int LexerScheme.YYINITIAL = 0
  TYPE[0]: CALL
  TOKENIZED[0]: int VAR1 . VAR2 = 0
  ORIGINAL[1]: LexerScheme.YYINITIAL
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2
  ORIGINAL[2]: YYINITIAL
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771653
FRAGMENT_COUNT: 3
  ORIGINAL[0]: prochordal_dogmatization.midtapRedistend(beal_sansar)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[1]: prochordal_dogmatization
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: beal_sansar
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771395
FRAGMENT_COUNT: 3
  ORIGINAL[0]: $obj5 = new Error(message)
  TYPE[0]: CALL
  TOKENIZED[0]: $obj5 = new FUN1 ( VAR1 )
  ORIGINAL[1]: new Error(message)
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( VAR1 )
  ORIGINAL[2]: $obj5
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: $obj5

CENTER_NODE: 30064771661
FRAGMENT_COUNT: 3
  ORIGINAL[0]: splanchnoptosis_alboranite.unfoughtAllotropic(assumptively_semiclosure)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[1]: splanchnoptosis_alboranite
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: assumptively_semiclosure
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771132
FRAGMENT_COUNT: 4
  ORIGINAL[0]: int l = packed.length()
  TYPE[0]: CALL
  TOKENIZED[0]: int VAR1 = VAR2 . FUN1 ( )
  ORIGINAL[1]: packed.length()
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . FUN1 ( )
  ORIGINAL[2]: l
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: packed
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771681
FRAGMENT_COUNT: 3
  ORIGINAL[0]: dehull_iodhydrin.sulfocarbimideCerite(syodicon_desoxymorphine)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[1]: dehull_iodhydrin
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: syodicon_desoxymorphine
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640286
FRAGMENT_COUNT: 2
  ORIGINAL[0]: number > yylength()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > FUN1 ( )
  ORIGINAL[1]: if (number > yylength())
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: if ( VAR1 > FUN1 ( ) )

CENTER_NODE: 30064771368
FRAGMENT_COUNT: 3
  ORIGINAL[0]: this.zzLexicalState
  TYPE[0]: CALL
  TOKENIZED[0]: this . VAR1
  ORIGINAL[1]: zzLexicalState
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: this
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: this

CENTER_NODE: 30064771104
FRAGMENT_COUNT: 3
  ORIGINAL[0]: int offset = 0
  TYPE[0]: CALL
  TOKENIZED[0]: int VAR1 = 0
  ORIGINAL[1]: offset
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: offset
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771930
FRAGMENT_COUNT: 3
  ORIGINAL[0]: String LexerScheme.ZZ_CMAP_PACKED = \
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 . VAR3 = \
  ORIGINAL[1]: LexerScheme.ZZ_CMAP_PACKED
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2
  ORIGINAL[2]: \
  TYPE[2]: CALL
  TOKENIZED[2]: \

CENTER_NODE: 68719476767
FRAGMENT_COUNT: 3
  ORIGINAL[0]: this.loofness_coassert = loofness_coassert
  TYPE[0]: CALL
  TOKENIZED[0]: this . VAR1 = VAR1
  ORIGINAL[1]: this.loofness_coassert
  TYPE[1]: CALL
  TOKENIZED[1]: this . VAR1
  ORIGINAL[2]: loofness_coassert
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771713
FRAGMENT_COUNT: 3
  ORIGINAL[0]: accrual_rhinocerian.laudificationAssoilment(donnert_transnihilation)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[1]: accrual_rhinocerian
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: donnert_transnihilation
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771148
FRAGMENT_COUNT: 3
  ORIGINAL[0]: int offset = 0
  TYPE[0]: CALL
  TOKENIZED[0]: int VAR1 = 0
  ORIGINAL[1]: offset
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: offset
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771954
FRAGMENT_COUNT: 2
  ORIGINAL[0]: int LexerScheme.ZZ_NO_MATCH = 1
  TYPE[0]: CALL
  TOKENIZED[0]: int VAR1 . VAR2 = 1
  ORIGINAL[1]: LexerScheme.ZZ_NO_MATCH
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2

CENTER_NODE: 68719477187
FRAGMENT_COUNT: 5
  ORIGINAL[0]: DaribahUnmoiled bedene_colitoxemia = new DaribahUnmoiled()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new DaribahUnmoiled()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: bedene_colitoxemia
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: bedene_colitoxemia
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: bedene_colitoxemia
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771683
FRAGMENT_COUNT: 3
  ORIGINAL[0]: TyposcriptPerigon unhated_hematopoietic = new TyposcriptPerigon()
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 = new FUN1 ( )
  ORIGINAL[1]: new TyposcriptPerigon()
  TYPE[1]: CALL
  TOKENIZED[1]: new FUN1 ( )
  ORIGINAL[2]: unhated_hematopoietic
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771951
FRAGMENT_COUNT: 3
  ORIGINAL[0]: String LexerScheme.ZZ_TRANS_PACKED_0 = \
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 VAR2 . VAR3 = \
  ORIGINAL[1]: LexerScheme.ZZ_TRANS_PACKED_0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2
  ORIGINAL[2]: \
  TYPE[2]: CALL
  TOKENIZED[2]: \

CENTER_NODE: 30064771585
FRAGMENT_COUNT: 3
  ORIGINAL[0]: philotechnic_deferentectomy.glacierSubmarginally(pugnacious_stickball)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . FUN1 ( VAR2 )
  ORIGINAL[1]: philotechnic_deferentectomy
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: pugnacious_stickball
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771576
FRAGMENT_COUNT: 3
  ORIGINAL[0]: new SarwanLaniate()
  TYPE[0]: CALL
  TOKENIZED[0]: new FUN1 ( )
  ORIGINAL[1]: crossbred_forhoo
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: crossbred_forhoo
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771628
FRAGMENT_COUNT: 3
  ORIGINAL[0]: new PseudoventricleEurasiatic()
  TYPE[0]: CALL
  TOKENIZED[0]: new FUN1 ( )
  ORIGINAL[1]: cobaltic_washaway
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: cobaltic_washaway
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771384
FRAGMENT_COUNT: 6
  ORIGINAL[0]: zzMarkedPos - zzStartRead
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 - VAR2
  ORIGINAL[1]: this.zzMarkedPos
  TYPE[1]: CALL
  TOKENIZED[1]: this . VAR1
  ORIGINAL[2]: this.zzStartRead
  TYPE[2]: CALL
  TOKENIZED[2]: this . VAR1
  ORIGINAL[3]: zzMarkedPos
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: this
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: this
  ORIGINAL[5]: this
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: this

CENTER_NODE: 47244640287
FRAGMENT_COUNT: 1
  ORIGINAL[0]: while (true)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: while ( true )

CENTER_NODE: 68719476867
FRAGMENT_COUNT: 4
  ORIGINAL[0]: new PrintStream(new FileOutputStream(holystoneImpliedness, false), true, \
  TYPE[0]: CALL
  TOKENIZED[0]: new FUN1 ( new FUN2 ( VAR1 , false ) , true , \
  ORIGINAL[1]: System.err
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2
  ORIGINAL[2]: err
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: System
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771921
FRAGMENT_COUNT: 2
  ORIGINAL[0]: int LexerScheme.ZZ_BUFFERSIZE = 2048
  TYPE[0]: CALL
  TOKENIZED[0]: int VAR1 . VAR2 = 2048
  ORIGINAL[1]: LexerScheme.ZZ_BUFFERSIZE
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2

