# Tokenized code fragments for 152111-v1.0.0-bad
# Total center nodes processed: 82
# Total code fragments found: 429

CENTER_NODE: 30064771718
FRAGMENT_COUNT: 4
  ORIGINAL[0]: svn_stringbuf_appendbytes(targetstr,cstr,strlen(cstr))
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , FUN2 ( VAR2 ) )
  ORIGINAL[1]: strlen(cstr)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )
  ORIGINAL[2]: targetstr
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: cstr
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771319
FRAGMENT_COUNT: 4
  ORIGINAL[0]: _s_z_ > _m_b_f_ -> size
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > VAR2 -> VAR3
  ORIGINAL[1]: memset(_m_b_f_ -> data,0,_s_z_)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 -> VAR2 , 0 , VAR3 )
  ORIGINAL[2]: _m_b_f_ -> data
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: _s_z_
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771747
FRAGMENT_COUNT: 11
  ORIGINAL[0]: bytes + count > (str -> data) && bytes < (str -> data + str -> blocksize)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 + VAR2 > ( VAR3 -> VAR4 ) && VAR1 < ( VAR3 -> VAR4 + VAR3 -> VAR5 )
  ORIGINAL[1]: str -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: str -> len - pos
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 - VAR3
  ORIGINAL[3]: str -> len
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: str -> len
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: len
  TYPE[5]: FIELD_IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: str
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: str
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: pos
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: str
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: str
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1

CENTER_NODE: 68719477003
FRAGMENT_COUNT: 3
  ORIGINAL[0]: i != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != 0
  ORIGINAL[1]: len
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: len
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771978
FRAGMENT_COUNT: 10
  ORIGINAL[0]: ( *p) == 10
  TYPE[0]: CALL
  TOKENIZED[0]: ( *p ) == 10
  ORIGINAL[1]: ( *p) == 13
  TYPE[1]: CALL
  TOKENIZED[1]: ( *p ) == 13
  ORIGINAL[2]: *p
  TYPE[2]: CALL
  TOKENIZED[2]: *p
  ORIGINAL[3]: count++
  TYPE[3]: CALL
  TOKENIZED[3]: count++
  ORIGINAL[4]: ( *(p + 1)) == 10
  TYPE[4]: CALL
  TOKENIZED[4]: ( * ( VAR1 + 1 ) ) == 10
  ORIGINAL[5]: *(p + 1)
  TYPE[5]: CALL
  TOKENIZED[5]: * ( VAR1 + 1 )
  ORIGINAL[6]: p + 1
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 + 1
  ORIGINAL[7]: p
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: count
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: p
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 30064771858
FRAGMENT_COUNT: 5
  ORIGINAL[0]: find_char_backward((str -> data),str -> len,ch)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( ( VAR1 -> VAR2 ) , VAR1 -> VAR3 , VAR4 )
  ORIGINAL[1]: str -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: data
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: str
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477085
FRAGMENT_COUNT: 7
  ORIGINAL[0]: new_string -> data = ((char *)mem) + sizeof(( *new_string))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = ( ( char * ) VAR3 ) + sizeof ( ( *new_string ) )
  ORIGINAL[1]: new_string -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: *new_string
  TYPE[2]: CALL
  TOKENIZED[2]: *new_string
  ORIGINAL[3]: new_string -> data
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: data
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: new_string
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: new_string
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 47244640361
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477628
FRAGMENT_COUNT: 6
  ORIGINAL[0]: svn_ctype_casecmp(a,b)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[1]: cmp || !a || !b
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 || !a || !b
  ORIGINAL[2]: b
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: a
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: b
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: b
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477481
FRAGMENT_COUNT: 4
  ORIGINAL[0]: original_string -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: original_string -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: original_string
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477657
FRAGMENT_COUNT: 3
  ORIGINAL[0]: svn_cstring_strtoui64(n,str,0,18446744073709551615UL,10)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , 0 , 18446744073709551615UL , 10 )
  ORIGINAL[1]: n
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: str
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771827
FRAGMENT_COUNT: 5
  ORIGINAL[0]: string_compare((str1 -> data),(str2 -> data),str1 -> len,str2 -> len)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( ( VAR1 -> VAR2 ) , ( VAR3 -> VAR2 ) , VAR1 -> VAR4 , VAR3 -> VAR4 )
  ORIGINAL[1]: str1 -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str1
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: str2
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477541
FRAGMENT_COUNT: 5
  ORIGINAL[0]: svn_cstring_split_append(a,input,sep_chars,chop_whitespace,pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 , VAR4 , VAR5 )
  ORIGINAL[1]: input
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: a
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: input
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: sep_chars
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476951
FRAGMENT_COUNT: 4
  ORIGINAL[0]: membuf_ensure(&membuf -> data,&membuf -> size,size,membuf -> pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( &membuf -> VAR1 , &membuf -> VAR2 , VAR2 , VAR3 -> VAR4 )
  ORIGINAL[1]: &membuf -> size
  TYPE[1]: CALL
  TOKENIZED[1]: &membuf -> VAR1
  ORIGINAL[2]: size
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: membuf
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772107
FRAGMENT_COUNT: 4
  ORIGINAL[0]: *n = ((int )val)
  TYPE[0]: CALL
  TOKENIZED[0]: *n = ( ( int ) VAR1 )
  ORIGINAL[1]: *n
  TYPE[1]: CALL
  TOKENIZED[1]: *n
  ORIGINAL[2]: (int )val
  TYPE[2]: CALL
  TOKENIZED[2]: ( int ) VAR1
  ORIGINAL[3]: n
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719476798
FRAGMENT_COUNT: 4
  ORIGINAL[0]: fprintf(stdout, \
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , \
  ORIGINAL[1]: out_filename
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: stdout
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: shmid
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772472
FRAGMENT_COUNT: 56
  ORIGINAL[0]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))))))))))))))))))))))))))))))) != 0
  TYPE[0]: CALL
  TOKENIZED[0]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) != 0
  ORIGINAL[1]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))))))))))))))))))))))))))))))))))
  TYPE[1]: CALL
  TOKENIZED[1]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[2]: free(((char *)( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))))))))))))))))))))))))))))))))))
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( ( ( char * ) ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[3]: (char *)( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))))))))))))))))))))))))))))))))
  TYPE[3]: CALL
  TOKENIZED[3]: ( char * ) ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[4]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))))))))))))))))))))))))))))))))))
  TYPE[4]: CALL
  TOKENIZED[4]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[5]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))))))))))))))))))))))))))))))
  TYPE[5]: CALL
  TOKENIZED[5]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[6]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))))))))))))))))))))))))))))))))
  TYPE[6]: CALL
  TOKENIZED[6]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[7]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))))))))))))))))))))))))))))
  TYPE[7]: CALL
  TOKENIZED[7]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[8]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))))))))))))))))))))))))))))))
  TYPE[8]: CALL
  TOKENIZED[8]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[9]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))))))))))))))))))))))))))
  TYPE[9]: CALL
  TOKENIZED[9]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[10]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))))))))))))))))))))))))))))
  TYPE[10]: CALL
  TOKENIZED[10]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[11]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))))))))))))))))))))))))
  TYPE[11]: CALL
  TOKENIZED[11]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[12]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))))))))))))))))))))))))))
  TYPE[12]: CALL
  TOKENIZED[12]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[13]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))))))))))))))))))))))
  TYPE[13]: CALL
  TOKENIZED[13]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[14]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))))))))))))))))))))))))
  TYPE[14]: CALL
  TOKENIZED[14]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[15]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))))))))))))))))))))
  TYPE[15]: CALL
  TOKENIZED[15]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[16]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))))))))))))))))))))))
  TYPE[16]: CALL
  TOKENIZED[16]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[17]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))))))))))))))))))
  TYPE[17]: CALL
  TOKENIZED[17]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[18]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))))))))))))))))))))
  TYPE[18]: CALL
  TOKENIZED[18]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[19]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))))))))))))))))
  TYPE[19]: CALL
  TOKENIZED[19]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[20]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))))))))))))))))))
  TYPE[20]: CALL
  TOKENIZED[20]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[21]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))))))))))))))
  TYPE[21]: CALL
  TOKENIZED[21]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[22]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))))))))))))))))
  TYPE[22]: CALL
  TOKENIZED[22]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[23]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))))))))))))
  TYPE[23]: CALL
  TOKENIZED[23]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[24]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))))))))))))))
  TYPE[24]: CALL
  TOKENIZED[24]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[25]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))))))))))
  TYPE[25]: CALL
  TOKENIZED[25]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[26]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))))))))))))
  TYPE[26]: CALL
  TOKENIZED[26]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[27]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))))))))
  TYPE[27]: CALL
  TOKENIZED[27]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[28]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))))))))))
  TYPE[28]: CALL
  TOKENIZED[28]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[29]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))))))
  TYPE[29]: CALL
  TOKENIZED[29]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[30]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))))))))
  TYPE[30]: CALL
  TOKENIZED[30]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[31]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))))
  TYPE[31]: CALL
  TOKENIZED[31]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[32]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))))))
  TYPE[32]: CALL
  TOKENIZED[32]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[33]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))))
  TYPE[33]: CALL
  TOKENIZED[33]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[34]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))))
  TYPE[34]: CALL
  TOKENIZED[34]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[35]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))))
  TYPE[35]: CALL
  TOKENIZED[35]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[36]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))))
  TYPE[36]: CALL
  TOKENIZED[36]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[37]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))))
  TYPE[37]: CALL
  TOKENIZED[37]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[38]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))))
  TYPE[38]: CALL
  TOKENIZED[38]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[39]: *( *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))))
  TYPE[39]: CALL
  TOKENIZED[39]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[40]: *( *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))))
  TYPE[40]: CALL
  TOKENIZED[40]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[41]: *( *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))))
  TYPE[41]: CALL
  TOKENIZED[41]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[42]: *( *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))))
  TYPE[42]: CALL
  TOKENIZED[42]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) ) )
  ORIGINAL[43]: *( *( *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))))
  TYPE[43]: CALL
  TOKENIZED[43]: * ( * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) ) )
  ORIGINAL[44]: *( *( *( *( *( *( *( *( *( *heteromeral_infestious)))))))))
  TYPE[44]: CALL
  TOKENIZED[44]: * ( * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) ) )
  ORIGINAL[45]: *( *( *( *( *( *( *( *( *heteromeral_infestious))))))))
  TYPE[45]: CALL
  TOKENIZED[45]: * ( * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) ) )
  ORIGINAL[46]: *( *( *( *( *( *( *( *heteromeral_infestious)))))))
  TYPE[46]: CALL
  TOKENIZED[46]: * ( * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) ) )
  ORIGINAL[47]: *( *( *( *( *( *( *heteromeral_infestious))))))
  TYPE[47]: CALL
  TOKENIZED[47]: * ( * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) ) )
  ORIGINAL[48]: *( *( *( *( *( *heteromeral_infestious)))))
  TYPE[48]: CALL
  TOKENIZED[48]: * ( * ( * ( * ( * ( *heteromeral_infestious ) ) ) ) )
  ORIGINAL[49]: *( *( *( *( *heteromeral_infestious))))
  TYPE[49]: CALL
  TOKENIZED[49]: * ( * ( * ( * ( *heteromeral_infestious ) ) ) )
  ORIGINAL[50]: *( *( *( *heteromeral_infestious)))
  TYPE[50]: CALL
  TOKENIZED[50]: * ( * ( * ( *heteromeral_infestious ) ) )
  ORIGINAL[51]: *( *( *heteromeral_infestious))
  TYPE[51]: CALL
  TOKENIZED[51]: * ( * ( *heteromeral_infestious ) )
  ORIGINAL[52]: *( *heteromeral_infestious)
  TYPE[52]: CALL
  TOKENIZED[52]: * ( *heteromeral_infestious )
  ORIGINAL[53]: *heteromeral_infestious
  TYPE[53]: CALL
  TOKENIZED[53]: *heteromeral_infestious
  ORIGINAL[54]: stonesoup_close_printf_context()
  TYPE[54]: CALL
  TOKENIZED[54]: FUN1 ( )
  ORIGINAL[55]: heteromeral_infestious
  TYPE[55]: IDENTIFIER
  TOKENIZED[55]: VAR1

CENTER_NODE: 68719477820
FRAGMENT_COUNT: 11
  ORIGINAL[0]: strb + lenb
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 + VAR2
  ORIGINAL[1]: strb
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: endb
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: strb
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: lenb
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: strb
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: strb
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: strb
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: strb
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: strb
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: strb
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1

CENTER_NODE: 30064771281
FRAGMENT_COUNT: 4
  ORIGINAL[0]: membuf_create(&membuf -> data,&membuf -> size,size,pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( &membuf -> VAR1 , &membuf -> VAR2 , VAR2 , VAR3 )
  ORIGINAL[1]: &membuf -> size
  TYPE[1]: CALL
  TOKENIZED[1]: &membuf -> VAR1
  ORIGINAL[2]: membuf -> size
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: size
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772100
FRAGMENT_COUNT: 2
  ORIGINAL[0]: - 9223372036854775807L - 1
  TYPE[0]: CALL
  TOKENIZED[0]: - 9223372036854775807L - 1
  ORIGINAL[1]: - 9223372036854775807L
  TYPE[1]: CALL
  TOKENIZED[1]: - 9223372036854775807L

CENTER_NODE: 68719476924
FRAGMENT_COUNT: 3
  ORIGINAL[0]: *size
  TYPE[0]: CALL
  TOKENIZED[0]: *size
  ORIGINAL[1]: minimum_size
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: size
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771847
FRAGMENT_COUNT: 6
  ORIGINAL[0]: str -> len > 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 > 0
  ORIGINAL[1]: (unsigned char )str -> data[str -> len - 1]
  TYPE[1]: CALL
  TOKENIZED[1]: ( unsigned char ) VAR1 -> VAR2 [ VAR1 -> VAR3 - 1 ]
  ORIGINAL[2]: str -> data[str -> len - 1]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 [ VAR1 -> VAR3 - 1 ]
  ORIGINAL[3]: str -> data
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: str -> len - 1
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2 - 1
  ORIGINAL[5]: str -> data
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2

CENTER_NODE: 68719476895
FRAGMENT_COUNT: 5
  ORIGINAL[0]: ptrs[i] != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ VAR2 ] != 0
  ORIGINAL[1]: ptrs[i]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ VAR2 ]
  ORIGINAL[2]: ptrs[i]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ VAR2 ]
  ORIGINAL[3]: ptrs
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: i
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477654
FRAGMENT_COUNT: 3
  ORIGINAL[0]: *__errno_location() == 34 && (val == - 9223372036854775807L - 1 || val == 9223372036854775807L) || val < 0 || ((apr_uint64_t )val) < minval || ((apr_uint64_t )val) > maxval
  TYPE[0]: CALL
  TOKENIZED[0]: *__errno_location ( ) == 34 && ( VAR1 == - 9223372036854775807L - 1 || VAR1 == 9223372036854775807L ) || VAR1 < 0 || ( ( VAR2 ) VAR1 ) < VAR3 || ( ( VAR2 ) VAR1 ) > VAR4
  ORIGINAL[1]: *n
  TYPE[1]: CALL
  TOKENIZED[1]: *n
  ORIGINAL[2]: n
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640357
FRAGMENT_COUNT: 1
  ORIGINAL[0]: for (;;)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: for ( ; ; )

CENTER_NODE: 68719476882
FRAGMENT_COUNT: 9
  ORIGINAL[0]: *stonesoup_s != (char)0
  TYPE[0]: CALL
  TOKENIZED[0]: *stonesoup_s != ( char ) 0
  ORIGINAL[1]: *stonesoup_s
  TYPE[1]: CALL
  TOKENIZED[1]: *stonesoup_s
  ORIGINAL[2]: *stonesoup_tainted_buff
  TYPE[2]: CALL
  TOKENIZED[2]: *stonesoup_tainted_buff
  ORIGINAL[3]: stonesoup_s - stonesoup_shm
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 - VAR2
  ORIGINAL[4]: stonesoup_s
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: stonesoup_s
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: stonesoup_s
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: stonesoup_shm
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: stonesoup_s
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 68719477664
FRAGMENT_COUNT: 4
  ORIGINAL[0]: &val
  TYPE[0]: CALL
  TOKENIZED[0]: &val
  ORIGINAL[1]: (unsigned int )val
  TYPE[1]: CALL
  TOKENIZED[1]: ( unsigned int ) VAR1
  ORIGINAL[2]: val
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: val
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477781
FRAGMENT_COUNT: 8
  ORIGINAL[0]: buffer[2 * 21]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ 2 * 21 ]
  ORIGINAL[1]: buffer[2 * 21]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ 2 * 21 ]
  ORIGINAL[2]: buffer[1]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ 1 ]
  ORIGINAL[3]: buffer
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: buffer
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: buffer
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: buffer
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: buffer
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 47244640365
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771433
FRAGMENT_COUNT: 4
  ORIGINAL[0]: *strbuf = svn_stringbuf_create_ensure(size,pool)
  TYPE[0]: CALL
  TOKENIZED[0]: *strbuf = FUN1 ( VAR1 , VAR2 )
  ORIGINAL[1]: svn_stringbuf_create_ensure(size,pool)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[2]: strbuf
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: strbuf
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771484
FRAGMENT_COUNT: 2
  ORIGINAL[0]: str -> len = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = 0
  ORIGINAL[1]: str -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2

CENTER_NODE: 47244640341
FRAGMENT_COUNT: 1
  ORIGINAL[0]: apr_fnmatch(this_pattern,str,0) == 0
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , 0 ) == 0

CENTER_NODE: 30064771164
FRAGMENT_COUNT: 3
  ORIGINAL[0]: retval == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 == 0
  ORIGINAL[1]: strlen(dirpath) + strlen(\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 ) + FUN1 ( \
  ORIGINAL[2]: strlen(\
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( \

CENTER_NODE: 30064771941
FRAGMENT_COUNT: 6
  ORIGINAL[0]: csep == '\\0'
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 == '\\0'
  ORIGINAL[1]: sep[1] != '\\0'
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ 1 ] != '\\0'
  ORIGINAL[2]: sep[1]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ 1 ]
  ORIGINAL[3]: sep
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: sep
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: sep
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477804
FRAGMENT_COUNT: 3
  ORIGINAL[0]: svn_string__similarity((&stringa),(&stringb),buffer,rlcs)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( ( &stringa ) , ( &stringb ) , VAR1 , VAR2 )
  ORIGINAL[1]: buffer
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: rlcs
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477140
FRAGMENT_COUNT: 4
  ORIGINAL[0]: strlen(value)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 )
  ORIGINAL[1]: amt
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: value
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: value
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477048
FRAGMENT_COUNT: 4
  ORIGINAL[0]: *data = apr_pvsprintf(pool,fmt,ap)
  TYPE[0]: CALL
  TOKENIZED[0]: *data = FUN1 ( VAR1 , VAR2 , VAR3 )
  ORIGINAL[1]: create_string(data,strlen(data),pool)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , FUN2 ( VAR1 ) , VAR2 )
  ORIGINAL[2]: data
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: data
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771240
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (c >= 97 && c <= 122) || (c >= 65 && c <= 90) || (c >= 48 && c <= 57)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 >= 97 && VAR1 <= 122 ) || ( VAR1 >= 65 && VAR1 <= 90 ) || ( VAR1 >= 48 && VAR1 <= 57 )
  ORIGINAL[1]: (c >= 97 && c <= 122) || (c >= 65 && c <= 90)
  TYPE[1]: CALL
  TOKENIZED[1]: ( VAR1 >= 97 && VAR1 <= 122 ) || ( VAR1 >= 65 && VAR1 <= 90 )
  ORIGINAL[2]: c >= 48 && c <= 57
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 >= 48 && VAR1 <= 57
  ORIGINAL[3]: c >= 48
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 >= 48
  ORIGINAL[4]: c <= 57
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 <= 57

CENTER_NODE: 68719477069
FRAGMENT_COUNT: 3
  ORIGINAL[0]: str -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: data
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: str
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771398
FRAGMENT_COUNT: 5
  ORIGINAL[0]: string_first_non_whitespace(str -> data,str -> len)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , VAR1 -> VAR3 )
  ORIGINAL[1]: str -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: data
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: str
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476849
FRAGMENT_COUNT: 7
  ORIGINAL[0]: va_start(argptr, format)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[1]: vfprintf(stonesoup_printf_context, format, argptr)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 , VAR3 )
  ORIGINAL[2]: <global> stonesoup_printf_context
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: <global> VAR1
  ORIGINAL[3]: format
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: argptr
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: <global> stonesoup_printf_context
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: <global> VAR1
  ORIGINAL[6]: stonesoup_printf_context
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 68719477526
FRAGMENT_COUNT: 10
  ORIGINAL[0]: *p
  TYPE[0]: CALL
  TOKENIZED[0]: *p
  ORIGINAL[1]: e >= p && 0 != (svn_ctype_table[(unsigned char )( *e)] & 0x0002)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 >= VAR2 && 0 != ( VAR3 [ ( unsigned char ) ( *e ) ] & 0x0002 )
  ORIGINAL[2]: e >= p
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 >= VAR2
  ORIGINAL[3]: p[0]
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 [ 0 ]
  ORIGINAL[4]: chop_whitespace
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: p
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: e
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: p
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: p
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: p
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 47244640303
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640374
FRAGMENT_COUNT: 4
  ORIGINAL[0]: i = length
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = VAR2
  ORIGINAL[1]: i > 3
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 > 3
  ORIGINAL[2]: i -= 3
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -= 3
  ORIGINAL[3]: for (i = length;i > 3;i -= 3)
  TYPE[3]: CONTROL_STRUCTURE
  TOKENIZED[3]: for ( VAR1 = VAR2 ; VAR1 > 3 ; VAR1 -= 3 )

CENTER_NODE: 30064771470
FRAGMENT_COUNT: 6
  ORIGINAL[0]: memset((str -> data),c,str -> len)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( ( VAR1 -> VAR2 ) , VAR3 , VAR1 -> VAR4 )
  ORIGINAL[1]: str -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: str -> len
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: len
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: c
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: str
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771444
FRAGMENT_COUNT: 4
  ORIGINAL[0]: svn_stringbuf_ncreate(str -> data,str -> len,pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , VAR1 -> VAR3 , VAR4 )
  ORIGINAL[1]: str -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: str -> len
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: pool
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477428
FRAGMENT_COUNT: 4
  ORIGINAL[0]: str -> len -= count
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -= VAR3
  ORIGINAL[1]: str -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: count
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: count
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771393
FRAGMENT_COUNT: 5
  ORIGINAL[0]: string_compare(str1 -> data,str2 -> data,str1 -> len,str2 -> len)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , VAR3 -> VAR2 , VAR1 -> VAR4 , VAR3 -> VAR4 )
  ORIGINAL[1]: str1 -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: data
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str1
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: str2
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477151
FRAGMENT_COUNT: 5
  ORIGINAL[0]: nbytes > str -> len
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > VAR2 -> VAR3
  ORIGINAL[1]: str -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: nbytes
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: nbytes
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477158
FRAGMENT_COUNT: 3
  ORIGINAL[0]: str -> len
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: len
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: str
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771527
FRAGMENT_COUNT: 7
  ORIGINAL[0]: str -> blocksize > old_len + 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 > VAR3 + 1
  ORIGINAL[1]: dest[old_len] = byte
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ VAR2 ] = VAR3
  ORIGINAL[2]: dest[old_len]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ VAR2 ]
  ORIGINAL[3]: dest
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: old_len
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: byte
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: dest
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 68719476969
FRAGMENT_COUNT: 3
  ORIGINAL[0]: *const _m_b_f_ = membuf
  TYPE[0]: CALL
  TOKENIZED[0]: *const VAR1 = VAR2
  ORIGINAL[1]: _m_b_f_
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: membuf
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771244
FRAGMENT_COUNT: 4
  ORIGINAL[0]: tracepoint(stonesoup_trace, trace_location, \
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , \
  ORIGINAL[1]: stonesoup_trace
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: trace_location
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: index
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771388
FRAGMENT_COUNT: 4
  ORIGINAL[0]: str -> len == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == 0
  ORIGINAL[1]: str -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477012
FRAGMENT_COUNT: 7
  ORIGINAL[0]: *new_string = (apr_palloc(pool,sizeof(( *new_string))))
  TYPE[0]: CALL
  TOKENIZED[0]: *new_string = ( FUN1 ( VAR1 , sizeof ( ( *new_string ) ) ) )
  ORIGINAL[1]: apr_palloc(pool,sizeof(( *new_string)))
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , sizeof ( ( *new_string ) ) )
  ORIGINAL[2]: new_string
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: pool
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: new_string
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: new_string
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: new_string
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 30064771862
FRAGMENT_COUNT: 5
  ORIGINAL[0]: string_compare(str1 -> data,(str2 -> data),str1 -> len,str2 -> len)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , ( VAR3 -> VAR2 ) , VAR1 -> VAR4 , VAR3 -> VAR4 )
  ORIGINAL[1]: str2 -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: data
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str2
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: str1
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771253
FRAGMENT_COUNT: 5
  ORIGINAL[0]: minimum_size = minimum_size + (8 - 1) & (~(8 - 1))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = VAR1 + ( 8 - 1 ) & ( ~ ( 8 - 1 ) )
  ORIGINAL[1]: minimum_size + (8 - 1) & (~(8 - 1))
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 + ( 8 - 1 ) & ( ~ ( 8 - 1 ) )
  ORIGINAL[2]: minimum_size + (8 - 1)
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 + ( 8 - 1 )
  ORIGINAL[3]: ~(8 - 1)
  TYPE[3]: CALL
  TOKENIZED[3]: ~ ( 8 - 1 )
  ORIGINAL[4]: minimum_size
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477487
FRAGMENT_COUNT: 3
  ORIGINAL[0]: str -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: data
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: str
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064772083
FRAGMENT_COUNT: 8
  ORIGINAL[0]: *__errno_location() == 22 || endptr == str || str[0] == '\\0' || ( *endptr) != '\\0'
  TYPE[0]: CALL
  TOKENIZED[0]: *__errno_location ( ) == 22 || VAR1 == VAR2 || VAR2 [ 0 ] == '\\0' || ( *endptr ) != '\\0'
  ORIGINAL[1]: *__errno_location() == 34 && (val == - 9223372036854775807L - 1 || val == 9223372036854775807L) || val < minval
  TYPE[1]: CALL
  TOKENIZED[1]: *__errno_location ( ) == 34 && ( VAR1 == - 9223372036854775807L - 1 || VAR1 == 9223372036854775807L ) || VAR1 < VAR2
  ORIGINAL[2]: *__errno_location() == 34 && (val == - 9223372036854775807L - 1 || val == 9223372036854775807L)
  TYPE[2]: CALL
  TOKENIZED[2]: *__errno_location ( ) == 34 && ( VAR1 == - 9223372036854775807L - 1 || VAR1 == 9223372036854775807L )
  ORIGINAL[3]: *__errno_location() == 34
  TYPE[3]: CALL
  TOKENIZED[3]: *__errno_location ( ) == 34
  ORIGINAL[4]: val == - 9223372036854775807L - 1 || val == 9223372036854775807L
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 == - 9223372036854775807L - 1 || VAR1 == 9223372036854775807L
  ORIGINAL[5]: val < minval
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 < VAR2
  ORIGINAL[6]: val
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: minval
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 68719476964
FRAGMENT_COUNT: 9
  ORIGINAL[0]: membuf -> size
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: membuf -> size
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: membuf -> pool
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: membuf -> data && old_data
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2 && VAR3
  ORIGINAL[4]: membuf -> data
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: membuf -> data
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: data
  TYPE[6]: FIELD_IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: old_data
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: membuf
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 30064771464
FRAGMENT_COUNT: 4
  ORIGINAL[0]: __builtin_va_start(ap,fmt)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[1]: ap
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: fmt
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719476994
FRAGMENT_COUNT: 3
  ORIGINAL[0]: !(0 != (svn_ctype_table[(unsigned char )str[i]] & 0x0002))
  TYPE[0]: CALL
  TOKENIZED[0]: ! ( 0 != ( VAR1 [ ( unsigned char ) VAR2 [ VAR3 ] ] & 0x0002 ) )
  ORIGINAL[1]: i
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: i
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477007
FRAGMENT_COUNT: 6
  ORIGINAL[0]: new_string = (apr_palloc(pool,sizeof(( *new_string))))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = ( FUN1 ( VAR2 , sizeof ( ( *new_string ) ) ) )
  ORIGINAL[1]: *new_string
  TYPE[1]: CALL
  TOKENIZED[1]: *new_string
  ORIGINAL[2]: new_string -> data
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: data
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: new_string
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: new_string
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477061
FRAGMENT_COUNT: 4
  ORIGINAL[0]: original_string -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: original_string -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: original_string
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477173
FRAGMENT_COUNT: 7
  ORIGINAL[0]: &mem
  TYPE[0]: CALL
  TOKENIZED[0]: &mem
  ORIGINAL[1]: mem && mem != (str -> data)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 && VAR1 != ( VAR2 -> VAR3 )
  ORIGINAL[2]: str -> data = mem
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 = VAR3
  ORIGINAL[3]: str -> data
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: mem
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: mem
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: mem
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 68719477073
FRAGMENT_COUNT: 2
  ORIGINAL[0]: svn_stringbuf_create_ensure(0,pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( 0 , VAR1 )
  ORIGINAL[1]: pool
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1

CENTER_NODE: 30064771322
FRAGMENT_COUNT: 4
  ORIGINAL[0]: len1 != len2
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != VAR2
  ORIGINAL[1]: memcmp(str1,str2,len1) == 0
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 , VAR3 ) == 0
  ORIGINAL[2]: memcmp(str1,str2,len1)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , VAR2 , VAR3 )
  ORIGINAL[3]: !0
  TYPE[3]: CALL
  TOKENIZED[3]: !0

CENTER_NODE: 68719477040
FRAGMENT_COUNT: 3
  ORIGINAL[0]: svn_string_ncreate(cstring,strlen(cstring),pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , FUN2 ( VAR1 ) , VAR2 )
  ORIGINAL[1]: strlen(cstring)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )
  ORIGINAL[2]: pool
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477371
FRAGMENT_COUNT: 4
  ORIGINAL[0]: appendstr -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: appendstr -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: appendstr
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477720
FRAGMENT_COUNT: 6
  ORIGINAL[0]: number >= 100000000
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= 100000000
  ORIGINAL[1]: reduced % 100
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 % 100
  ORIGINAL[2]: reduced
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: <global> decimal_table
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: <global> VAR1
  ORIGINAL[4]: reduced
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: reduced
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 47244640302
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477042
FRAGMENT_COUNT: 4
  ORIGINAL[0]: strbuf -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: strbuf -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: strbuf
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771365
FRAGMENT_COUNT: 6
  ORIGINAL[0]: sizeof(( *new_string))
  TYPE[0]: CALL
  TOKENIZED[0]: sizeof ( ( *new_string ) )
  ORIGINAL[1]: *new_string
  TYPE[1]: CALL
  TOKENIZED[1]: *new_string
  ORIGINAL[2]: new_string
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: new_string
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: new_string
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: new_string
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771452
FRAGMENT_COUNT: 5
  ORIGINAL[0]: new_string = (apr_palloc(pool,sizeof(( *new_string))))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = ( FUN1 ( VAR2 , sizeof ( ( *new_string ) ) ) )
  ORIGINAL[1]: apr_palloc(pool,sizeof(( *new_string)))
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , sizeof ( ( *new_string ) ) )
  ORIGINAL[2]: sizeof(( *new_string))
  TYPE[2]: CALL
  TOKENIZED[2]: sizeof ( ( *new_string ) )
  ORIGINAL[3]: new_string
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: pool
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477051
FRAGMENT_COUNT: 4
  ORIGINAL[0]: __builtin_va_start(ap,fmt)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[1]: ap
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: fmt
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: ap
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477559
FRAGMENT_COUNT: 5
  ORIGINAL[0]: i < list -> nelts
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2 -> VAR3
  ORIGINAL[1]: *this_str = ((char **)(list -> elts))[i]
  TYPE[1]: CALL
  TOKENIZED[1]: *this_str = ( ( char ** ) ( VAR1 -> VAR2 ) ) [ VAR3 ]
  ORIGINAL[2]: ((char **)(list -> elts))[i]
  TYPE[2]: CALL
  TOKENIZED[2]: ( ( char ** ) ( VAR1 -> VAR2 ) ) [ VAR3 ]
  ORIGINAL[3]: this_str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: this_str
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477621
FRAGMENT_COUNT: 6
  ORIGINAL[0]: i < strings -> nelts
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2 -> VAR3
  ORIGINAL[1]: new_str -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: data
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: new_str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: new_str
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: new_str
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477779
FRAGMENT_COUNT: 4
  ORIGINAL[0]: ui64toa_sep(number,seperator,buffer)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 )
  ORIGINAL[1]: apr_pstrdup(pool,buffer)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[2]: pool
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: buffer
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477751
FRAGMENT_COUNT: 4
  ORIGINAL[0]: number >= 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= 0
  ORIGINAL[1]: (apr_uint64_t )number
  TYPE[1]: CALL
  TOKENIZED[1]: ( VAR1 ) VAR2
  ORIGINAL[2]: number
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: number
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477106
FRAGMENT_COUNT: 3
  ORIGINAL[0]: svn_stringbuf_ncreate(cstring,strlen(cstring),pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , FUN2 ( VAR1 ) , VAR2 )
  ORIGINAL[1]: strlen(cstring)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )
  ORIGINAL[2]: pool
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771405
FRAGMENT_COUNT: 4
  ORIGINAL[0]: &strbuf -> data
  TYPE[0]: CALL
  TOKENIZED[0]: &strbuf -> VAR1
  ORIGINAL[1]: strbuf -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: data
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: strbuf
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477453
FRAGMENT_COUNT: 7
  ORIGINAL[0]: pos + old_count > str -> len
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 + VAR2 > VAR3 -> VAR4
  ORIGINAL[1]: str -> len - pos
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 - VAR3
  ORIGINAL[2]: str -> len
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: pos
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: pos
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: pos
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: pos
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

