# Tokenized code fragments for 152176-v1.0.0-bad
# Total center nodes processed: 42
# Total code fragments found: 143

CENTER_NODE: 30064771080
FRAGMENT_COUNT: 4
  ORIGINAL[0]: ss_tc_root = getenv(\
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = FUN1 ( \
  ORIGINAL[1]: getenv(\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( \
  ORIGINAL[2]: ss_tc_root
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: ss_tc_root
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771488
FRAGMENT_COUNT: 2
  ORIGINAL[0]: ((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1)
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 )
  ORIGINAL[1]: 8 - 1
  TYPE[1]: CALL
  TOKENIZED[1]: 8 - 1

CENTER_NODE: 30064771303
FRAGMENT_COUNT: 8
  ORIGINAL[0]: !hashp -> hctl
  TYPE[0]: CALL
  TOKENIZED[0]: !hashp -> VAR1
  ORIGINAL[1]: hashp -> hctl = ((HASHHDR *)((hashp -> alloc)(sizeof(HASHHDR ))))
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 = ( ( VAR3 * ) ( ( VAR1 -> VAR4 ) ( sizeof ( VAR3 ) ) ) )
  ORIGINAL[2]: hashp -> hctl
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: (HASHHDR *)((hashp -> alloc)(sizeof(HASHHDR )))
  TYPE[3]: CALL
  TOKENIZED[3]: ( VAR1 * ) ( ( VAR2 -> VAR3 ) ( sizeof ( VAR1 ) ) )
  ORIGINAL[4]: hashp -> hctl
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hctl
  TYPE[5]: FIELD_IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 47244640304
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477113
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hashp -> hash
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hash
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: hashp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477302
FRAGMENT_COUNT: 8
  ORIGINAL[0]: curBucket > max_bucket
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > VAR2
  ORIGINAL[1]: segment_num = (curBucket >> hashp -> sshift)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = ( VAR2 >> VAR3 -> VAR4 )
  ORIGINAL[2]: segment_ndx = curBucket & ssize - 1
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = VAR2 & VAR3 - 1
  ORIGINAL[3]: curBucket & ssize - 1
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 & VAR2 - 1
  ORIGINAL[4]: segment_ndx
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: curBucket
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: segment_ndx
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: segment_ndx
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 30064772431
FRAGMENT_COUNT: 3
  ORIGINAL[0]: pamphletic_buccinator = 29
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = 29
  ORIGINAL[1]: pamphletic_buccinator
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: <global> stonesoup_global_variable
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: <global> VAR1

CENTER_NODE: 68719476825
FRAGMENT_COUNT: 3
  ORIGINAL[0]: keysize - 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 - 1
  ORIGINAL[1]: key2
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: keysize
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640374
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772123
FRAGMENT_COUNT: 7
  ORIGINAL[0]: new_segnum >= hctl -> nsegs
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= VAR2 -> VAR3
  ORIGINAL[1]: !(hashp -> dir[new_segnum] = seg_alloc(hashp))
  TYPE[1]: CALL
  TOKENIZED[1]: ! ( VAR1 -> VAR2 [ VAR3 ] = FUN1 ( VAR1 ) )
  ORIGINAL[2]: old_segndx = old_bucket & hashp -> ssize - 1
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = VAR2 & VAR3 -> VAR4 - 1
  ORIGINAL[3]: old_bucket & hashp -> ssize - 1
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 & VAR2 -> VAR3 - 1
  ORIGINAL[4]: hashp -> ssize - 1
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2 - 1
  ORIGINAL[5]: old_segndx
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: old_bucket
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 68719477545
FRAGMENT_COUNT: 2
  ORIGINAL[0]: my_log2(num)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 )
  ORIGINAL[1]: num
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1

CENTER_NODE: 30064771649
FRAGMENT_COUNT: 3
  ORIGINAL[0]: (num_entries - 1) / 1
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 - 1 ) / 1
  ORIGINAL[1]: num_entries - 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 - 1
  ORIGINAL[2]: num_entries
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640394
FRAGMENT_COUNT: 1
  ORIGINAL[0]: num > (2147483647 / 2)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > ( 2147483647 / 2 )

CENTER_NODE: 68719477677
FRAGMENT_COUNT: 5
  ORIGINAL[0]: stonesoup_is_valid == 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 == 1
  ORIGINAL[1]: popen(stonesoup_cmd_string,\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , \
  ORIGINAL[2]: stonesoup_cmd_string
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: stonesoup_fpipe
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: stonesoup_cmd_string
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477122
FRAGMENT_COUNT: 8
  ORIGINAL[0]: bucket = hash_val & hctl -> high_mask
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = VAR2 & VAR3 -> VAR4
  ORIGINAL[1]: bucket > hctl -> max_bucket
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 > VAR2 -> VAR3
  ORIGINAL[2]: hctl -> max_bucket
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: bucket
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: bucket
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hctl
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: bucket
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: bucket
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 47244640274
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771665
FRAGMENT_COUNT: 4
  ORIGINAL[0]: hashp != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: hash_stats(\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( \
  ORIGINAL[2]: hashp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719476819
FRAGMENT_COUNT: 4
  ORIGINAL[0]: *stonesoup_tainted_buff
  TYPE[0]: CALL
  TOKENIZED[0]: *stonesoup_tainted_buff
  ORIGINAL[1]: getenv(\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( \
  ORIGINAL[2]: *stonesoup_tainted_buff
  TYPE[2]: CALL
  TOKENIZED[2]: *stonesoup_tainted_buff
  ORIGINAL[3]: stonesoup_tainted_buff
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640346
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640387
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772519
FRAGMENT_COUNT: 1
  ORIGINAL[0]: *seq_scan_tables[100]
  TYPE[0]: CALL
  TOKENIZED[0]: *seq_scan_tables [ 100 ]

CENTER_NODE: 47244640393
FRAGMENT_COUNT: 2
  ORIGINAL[0]: limit < num
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: for ((i = 0 , limit = 1);limit < num;(i++ , limit <<= 1))
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: for ( ( VAR1 = 0 , VAR2 = 1 ) ; VAR2 < VAR3 ; ( i++ , VAR2 <<= 1 ) )

CENTER_NODE: 30064771970
FRAGMENT_COUNT: 5
  ORIGINAL[0]: status -> curEntry = ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = ( ( void * ) 0 )
  ORIGINAL[1]: status -> curEntry
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: (void *)0
  TYPE[2]: CALL
  TOKENIZED[2]: ( void * ) 0
  ORIGINAL[3]: curEntry
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: status
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771571
FRAGMENT_COUNT: 15
  ORIGINAL[0]: hctl -> nsegs < nsegs
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 < VAR2
  ORIGINAL[1]: hctl -> nsegs
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctl -> nsegs++
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> nsegs++
  ORIGINAL[3]: hctl -> nsegs
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: segp++
  TYPE[4]: CALL
  TOKENIZED[4]: segp++
  ORIGINAL[5]: *segp == ((void *)0)
  TYPE[5]: CALL
  TOKENIZED[5]: *segp == ( ( void * ) 0 )
  ORIGINAL[6]: *segp
  TYPE[6]: CALL
  TOKENIZED[6]: *segp
  ORIGINAL[7]: (void *)0
  TYPE[7]: CALL
  TOKENIZED[7]: ( void * ) 0
  ORIGINAL[8]: (bool )0
  TYPE[8]: CALL
  TOKENIZED[8]: ( VAR1 ) 0
  ORIGINAL[9]: nsegs
  TYPE[9]: FIELD_IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: nsegs
  TYPE[10]: FIELD_IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: hctl
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1
  ORIGINAL[12]: nsegs
  TYPE[12]: IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: hctl
  TYPE[13]: IDENTIFIER
  TOKENIZED[13]: VAR1
  ORIGINAL[14]: segp
  TYPE[14]: IDENTIFIER
  TOKENIZED[14]: VAR1

CENTER_NODE: 47244640379
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640375
FRAGMENT_COUNT: 2
  ORIGINAL[0]: (((intptr_t )_vstart) & sizeof(long ) - 1) == 0 && (_len & sizeof(long ) - 1) == 0 && _val == 0 && _len <= 1024 && 1024 != 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( ( VAR1 ) VAR2 ) & sizeof ( long ) - 1 ) == 0 && ( VAR3 & sizeof ( long ) - 1 ) == 0 && VAR4 == 0 && VAR3 <= 1024 && 1024 != 0
  ORIGINAL[1]: else
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: else

CENTER_NODE: 47244640331
FRAGMENT_COUNT: 1
  ORIGINAL[0]: action
  TYPE[0]: IDENTIFIER
  TOKENIZED[0]: VAR1

CENTER_NODE: 30064772567
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (shm = shmat(shmid, NULL, 0)) == (char *) -1
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 = FUN1 ( VAR2 , VAR3 , 0 ) ) == ( char * ) -1
  ORIGINAL[1]: strcpy(shm, str)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[2]: shm
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: out_filename
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771964
FRAGMENT_COUNT: 5
  ORIGINAL[0]: hashp -> hctl -> nentries
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[1]: hashp -> hctl
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctl
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: nentries
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477334
FRAGMENT_COUNT: 4
  ORIGINAL[0]: !status -> hashp -> frozen
  TYPE[0]: CALL
  TOKENIZED[0]: !status -> VAR1 -> VAR2
  ORIGINAL[1]: status -> hashp
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hashp
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: status
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477598
FRAGMENT_COUNT: 4
  ORIGINAL[0]: i >= 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= 0
  ORIGINAL[1]: seq_scan_level[i] >= nestDepth
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ VAR2 ] >= VAR3
  ORIGINAL[2]: seq_scan_level[i]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ VAR2 ]
  ORIGINAL[3]: nestDepth
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640344
FRAGMENT_COUNT: 1
  ORIGINAL[0]: for (;;)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: for ( ; ; )

CENTER_NODE: 68719477128
FRAGMENT_COUNT: 4
  ORIGINAL[0]: hash_search_with_hash_value(hashp,keyPtr,((hashp -> hash)(keyPtr,hashp -> keysize)),action,foundPtr)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , ( ( VAR1 -> VAR3 ) ( VAR2 , VAR1 -> VAR4 ) ) , VAR5 , VAR6 )
  ORIGINAL[1]: hashp -> keysize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hashp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: keyPtr
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640335
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640347
FRAGMENT_COUNT: 1
  ORIGINAL[0]: tas(&hctlv -> mutex)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( &hctlv -> VAR1 )

CENTER_NODE: 30064771658
FRAGMENT_COUNT: 3
  ORIGINAL[0]: sizeof(HASHHDR ) + (info -> dsize) * sizeof(HASHSEGMENT )
  TYPE[0]: CALL
  TOKENIZED[0]: sizeof ( VAR1 ) + ( VAR2 -> VAR3 ) * sizeof ( VAR4 )
  ORIGINAL[1]: sizeof(HASHHDR )
  TYPE[1]: CALL
  TOKENIZED[1]: sizeof ( VAR1 )
  ORIGINAL[2]: (info -> dsize) * sizeof(HASHSEGMENT )
  TYPE[2]: CALL
  TOKENIZED[2]: ( VAR1 -> VAR2 ) * sizeof ( VAR3 )

CENTER_NODE: 47244640380
FRAGMENT_COUNT: 2
  ORIGINAL[0]: (((intptr_t )_vstart) & sizeof(long ) - 1) == 0 && (_len & sizeof(long ) - 1) == 0 && _val == 0 && _len <= 1024 && 1024 != 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( ( VAR1 ) VAR2 ) & sizeof ( long ) - 1 ) == 0 && ( VAR3 & sizeof ( long ) - 1 ) == 0 && VAR4 == 0 && VAR3 <= 1024 && 1024 != 0
  ORIGINAL[1]: else
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: else

CENTER_NODE: 68719477074
FRAGMENT_COUNT: 3
  ORIGINAL[0]: sizeof(HASHSEGMENT )
  TYPE[0]: CALL
  TOKENIZED[0]: sizeof ( VAR1 )
  ORIGINAL[1]: nDirEntries
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: HASHSEGMENT
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064772295
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hashp -> isfixed
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: (intptr_t )(8 - 1)
  TYPE[1]: CALL
  TOKENIZED[1]: ( VAR1 ) ( 8 - 1 )
  ORIGINAL[2]: 8 - 1
  TYPE[2]: CALL
  TOKENIZED[2]: 8 - 1

CENTER_NODE: 30064772065
FRAGMENT_COUNT: 10
  ORIGINAL[0]: hashp -> isshared
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: elog_finish(20,\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( 20 , \
  ORIGINAL[2]: hashp -> tabname
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hashp -> tabname
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: tabname
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: hashp
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: hashp
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 68719476994
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hctl -> keysize
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: sizeof(char *)
  TYPE[1]: CALL
  TOKENIZED[1]: sizeof ( char * )
  ORIGINAL[2]: char
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: char

CENTER_NODE: 68719477535
FRAGMENT_COUNT: 4
  ORIGINAL[0]: hashp -> isshared
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hashp -> tabname
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: tabname
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

