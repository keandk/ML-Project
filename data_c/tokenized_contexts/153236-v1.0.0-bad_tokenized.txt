# Tokenized code fragments for 153236-v1.0.0-bad
# Total center nodes processed: 42
# Total code fragments found: 170

CENTER_NODE: 47244640340
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771101
FRAGMENT_COUNT: 5
  ORIGINAL[0]: stat(dirpath, &st) == -1
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , &st ) == -1
  ORIGINAL[1]: retval = mkdir(dirpath, 0700)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = FUN1 ( VAR2 , 0700 )
  ORIGINAL[2]: mkdir(dirpath, 0700)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , 0700 )
  ORIGINAL[3]: retval
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: dirpath
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771652
FRAGMENT_COUNT: 6
  ORIGINAL[0]: hash_search_with_hash_value(hashp,keyPtr,((hashp -> hash)(keyPtr,hashp -> keysize)),action,foundPtr)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , ( ( VAR1 -> VAR3 ) ( VAR2 , VAR1 -> VAR4 ) ) , VAR5 , VAR6 )
  ORIGINAL[1]: (hashp -> hash)(keyPtr,hashp -> keysize)
  TYPE[1]: CALL
  TOKENIZED[1]: ( VAR1 -> VAR2 ) ( VAR3 , VAR1 -> VAR4 )
  ORIGINAL[2]: hashp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: keyPtr
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: action
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: foundPtr
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771634
FRAGMENT_COUNT: 6
  ORIGINAL[0]: hashp != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: MemoryContextDelete(hashp -> hcxt)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 -> VAR2 )
  ORIGINAL[2]: hashp -> hcxt
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hcxt
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 47244640268
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640381
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477315
FRAGMENT_COUNT: 8
  ORIGINAL[0]: hashp -> tabname
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: !hashp -> frozen
  TYPE[1]: CALL
  TOKENIZED[1]: !hashp -> VAR1
  ORIGINAL[2]: hashp -> frozen
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: has_seq_scans(hashp)
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( VAR1 )
  ORIGINAL[4]: hashp -> tabname
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 68719477097
FRAGMENT_COUNT: 4
  ORIGINAL[0]: hctl -> high_mask
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: high_mask
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: hash_val
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hctl
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719476796
FRAGMENT_COUNT: 4
  ORIGINAL[0]: c >= 97 && c <= 122
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= 97 && VAR1 <= 122
  ORIGINAL[1]: c
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: c
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: c
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771565
FRAGMENT_COUNT: 2
  ORIGINAL[0]: ((intptr_t )(sizeof(HASHHDR ))) + (8 - 1)
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 )
  ORIGINAL[1]: 8 - 1
  TYPE[1]: CALL
  TOKENIZED[1]: 8 - 1

CENTER_NODE: 30064772449
FRAGMENT_COUNT: 18
  ORIGINAL[0]: stonesoup_data != NULL
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != VAR2
  ORIGINAL[1]: stonesoup_data->buffer
  TYPE[1]: CALL
  TOKENIZED[1]: stonesoup_data->buffer
  ORIGINAL[2]: stonesoup_data->buffer[64 - 1]
  TYPE[2]: CALL
  TOKENIZED[2]: stonesoup_data->buffer [ 64 - 1 ]
  ORIGINAL[3]: stonesoup_data->buff_pointer = stonesoup_data->buffer
  TYPE[3]: CALL
  TOKENIZED[3]: stonesoup_data->buff_pointer = stonesoup_data->buffer
  ORIGINAL[4]: stonesoup_data->buff_pointer
  TYPE[4]: CALL
  TOKENIZED[4]: stonesoup_data->buff_pointer
  ORIGINAL[5]: stonesoup_data->buffer
  TYPE[5]: CALL
  TOKENIZED[5]: stonesoup_data->buffer
  ORIGINAL[6]: stonesoup_data->buffer
  TYPE[6]: CALL
  TOKENIZED[6]: stonesoup_data->buffer
  ORIGINAL[7]: stonesoup_data->buffer
  TYPE[7]: CALL
  TOKENIZED[7]: stonesoup_data->buffer
  ORIGINAL[8]: stonesoup_data->buffer
  TYPE[8]: CALL
  TOKENIZED[8]: stonesoup_data->buffer
  ORIGINAL[9]: stonesoup_data->buffer
  TYPE[9]: CALL
  TOKENIZED[9]: stonesoup_data->buffer
  ORIGINAL[10]: buffer
  TYPE[10]: FIELD_IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: stonesoup_data
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1
  ORIGINAL[12]: stonesoup_data
  TYPE[12]: IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: stonesoup_data
  TYPE[13]: IDENTIFIER
  TOKENIZED[13]: VAR1
  ORIGINAL[14]: stonesoup_data
  TYPE[14]: IDENTIFIER
  TOKENIZED[14]: VAR1
  ORIGINAL[15]: stonesoup_data
  TYPE[15]: IDENTIFIER
  TOKENIZED[15]: VAR1
  ORIGINAL[16]: stonesoup_data
  TYPE[16]: IDENTIFIER
  TOKENIZED[16]: VAR1
  ORIGINAL[17]: stonesoup_data
  TYPE[17]: IDENTIFIER
  TOKENIZED[17]: VAR1

CENTER_NODE: 47244640329
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640355
FRAGMENT_COUNT: 1
  ORIGINAL[0]: !status -> hashp -> frozen
  TYPE[0]: CALL
  TOKENIZED[0]: !status -> VAR1 -> VAR2

CENTER_NODE: 47244640368
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772312
FRAGMENT_COUNT: 10
  ORIGINAL[0]: &hctlv -> mutex
  TYPE[0]: CALL
  TOKENIZED[0]: &hctlv -> VAR1
  ORIGINAL[1]: hctlv -> mutex
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: &hctlv -> mutex
  TYPE[2]: CALL
  TOKENIZED[2]: &hctlv -> VAR1
  ORIGINAL[3]: hctlv -> mutex
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hctlv -> num_partitions != 0
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2 != 0
  ORIGINAL[5]: &hctlv -> mutex
  TYPE[5]: CALL
  TOKENIZED[5]: &hctlv -> VAR1
  ORIGINAL[6]: hctlv -> mutex
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: mutex
  TYPE[7]: FIELD_IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: hctlv
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: hctlv
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 30064771534
FRAGMENT_COUNT: 4
  ORIGINAL[0]: segp++
  TYPE[0]: CALL
  TOKENIZED[0]: segp++
  ORIGINAL[1]: *segp == ((void *)0)
  TYPE[1]: CALL
  TOKENIZED[1]: *segp == ( ( void * ) 0 )
  ORIGINAL[2]: hctl
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: segp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772320
FRAGMENT_COUNT: 5
  ORIGINAL[0]: hashp -> isshared
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: elog_finish(21,\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( 21 , \
  ORIGINAL[2]: hashp -> tabname
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: tabname
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640373
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772492
FRAGMENT_COUNT: 1
  ORIGINAL[0]: *seq_scan_tables[100]
  TYPE[0]: CALL
  TOKENIZED[0]: *seq_scan_tables [ 100 ]

CENTER_NODE: 30064771910
FRAGMENT_COUNT: 5
  ORIGINAL[0]: hctlv -> num_partitions != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 != 0
  ORIGINAL[1]: *((volatile slock_t *)(&hctlv -> mutex)) = 0
  TYPE[1]: CALL
  TOKENIZED[1]: * ( ( volatile VAR1 * ) ( &hctlv -> VAR2 ) ) = 0
  ORIGINAL[2]: *((volatile slock_t *)(&hctlv -> mutex))
  TYPE[2]: CALL
  TOKENIZED[2]: * ( ( volatile VAR1 * ) ( &hctlv -> VAR2 ) )
  ORIGINAL[3]: (volatile slock_t *)(&hctlv -> mutex)
  TYPE[3]: CALL
  TOKENIZED[3]: ( volatile VAR1 * ) ( &hctlv -> VAR2 )
  ORIGINAL[4]: (volatile slock_t *)(&hctlv -> mutex)
  TYPE[4]: CALL
  TOKENIZED[4]: ( volatile VAR1 * ) ( &hctlv -> VAR2 )

CENTER_NODE: 47244640348
FRAGMENT_COUNT: 1
  ORIGINAL[0]: (curElem = status -> curEntry) != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 = VAR2 -> VAR3 ) != ( ( void * ) 0 )

CENTER_NODE: 30064772330
FRAGMENT_COUNT: 2
  ORIGINAL[0]: 1L << my_log2(num)
  TYPE[0]: CALL
  TOKENIZED[0]: 1L << FUN1 ( VAR1 )
  ORIGINAL[1]: my_log2(num)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )

CENTER_NODE: 68719477091
FRAGMENT_COUNT: 4
  ORIGINAL[0]: hashp -> keysize
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: keysize
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: keyPtr
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772356
FRAGMENT_COUNT: 6
  ORIGINAL[0]: seq_scan_tables[i] == hashp
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ VAR2 ] == VAR3
  ORIGINAL[1]: seq_scan_tables[i] = seq_scan_tables[num_seq_scans - 1]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ VAR2 ] = VAR1 [ VAR3 - 1 ]
  ORIGINAL[2]: seq_scan_tables[i]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ VAR2 ]
  ORIGINAL[3]: seq_scan_tables[num_seq_scans - 1]
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 [ VAR2 - 1 ]
  ORIGINAL[4]: num_seq_scans - 1
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 - 1
  ORIGINAL[5]: <global> seq_scan_tables
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: <global> VAR1

CENTER_NODE: 30064771389
FRAGMENT_COUNT: 5
  ORIGINAL[0]: *hctl = hashp -> hctl
  TYPE[0]: CALL
  TOKENIZED[0]: *hctl = VAR1 -> VAR2
  ORIGINAL[1]: hashp -> hctl
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctl
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hctl
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772405
FRAGMENT_COUNT: 4
  ORIGINAL[0]: gos_biochore != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != 0
  ORIGINAL[1]: *meekhearted_fribblery = smallpoxes_podology
  TYPE[1]: CALL
  TOKENIZED[1]: *meekhearted_fribblery = VAR1
  ORIGINAL[2]: *meekhearted_fribblery
  TYPE[2]: CALL
  TOKENIZED[2]: *meekhearted_fribblery
  ORIGINAL[3]: smallpoxes_podology
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771627
FRAGMENT_COUNT: 4
  ORIGINAL[0]: sizeof(HASHHDR ) + (info -> dsize) * sizeof(HASHSEGMENT )
  TYPE[0]: CALL
  TOKENIZED[0]: sizeof ( VAR1 ) + ( VAR2 -> VAR3 ) * sizeof ( VAR4 )
  ORIGINAL[1]: (info -> dsize) * sizeof(HASHSEGMENT )
  TYPE[1]: CALL
  TOKENIZED[1]: ( VAR1 -> VAR2 ) * sizeof ( VAR3 )
  ORIGINAL[2]: info -> dsize
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: sizeof(HASHSEGMENT )
  TYPE[3]: CALL
  TOKENIZED[3]: sizeof ( VAR1 )

CENTER_NODE: 30064771135
FRAGMENT_COUNT: 4
  ORIGINAL[0]: strncmp(key1,key2,keysize - 1)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 - 1 )
  ORIGINAL[1]: keysize - 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 - 1
  ORIGINAL[2]: key1
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: key2
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477524
FRAGMENT_COUNT: 4
  ORIGINAL[0]: my_log2(num)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 )
  ORIGINAL[1]: num
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: num
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: num
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771243
FRAGMENT_COUNT: 4
  ORIGINAL[0]: flags & 0x040
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 & 0x040
  ORIGINAL[1]: hashp -> isshared = ((bool )1)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 = ( ( VAR3 ) 1 )
  ORIGINAL[2]: hashp -> isshared
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: (bool )1
  TYPE[3]: CALL
  TOKENIZED[3]: ( VAR1 ) 1

CENTER_NODE: 30064771691
FRAGMENT_COUNT: 7
  ORIGINAL[0]: hashp -> dir[segment_num]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 [ VAR3 ]
  ORIGINAL[1]: hashp -> dir
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: dir
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: segment_num
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 30064772151
FRAGMENT_COUNT: 9
  ORIGINAL[0]: hashp -> hctl -> max_dsize != (- 1)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3 != ( - 1 )
  ORIGINAL[1]: old_p = hashp -> dir
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = VAR2 -> VAR3
  ORIGINAL[2]: hashp -> dir
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: dir
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: old_p
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: hashp
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 30064771463
FRAGMENT_COUNT: 4
  ORIGINAL[0]: ((intptr_t )entrysize) + (8 - 1) & ~((intptr_t )(8 - 1))
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( VAR1 ) VAR2 ) + ( 8 - 1 ) & ~ ( ( VAR1 ) ( 8 - 1 ) )
  ORIGINAL[1]: ((intptr_t )entrysize) + (8 - 1)
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( VAR1 ) VAR2 ) + ( 8 - 1 )
  ORIGINAL[2]: ~((intptr_t )(8 - 1))
  TYPE[2]: CALL
  TOKENIZED[2]: ~ ( ( VAR1 ) ( 8 - 1 ) )
  ORIGINAL[3]: (intptr_t )(8 - 1)
  TYPE[3]: CALL
  TOKENIZED[3]: ( VAR1 ) ( 8 - 1 )

CENTER_NODE: 30064771612
FRAGMENT_COUNT: 4
  ORIGINAL[0]: nBuckets = next_pow2_long((num_entries - 1) / 1 + 1)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = FUN1 ( ( VAR2 - 1 ) / 1 + 1 )
  ORIGINAL[1]: next_pow2_long((num_entries - 1) / 1 + 1)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( ( VAR1 - 1 ) / 1 + 1 )
  ORIGINAL[2]: nBuckets
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: nSegments
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771931
FRAGMENT_COUNT: 5
  ORIGINAL[0]: hashp -> hctl -> nentries
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[1]: hashp -> hctl
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctl
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: nentries
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640338
FRAGMENT_COUNT: 1
  ORIGINAL[0]: for (;;)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: for ( ; ; )

CENTER_NODE: 30064772209
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (hashp -> alloc)(sizeof(HASHBUCKET ) * (hashp -> ssize))
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 -> VAR2 ) ( sizeof ( VAR3 ) * ( VAR1 -> VAR4 ) )
  ORIGINAL[1]: hashp -> alloc
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: alloc
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: HASHBUCKET
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772118
FRAGMENT_COUNT: 5
  ORIGINAL[0]: ((long )(calc_bucket(hctl,currElement -> hashvalue))) == old_bucket
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( long ) ( FUN1 ( VAR1 , VAR2 -> VAR3 ) ) ) == VAR4
  ORIGINAL[1]: oldlink = &currElement -> link
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = &currElement -> VAR2
  ORIGINAL[2]: &currElement -> link
  TYPE[2]: CALL
  TOKENIZED[2]: &currElement -> VAR1
  ORIGINAL[3]: currElement
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: oldlink
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477517
FRAGMENT_COUNT: 5
  ORIGINAL[0]: limit < num
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: num
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: num
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: limit
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: num
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640298
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771933
FRAGMENT_COUNT: 7
  ORIGINAL[0]: status -> hashp = hashp
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = VAR2
  ORIGINAL[1]: status -> hashp
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hashp
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: status
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: status
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: status
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 30064771133
FRAGMENT_COUNT: 1
  ORIGINAL[0]: buffer[64]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ 64 ]

