# Tokenized code fragments for 153282-v1.0.0-bad
# Total center nodes processed: 33
# Total code fragments found: 150

CENTER_NODE: 30064771671
FRAGMENT_COUNT: 5
  ORIGINAL[0]: fh -> err != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 != 0
  ORIGINAL[1]: fh -> err_info == ((void *)0)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 == ( ( void * ) 0 )
  ORIGINAL[2]: fh -> err_info
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: (void *)0
  TYPE[3]: CALL
  TOKENIZED[3]: ( void * ) 0
  ORIGINAL[4]: (void *)0
  TYPE[4]: CALL
  TOKENIZED[4]: ( void * ) 0

CENTER_NODE: 47244640304
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771130
FRAGMENT_COUNT: 1
  ORIGINAL[0]: buffer[64]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ 64 ]

CENTER_NODE: 68719477051
FRAGMENT_COUNT: 3
  ORIGINAL[0]: stream -> is_compressed
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: is_compressed
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: stream
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771082
FRAGMENT_COUNT: 22
  ORIGINAL[0]: ss_tc_root != NULL
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != VAR2
  ORIGINAL[1]: size_dirpath = strlen(ss_tc_root) + strlen(\
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = FUN1 ( VAR2 ) + FUN1 ( \
  ORIGINAL[2]: strlen(ss_tc_root) + strlen(\
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 ) + FUN1 ( \
  ORIGINAL[3]: strlen(ss_tc_root) + strlen(\
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( VAR1 ) + FUN1 ( \
  ORIGINAL[4]: strlen(ss_tc_root)
  TYPE[4]: CALL
  TOKENIZED[4]: FUN1 ( VAR1 )
  ORIGINAL[5]: strlen(\
  TYPE[5]: CALL
  TOKENIZED[5]: FUN1 ( \
  ORIGINAL[6]: dirpath = (char*) malloc (size_dirpath * sizeof(char))
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 = ( char* ) FUN1 ( VAR2 * sizeof ( char ) )
  ORIGINAL[7]: (char*) malloc (size_dirpath * sizeof(char))
  TYPE[7]: CALL
  TOKENIZED[7]: ( char* ) FUN1 ( VAR1 * sizeof ( char ) )
  ORIGINAL[8]: malloc (size_dirpath * sizeof(char))
  TYPE[8]: CALL
  TOKENIZED[8]: FUN1 ( VAR1 * sizeof ( char ) )
  ORIGINAL[9]: size_dirpath * sizeof(char)
  TYPE[9]: CALL
  TOKENIZED[9]: VAR1 * sizeof ( char )
  ORIGINAL[10]: sizeof(char)
  TYPE[10]: CALL
  TOKENIZED[10]: sizeof ( char )
  ORIGINAL[11]: dirpath != NULL
  TYPE[11]: CALL
  TOKENIZED[11]: VAR1 != VAR2
  ORIGINAL[12]: ss_tc_root
  TYPE[12]: IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: NULL
  TYPE[13]: IDENTIFIER
  TOKENIZED[13]: VAR1
  ORIGINAL[14]: size_dirpath
  TYPE[14]: IDENTIFIER
  TOKENIZED[14]: VAR1
  ORIGINAL[15]: ss_tc_root
  TYPE[15]: IDENTIFIER
  TOKENIZED[15]: VAR1
  ORIGINAL[16]: dirpath
  TYPE[16]: IDENTIFIER
  TOKENIZED[16]: VAR1
  ORIGINAL[17]: size_dirpath
  TYPE[17]: IDENTIFIER
  TOKENIZED[17]: VAR1
  ORIGINAL[18]: char
  TYPE[18]: IDENTIFIER
  TOKENIZED[18]: char
  ORIGINAL[19]: dirpath
  TYPE[19]: IDENTIFIER
  TOKENIZED[19]: VAR1
  ORIGINAL[20]: NULL
  TYPE[20]: IDENTIFIER
  TOKENIZED[20]: VAR1
  ORIGINAL[21]: <global> stonesoup_printf_context
  TYPE[21]: IDENTIFIER
  TOKENIZED[21]: <global> VAR1

CENTER_NODE: 30064771705
FRAGMENT_COUNT: 3
  ORIGINAL[0]: file -> err = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = 0
  ORIGINAL[1]: file -> err
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: file
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771524
FRAGMENT_COUNT: 3
  ORIGINAL[0]: stream -> raw_pos
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: raw_pos
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: stream
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719476969
FRAGMENT_COUNT: 6
  ORIGINAL[0]: (fd = open(path,0 | 0,0)) == - 1
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 = FUN1 ( VAR2 , 0 | 0 , 0 ) ) == - 1
  ORIGINAL[1]: ft = file_fdopen(fd)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = FUN1 ( VAR2 )
  ORIGINAL[2]: file_fdopen(fd)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 )
  ORIGINAL[3]: ft
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: fd
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: ft
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771593
FRAGMENT_COUNT: 3
  ORIGINAL[0]: ret < 1?- 1 : buf[0]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < 1?- 1 : VAR2 [ 0 ]
  ORIGINAL[1]: ret < 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 < 1
  ORIGINAL[2]: - 1
  TYPE[2]: CALL
  TOKENIZED[2]: - 1

CENTER_NODE: 30064771488
FRAGMENT_COUNT: 4
  ORIGINAL[0]: lseek(file -> fd,off,0) == (- 1)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , VAR3 , 0 ) == ( - 1 )
  ORIGINAL[1]: file -> err = 0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 = 0
  ORIGINAL[2]: file -> err
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: file
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771131
FRAGMENT_COUNT: 1
  ORIGINAL[0]: after[64]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ 64 ]

CENTER_NODE: 30064771129
FRAGMENT_COUNT: 1
  ORIGINAL[0]: before[64]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ 64 ]

CENTER_NODE: 30064771181
FRAGMENT_COUNT: 4
  ORIGINAL[0]: state -> eof == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == 0
  ORIGINAL[1]: raw_read(state,state -> in,state -> size,((unsigned int *)(&state -> avail_in)))
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR1 -> VAR2 , VAR1 -> VAR3 , ( ( unsigned int * ) ( &state -> VAR4 ) ) )
  ORIGINAL[2]: (unsigned int *)(&state -> avail_in)
  TYPE[2]: CALL
  TOKENIZED[2]: ( unsigned int * ) ( &state -> VAR1 )
  ORIGINAL[3]: &state -> avail_in
  TYPE[3]: CALL
  TOKENIZED[3]: &state -> VAR1

CENTER_NODE: 30064771520
FRAGMENT_COUNT: 5
  ORIGINAL[0]: stream -> pos + ((stream -> seek?stream -> skip : 0))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 + ( ( VAR1 -> seek?stream -> VAR3 : 0 ) )
  ORIGINAL[1]: stream -> pos
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: pos
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: stream
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: stream
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771573
FRAGMENT_COUNT: 5
  ORIGINAL[0]: file -> have
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: got += n
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 += VAR2
  ORIGINAL[2]: got
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: n
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: file
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771189
FRAGMENT_COUNT: 1
  ORIGINAL[0]: window[32768]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ 32768 ]

CENTER_NODE: 68719477160
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (fd = open(path,0 | 0,0)) == - 1
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 = FUN1 ( VAR2 , 0 | 0 , 0 ) ) == - 1
  ORIGINAL[1]: file -> fd = fd
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 = VAR2
  ORIGINAL[2]: file -> fd
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: fd
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: fd
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771212
FRAGMENT_COUNT: 15
  ORIGINAL[0]: low < max
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: pos < item -> out
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 < VAR2 -> VAR3
  ORIGINAL[2]: pos > item -> out
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 > VAR2 -> VAR3
  ORIGINAL[3]: item -> out
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: smallest = item
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 = VAR2
  ORIGINAL[5]: low = i + 1
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 = VAR2 + 1
  ORIGINAL[6]: i + 1
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 + 1
  ORIGINAL[7]: low
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: max
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: pos
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: smallest
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: item
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1
  ORIGINAL[12]: low
  TYPE[12]: IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: i
  TYPE[13]: IDENTIFIER
  TOKENIZED[13]: VAR1
  ORIGINAL[14]: item
  TYPE[14]: IDENTIFIER
  TOKENIZED[14]: VAR1

CENTER_NODE: 68719477035
FRAGMENT_COUNT: 7
  ORIGINAL[0]: state -> compression == 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == 1
  ORIGINAL[1]: state -> compression
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: state -> out
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: out
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: state
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: state
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: state
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 47244640278
FRAGMENT_COUNT: 1
  ORIGINAL[0]: file -> fast_seek -> len != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3 != 0

CENTER_NODE: 30064771534
FRAGMENT_COUNT: 4
  ORIGINAL[0]: err != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: *err
  TYPE[1]: CALL
  TOKENIZED[1]: *err
  ORIGINAL[2]: *__errno_location()
  TYPE[2]: CALL
  TOKENIZED[2]: *__errno_location ( )
  ORIGINAL[3]: __errno_location()
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( )

CENTER_NODE: 30064771315
FRAGMENT_COUNT: 3
  ORIGINAL[0]: state -> err = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = 0
  ORIGINAL[1]: state -> err
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: state
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640342
FRAGMENT_COUNT: 1
  ORIGINAL[0]: file -> err
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2

CENTER_NODE: 30064771281
FRAGMENT_COUNT: 6
  ORIGINAL[0]: state -> avail_in == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == 0
  ORIGINAL[1]: state -> avail_in == 0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 == 0
  ORIGINAL[2]: state -> compression = 1
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 = 1
  ORIGINAL[3]: state -> compression
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: compression
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: state
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771678
FRAGMENT_COUNT: 4
  ORIGINAL[0]: stream -> err_info = ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = ( ( void * ) 0 )
  ORIGINAL[1]: stream -> err_info
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: (void *)0
  TYPE[2]: CALL
  TOKENIZED[2]: ( void * ) 0
  ORIGINAL[3]: stream
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477156
FRAGMENT_COUNT: 5
  ORIGINAL[0]: close(file -> fd)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 )
  ORIGINAL[1]: file -> fd
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: file -> fd
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: fd
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: file
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476911
FRAGMENT_COUNT: 4
  ORIGINAL[0]: state -> have
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: state -> err
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: err
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: state
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771659
FRAGMENT_COUNT: 9
  ORIGINAL[0]: file -> eof && file -> avail_in == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 && VAR1 -> VAR3 == 0
  ORIGINAL[1]: file -> eof
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: file -> avail_in == 0
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 == 0
  ORIGINAL[3]: file -> avail_in
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: eof
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: avail_in
  TYPE[5]: FIELD_IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: file
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: file
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: file
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 68719477040
FRAGMENT_COUNT: 3
  ORIGINAL[0]: file_seek(file,delta,1,err)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , 1 , VAR3 )
  ORIGINAL[1]: file
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: delta
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719476807
FRAGMENT_COUNT: 6
  ORIGINAL[0]: *have
  TYPE[0]: CALL
  TOKENIZED[0]: *have
  ORIGINAL[1]: *have
  TYPE[1]: CALL
  TOKENIZED[1]: *have
  ORIGINAL[2]: *have < count
  TYPE[2]: CALL
  TOKENIZED[2]: *have < VAR1
  ORIGINAL[3]: *have
  TYPE[3]: CALL
  TOKENIZED[3]: *have
  ORIGINAL[4]: buf
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: have
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771190
FRAGMENT_COUNT: 1
  ORIGINAL[0]: window[32768]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ 32768 ]

CENTER_NODE: 30064771415
FRAGMENT_COUNT: 5
  ORIGINAL[0]: stream -> fast_seek = seek
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = VAR3
  ORIGINAL[1]: stream -> fast_seek
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: fast_seek
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: stream
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: seek
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771714
FRAGMENT_COUNT: 4
  ORIGINAL[0]: *compressed_file_extensions[] = {(\
  TYPE[0]: CALL
  TOKENIZED[0]: *compressed_file_extensions [ ] = { ( \
  ORIGINAL[1]: {(\
  TYPE[1]: CALL
  TOKENIZED[1]: { ( \
  ORIGINAL[2]: compressed_file_extensions
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: catapult_glycyl
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

