# Tokenized code fragments for 152846-v1.0.0-bad
# Total center nodes processed: 41
# Total code fragments found: 174

CENTER_NODE: 30064771169
FRAGMENT_COUNT: 5
  ORIGINAL[0]: strncmp(key1,key2,keysize - 1)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 - 1 )
  ORIGINAL[1]: keysize - 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 - 1
  ORIGINAL[2]: key1
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: key2
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: keysize
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640274
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640408
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772047
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hashp -> hctl -> nentries
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[1]: hashp -> hctl
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: nentries
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064772052
FRAGMENT_COUNT: 5
  ORIGINAL[0]: status -> curBucket = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = 0
  ORIGINAL[1]: status -> curBucket
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: curBucket
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: status
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: status
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477645
FRAGMENT_COUNT: 5
  ORIGINAL[0]: seq_scan_tables[i] == hashp
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ VAR2 ] == VAR3
  ORIGINAL[1]: seq_scan_level[i]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ VAR2 ]
  ORIGINAL[2]: i
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: <global> seq_scan_level
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: <global> VAR1
  ORIGINAL[4]: i
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640324
FRAGMENT_COUNT: 3
  ORIGINAL[0]: stonesoup_i < strlen(hypostasy_traditious)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < FUN1 ( VAR2 )
  ORIGINAL[1]: ++stonesoup_i
  TYPE[1]: CALL
  TOKENIZED[1]: ++stonesoup_i
  ORIGINAL[2]: for (;stonesoup_i < strlen(hypostasy_traditious);++stonesoup_i)
  TYPE[2]: CONTROL_STRUCTURE
  TOKENIZED[2]: for ( ; VAR1 < FUN1 ( VAR2 ) ; ++stonesoup_i )

CENTER_NODE: 47244640400
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477250
FRAGMENT_COUNT: 5
  ORIGINAL[0]: currBucket -> hashvalue == hashvalue
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == VAR2
  ORIGINAL[1]: (char *)currBucket
  TYPE[1]: CALL
  TOKENIZED[1]: ( char * ) VAR1
  ORIGINAL[2]: (intptr_t )(sizeof(HASHELEMENT ))
  TYPE[2]: CALL
  TOKENIZED[2]: ( VAR1 ) ( sizeof ( VAR2 ) )
  ORIGINAL[3]: intptr_t
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: HASHELEMENT
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771603
FRAGMENT_COUNT: 5
  ORIGINAL[0]: size = add_size(size,mul_size(nDirEntries,sizeof(HASHSEGMENT )))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = FUN1 ( VAR1 , FUN2 ( VAR2 , sizeof ( VAR3 ) ) )
  ORIGINAL[1]: add_size(size,mul_size(nDirEntries,sizeof(HASHSEGMENT )))
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , FUN2 ( VAR2 , sizeof ( VAR3 ) ) )
  ORIGINAL[2]: mul_size(nDirEntries,sizeof(HASHSEGMENT ))
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , sizeof ( VAR2 ) )
  ORIGINAL[3]: size
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: size
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771663
FRAGMENT_COUNT: 9
  ORIGINAL[0]: hashp != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: (void *)0
  TYPE[1]: CALL
  TOKENIZED[1]: ( void * ) 0
  ORIGINAL[2]: hash_stats(\
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( \
  ORIGINAL[3]: MemoryContextDelete(hashp -> hcxt)
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( VAR1 -> VAR2 )
  ORIGINAL[4]: hashp -> hcxt
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hcxt
  TYPE[5]: FIELD_IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: hashp
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 30064771116
FRAGMENT_COUNT: 5
  ORIGINAL[0]: filepath != NULL
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != VAR2
  ORIGINAL[1]: stonesoup_printf_context = fopen(filepath, \
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = FUN1 ( VAR2 , \
  ORIGINAL[2]: fopen(filepath, \
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , \
  ORIGINAL[3]: <global> stonesoup_printf_context
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: <global> VAR1
  ORIGINAL[4]: filepath
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772043
FRAGMENT_COUNT: 4
  ORIGINAL[0]: hctlv -> num_partitions != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 != 0
  ORIGINAL[1]: *((volatile slock_t *)(&hctlv -> mutex)) = 0
  TYPE[1]: CALL
  TOKENIZED[1]: * ( ( volatile VAR1 * ) ( &hctlv -> VAR2 ) ) = 0
  ORIGINAL[2]: *((volatile slock_t *)(&hctlv -> mutex))
  TYPE[2]: CALL
  TOKENIZED[2]: * ( ( volatile VAR1 * ) ( &hctlv -> VAR2 ) )
  ORIGINAL[3]: (volatile slock_t *)(&hctlv -> mutex)
  TYPE[3]: CALL
  TOKENIZED[3]: ( volatile VAR1 * ) ( &hctlv -> VAR2 )

CENTER_NODE: 68719477013
FRAGMENT_COUNT: 5
  ORIGINAL[0]: nelem_alloc = (allocSize / elementSize)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = ( VAR2 / VAR3 )
  ORIGINAL[1]: nelem_alloc < 32
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 < 32
  ORIGINAL[2]: nelem_alloc
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: nelem_alloc
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: nelem_alloc
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640365
FRAGMENT_COUNT: 1
  ORIGINAL[0]: for (;;)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: for ( ; ; )

CENTER_NODE: 68719477113
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hashp -> hash
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hash
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: hashp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477502
FRAGMENT_COUNT: 7
  ORIGINAL[0]: hashp -> hctl -> max_dsize != (- 1)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3 != ( - 1 )
  ORIGINAL[1]: p = ((HASHSEGMENT *)((hashp -> alloc)(((Size )new_dirsize))))
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = ( ( VAR2 * ) ( ( VAR3 -> VAR4 ) ( ( ( VAR5 ) VAR6 ) ) ) )
  ORIGINAL[2]: p != ((void *)0)
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[3]: (void *)0
  TYPE[3]: CALL
  TOKENIZED[3]: ( void * ) 0
  ORIGINAL[4]: p
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: p
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: p
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 68719477207
FRAGMENT_COUNT: 4
  ORIGINAL[0]: hashp -> keysize
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: keysize
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: keyPtr
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771161
FRAGMENT_COUNT: 7
  ORIGINAL[0]: *stonesoup_s != (char)0
  TYPE[0]: CALL
  TOKENIZED[0]: *stonesoup_s != ( char ) 0
  ORIGINAL[1]: (*stonesoup_tainted_buff)[stonesoup_s - stonesoup_shm] = *stonesoup_s
  TYPE[1]: CALL
  TOKENIZED[1]: ( *stonesoup_tainted_buff ) [ VAR1 - VAR2 ] = *stonesoup_s
  ORIGINAL[2]: (*stonesoup_tainted_buff)[stonesoup_s - stonesoup_shm]
  TYPE[2]: CALL
  TOKENIZED[2]: ( *stonesoup_tainted_buff ) [ VAR1 - VAR2 ]
  ORIGINAL[3]: *stonesoup_tainted_buff
  TYPE[3]: CALL
  TOKENIZED[3]: *stonesoup_tainted_buff
  ORIGINAL[4]: stonesoup_s - stonesoup_shm
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 - VAR2
  ORIGINAL[5]: *stonesoup_s
  TYPE[5]: CALL
  TOKENIZED[5]: *stonesoup_s
  ORIGINAL[6]: stonesoup_s
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 30064771470
FRAGMENT_COUNT: 3
  ORIGINAL[0]: 2 * sizeof(char *)
  TYPE[0]: CALL
  TOKENIZED[0]: 2 * sizeof ( char * )
  ORIGINAL[1]: sizeof(char *)
  TYPE[1]: CALL
  TOKENIZED[1]: sizeof ( char * )
  ORIGINAL[2]: char
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: char

CENTER_NODE: 68719477623
FRAGMENT_COUNT: 4
  ORIGINAL[0]: my_log2(num)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 )
  ORIGINAL[1]: num
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: num
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: num
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771653
FRAGMENT_COUNT: 3
  ORIGINAL[0]: (nBuckets - 1) / 256 + 1
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 - 1 ) / 256 + 1
  ORIGINAL[1]: (nBuckets - 1) / 256
  TYPE[1]: CALL
  TOKENIZED[1]: ( VAR1 - 1 ) / 256
  ORIGINAL[2]: nBuckets - 1
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 - 1

CENTER_NODE: 68719477614
FRAGMENT_COUNT: 4
  ORIGINAL[0]: i = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = 0
  ORIGINAL[1]: limit = 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = 1
  ORIGINAL[2]: limit
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: limit
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640395
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771358
FRAGMENT_COUNT: 5
  ORIGINAL[0]: flags & 0x004
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 & 0x004
  ORIGINAL[1]: hctl -> max_dsize = info -> max_dsize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 = VAR3 -> VAR2
  ORIGINAL[2]: hctl -> max_dsize
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: info -> max_dsize
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hctl
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640401
FRAGMENT_COUNT: 2
  ORIGINAL[0]: (((intptr_t )_vstart) & sizeof(long ) - 1) == 0 && (_len & sizeof(long ) - 1) == 0 && _val == 0 && _len <= 1024 && 1024 != 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( ( VAR1 ) VAR2 ) & sizeof ( long ) - 1 ) == 0 && ( VAR3 & sizeof ( long ) - 1 ) == 0 && VAR4 == 0 && VAR3 <= 1024 && 1024 != 0
  ORIGINAL[1]: else
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: else

CENTER_NODE: 47244640356
FRAGMENT_COUNT: 0

CENTER_NODE: 68719476785
FRAGMENT_COUNT: 6
  ORIGINAL[0]: vfprintf(stonesoup_printf_context, format, argptr)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 )
  ORIGINAL[1]: argptr
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: <global> stonesoup_printf_context
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: <global> VAR1
  ORIGINAL[3]: format
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: argptr
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: argptr
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477363
FRAGMENT_COUNT: 16
  ORIGINAL[0]: (curElem = status -> curEntry) != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 = VAR2 -> VAR3 ) != ( ( void * ) 0 )
  ORIGINAL[1]: curBucket = status -> curBucket
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = VAR2 -> VAR1
  ORIGINAL[2]: hashp = status -> hashp
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = VAR2 -> VAR1
  ORIGINAL[3]: status -> hashp
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hashp -> hctl
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hashp -> ssize
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: hashp -> sshift
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: hashp -> dir
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2
  ORIGINAL[8]: hashp -> dir
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: hashp
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: status
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: hashp
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1
  ORIGINAL[12]: hashp
  TYPE[12]: IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: hashp
  TYPE[13]: IDENTIFIER
  TOKENIZED[13]: VAR1
  ORIGINAL[14]: hashp
  TYPE[14]: IDENTIFIER
  TOKENIZED[14]: VAR1
  ORIGINAL[15]: hashp
  TYPE[15]: IDENTIFIER
  TOKENIZED[15]: VAR1

CENTER_NODE: 30064772377
FRAGMENT_COUNT: 5
  ORIGINAL[0]: hashp -> isfixed
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: ((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1))
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR1 ) ( 8 - 1 ) )
  ORIGINAL[2]: ((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1)
  TYPE[2]: CALL
  TOKENIZED[2]: ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 )
  ORIGINAL[3]: ~((intptr_t )(8 - 1))
  TYPE[3]: CALL
  TOKENIZED[3]: ~ ( ( VAR1 ) ( 8 - 1 ) )
  ORIGINAL[4]: (intptr_t )(8 - 1)
  TYPE[4]: CALL
  TOKENIZED[4]: ( VAR1 ) ( 8 - 1 )

CENTER_NODE: 68719477607
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hashp -> isshared
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: elog_start(\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( \
  ORIGINAL[2]: <global> __func__
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: <global> VAR1

CENTER_NODE: 30064772519
FRAGMENT_COUNT: 1
  ORIGINAL[0]: *seq_scan_tables[100]
  TYPE[0]: CALL
  TOKENIZED[0]: *seq_scan_tables [ 100 ]

CENTER_NODE: 30064771547
FRAGMENT_COUNT: 12
  ORIGINAL[0]: nsegs > hctl -> dsize
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > VAR2 -> VAR3
  ORIGINAL[1]: !hashp -> dir
  TYPE[1]: CALL
  TOKENIZED[1]: !hashp -> VAR1
  ORIGINAL[2]: hashp -> dir
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: !hashp -> dir
  TYPE[3]: CALL
  TOKENIZED[3]: !hashp -> VAR1
  ORIGINAL[4]: hashp -> dir
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hashp -> dir
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: dir
  TYPE[6]: FIELD_IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: hashp
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: hashp
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: hashp
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: hashp
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1

CENTER_NODE: 68719477416
FRAGMENT_COUNT: 7
  ORIGINAL[0]: hashp -> tabname
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: !hashp -> frozen && has_seq_scans(hashp)
  TYPE[1]: CALL
  TOKENIZED[1]: !hashp -> VAR1 && FUN1 ( VAR2 )
  ORIGINAL[2]: hashp -> frozen
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hashp -> tabname
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: tabname
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 68719477442
FRAGMENT_COUNT: 12
  ORIGINAL[0]: new_segnum >= hctl -> nsegs
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= VAR2 -> VAR3
  ORIGINAL[1]: hctl -> nsegs
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctl -> dsize
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: !(hashp -> dir[new_segnum] = seg_alloc(hashp))
  TYPE[3]: CALL
  TOKENIZED[3]: ! ( VAR1 -> VAR2 [ VAR3 ] = FUN1 ( VAR1 ) )
  ORIGINAL[4]: hctl -> nsegs
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hctl -> max_bucket
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: hctl -> low_mask
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: (uint32 )new_bucket
  TYPE[7]: CALL
  TOKENIZED[7]: ( VAR1 ) VAR2
  ORIGINAL[8]: hctl -> high_mask
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: high_mask
  TYPE[9]: FIELD_IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: hctl
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: hctl
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1

CENTER_NODE: 47244640304
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772552
FRAGMENT_COUNT: 4
  ORIGINAL[0]: strlen(str) >= shmsz
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 ) >= VAR2
  ORIGINAL[1]: errors++
  TYPE[1]: CALL
  TOKENIZED[1]: errors++
  ORIGINAL[2]: errors
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: errors
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772448
FRAGMENT_COUNT: 3
  ORIGINAL[0]: 1L << my_log2(num)
  TYPE[0]: CALL
  TOKENIZED[0]: 1L << FUN1 ( VAR1 )
  ORIGINAL[1]: my_log2(num)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )
  ORIGINAL[2]: num
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771658
FRAGMENT_COUNT: 3
  ORIGINAL[0]: sizeof(HASHHDR ) + (info -> dsize) * sizeof(HASHSEGMENT )
  TYPE[0]: CALL
  TOKENIZED[0]: sizeof ( VAR1 ) + ( VAR2 -> VAR3 ) * sizeof ( VAR4 )
  ORIGINAL[1]: sizeof(HASHHDR )
  TYPE[1]: CALL
  TOKENIZED[1]: sizeof ( VAR1 )
  ORIGINAL[2]: (info -> dsize) * sizeof(HASHSEGMENT )
  TYPE[2]: CALL
  TOKENIZED[2]: ( VAR1 -> VAR2 ) * sizeof ( VAR3 )

CENTER_NODE: 47244640367
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772142
FRAGMENT_COUNT: 5
  ORIGINAL[0]: !status -> hashp -> frozen
  TYPE[0]: CALL
  TOKENIZED[0]: !status -> VAR1 -> VAR2
  ORIGINAL[1]: status -> hashp -> frozen
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[2]: status -> hashp
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: status -> hashp
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: frozen
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1

