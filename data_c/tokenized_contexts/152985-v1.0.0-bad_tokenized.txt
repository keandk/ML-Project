# Tokenized code fragments for 152985-v1.0.0-bad
# Total center nodes processed: 43
# Total code fragments found: 205

CENTER_NODE: 68719477069
FRAGMENT_COUNT: 6
  ORIGINAL[0]: hashp != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: hash_stats(\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( \
  ORIGINAL[2]: hashp -> hcxt
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hcxt
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771561
FRAGMENT_COUNT: 3
  ORIGINAL[0]: (intptr_t )(256 * sizeof(HASHBUCKET ))
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 ) ( 256 * sizeof ( VAR2 ) )
  ORIGINAL[1]: 256 * sizeof(HASHBUCKET )
  TYPE[1]: CALL
  TOKENIZED[1]: 256 * sizeof ( VAR1 )
  ORIGINAL[2]: sizeof(HASHBUCKET )
  TYPE[2]: CALL
  TOKENIZED[2]: sizeof ( VAR1 )

CENTER_NODE: 68719476781
FRAGMENT_COUNT: 5
  ORIGINAL[0]: strncmp(key1,key2,keysize - 1)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 - 1 )
  ORIGINAL[1]: keysize - 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 - 1
  ORIGINAL[2]: key1
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: key2
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: keysize
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771109
FRAGMENT_COUNT: 3
  ORIGINAL[0]: getenv(\
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( \
  ORIGINAL[1]: mg_set_option(stonesoup_server, \
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , \
  ORIGINAL[2]: stonesoup_server
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771596
FRAGMENT_COUNT: 3
  ORIGINAL[0]: next_pow2_long((num_entries - 1) / 1 + 1)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( ( VAR1 - 1 ) / 1 + 1 )
  ORIGINAL[1]: (num_entries - 1) / 1 + 1
  TYPE[1]: CALL
  TOKENIZED[1]: ( VAR1 - 1 ) / 1 + 1
  ORIGINAL[2]: (num_entries - 1) / 1
  TYPE[2]: CALL
  TOKENIZED[2]: ( VAR1 - 1 ) / 1

CENTER_NODE: 30064771611
FRAGMENT_COUNT: 4
  ORIGINAL[0]: (info -> dsize) * sizeof(HASHSEGMENT )
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 -> VAR2 ) * sizeof ( VAR3 )
  ORIGINAL[1]: info -> dsize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: sizeof(HASHSEGMENT )
  TYPE[2]: CALL
  TOKENIZED[2]: sizeof ( VAR1 )
  ORIGINAL[3]: HASHSEGMENT
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771630
FRAGMENT_COUNT: 6
  ORIGINAL[0]: bucket > hctl -> max_bucket
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > VAR2 -> VAR3
  ORIGINAL[1]: hctl -> max_bucket
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: max_bucket
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: bucket
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hctl
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hctl
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719476849
FRAGMENT_COUNT: 8
  ORIGINAL[0]: flags & 0x100
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 & 0x100
  ORIGINAL[1]: hashp -> alloc = info -> alloc
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 = VAR3 -> VAR2
  ORIGINAL[2]: hashp -> alloc = DynaHashAlloc
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 = VAR3
  ORIGINAL[3]: flags & 0x040
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 & 0x040
  ORIGINAL[4]: flags
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: flags
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: flags
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: flags
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 47244640258
FRAGMENT_COUNT: 1
  ORIGINAL[0]: while (1)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: while ( 1 )

CENTER_NODE: 47244640385
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477434
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (((intptr_t )_vstart) & sizeof(long ) - 1) == 0 && (_len & sizeof(long ) - 1) == 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( ( VAR1 ) VAR2 ) & sizeof ( long ) - 1 ) == 0 && ( VAR3 & sizeof ( long ) - 1 ) == 0
  ORIGINAL[1]: _val == 0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 == 0
  ORIGINAL[2]: _val
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: _val
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: _val
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640272
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640302
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772317
FRAGMENT_COUNT: 4
  ORIGINAL[0]: num > (2147483647 / 2)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > ( 2147483647 / 2 )
  ORIGINAL[1]: num = (2147483647 / 2)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = ( 2147483647 / 2 )
  ORIGINAL[2]: 2147483647 / 2
  TYPE[2]: CALL
  TOKENIZED[2]: 2147483647 / 2
  ORIGINAL[3]: num
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772302
FRAGMENT_COUNT: 5
  ORIGINAL[0]: hashp -> isshared
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: elog_finish(21,\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( 21 , \
  ORIGINAL[2]: hashp -> tabname
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: tabname
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771935
FRAGMENT_COUNT: 5
  ORIGINAL[0]: status -> curEntry == ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == ( ( void * ) 0 )
  ORIGINAL[1]: ++status -> curBucket
  TYPE[1]: CALL
  TOKENIZED[1]: ++status -> VAR1
  ORIGINAL[2]: status -> curBucket
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: curBucket
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: status
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771637
FRAGMENT_COUNT: 6
  ORIGINAL[0]: (hashp -> hash)(keyPtr,hashp -> keysize)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 -> VAR2 ) ( VAR3 , VAR1 -> VAR4 )
  ORIGINAL[1]: hashp -> keysize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: keysize
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: keyPtr
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 47244640333
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477007
FRAGMENT_COUNT: 20
  ORIGINAL[0]: hctl -> mutex
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hctl -> ffactor
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctl -> num_partitions
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hctl -> max_bucket
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hctl -> low_mask
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hctl -> high_mask
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: hctl -> ssize
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: hctl -> dsize
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2
  ORIGINAL[8]: hctl -> dsize
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: !hashp -> dir
  TYPE[9]: CALL
  TOKENIZED[9]: !hashp -> VAR1
  ORIGINAL[10]: hctl -> dsize
  TYPE[10]: CALL
  TOKENIZED[10]: VAR1 -> VAR2
  ORIGINAL[11]: !hashp -> dir
  TYPE[11]: CALL
  TOKENIZED[11]: !hashp -> VAR1
  ORIGINAL[12]: segp = hashp -> dir
  TYPE[12]: CALL
  TOKENIZED[12]: VAR1 = VAR2 -> VAR3
  ORIGINAL[13]: hctl -> nsegs
  TYPE[13]: CALL
  TOKENIZED[13]: VAR1 -> VAR2
  ORIGINAL[14]: hctl -> nsegs
  TYPE[14]: CALL
  TOKENIZED[14]: VAR1 -> VAR2
  ORIGINAL[15]: segp++
  TYPE[15]: CALL
  TOKENIZED[15]: segp++
  ORIGINAL[16]: *segp == ((void *)0)
  TYPE[16]: CALL
  TOKENIZED[16]: *segp == ( ( void * ) 0 )
  ORIGINAL[17]: nsegs
  TYPE[17]: FIELD_IDENTIFIER
  TOKENIZED[17]: VAR1
  ORIGINAL[18]: hctl
  TYPE[18]: IDENTIFIER
  TOKENIZED[18]: VAR1
  ORIGINAL[19]: hctl
  TYPE[19]: IDENTIFIER
  TOKENIZED[19]: VAR1

CENTER_NODE: 30064771916
FRAGMENT_COUNT: 3
  ORIGINAL[0]: status -> curBucket = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = 0
  ORIGINAL[1]: status -> curBucket
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: status
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477300
FRAGMENT_COUNT: 25
  ORIGINAL[0]: *hctl = hashp -> hctl
  TYPE[0]: CALL
  TOKENIZED[0]: *hctl = VAR1 -> VAR2
  ORIGINAL[1]: hashp -> hctl
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctl -> max_bucket
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hctl -> nsegs
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hctl -> dsize
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hctl -> nsegs
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: hctl -> max_bucket
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: hctl -> low_mask
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2
  ORIGINAL[8]: hctl -> high_mask
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: hctl -> high_mask
  TYPE[9]: CALL
  TOKENIZED[9]: VAR1 -> VAR2
  ORIGINAL[10]: hctl -> low_mask
  TYPE[10]: CALL
  TOKENIZED[10]: VAR1 -> VAR2
  ORIGINAL[11]: hctl
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1
  ORIGINAL[12]: hashp
  TYPE[12]: IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: hctl
  TYPE[13]: IDENTIFIER
  TOKENIZED[13]: VAR1
  ORIGINAL[14]: hctl
  TYPE[14]: IDENTIFIER
  TOKENIZED[14]: VAR1
  ORIGINAL[15]: hctl
  TYPE[15]: IDENTIFIER
  TOKENIZED[15]: VAR1
  ORIGINAL[16]: hctl
  TYPE[16]: IDENTIFIER
  TOKENIZED[16]: VAR1
  ORIGINAL[17]: hctl
  TYPE[17]: IDENTIFIER
  TOKENIZED[17]: VAR1
  ORIGINAL[18]: hctl
  TYPE[18]: IDENTIFIER
  TOKENIZED[18]: VAR1
  ORIGINAL[19]: hctl
  TYPE[19]: IDENTIFIER
  TOKENIZED[19]: VAR1
  ORIGINAL[20]: hctl
  TYPE[20]: IDENTIFIER
  TOKENIZED[20]: VAR1
  ORIGINAL[21]: hctl
  TYPE[21]: IDENTIFIER
  TOKENIZED[21]: VAR1
  ORIGINAL[22]: hctl
  TYPE[22]: IDENTIFIER
  TOKENIZED[22]: VAR1
  ORIGINAL[23]: hctl
  TYPE[23]: IDENTIFIER
  TOKENIZED[23]: VAR1
  ORIGINAL[24]: hctl
  TYPE[24]: IDENTIFIER
  TOKENIZED[24]: VAR1

CENTER_NODE: 68719477626
FRAGMENT_COUNT: 9
  ORIGINAL[0]: xerophilous_maximes != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != 0
  ORIGINAL[1]: stonesoup_stack_buff_64[63]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ 63 ]
  ORIGINAL[2]: stonesoup_stack_buff_64[stonesoup_ss_i]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ VAR2 ]
  ORIGINAL[3]: tracepoint(stonesoup_trace, variable_buffer, \
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( VAR1 , VAR2 , \
  ORIGINAL[4]: stonesoup_stack_buff_64
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: stonesoup_trace
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: variable_buffer
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: stonesoup_stack_buff_64
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: stonesoup_stack_buff_64
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 30064771075
FRAGMENT_COUNT: 2
  ORIGINAL[0]: va_end(argptr)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 )
  ORIGINAL[1]: argptr
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1

CENTER_NODE: 30064772312
FRAGMENT_COUNT: 2
  ORIGINAL[0]: 1L << my_log2(num)
  TYPE[0]: CALL
  TOKENIZED[0]: 1L << FUN1 ( VAR1 )
  ORIGINAL[1]: my_log2(num)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )

CENTER_NODE: 68719476945
FRAGMENT_COUNT: 8
  ORIGINAL[0]: (((intptr_t )_vstart) & sizeof(long ) - 1) == 0 && (_len & sizeof(long ) - 1) == 0 && _val == 0 && _len <= 1024 && 1024 != 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( ( VAR1 ) VAR2 ) & sizeof ( long ) - 1 ) == 0 && ( VAR3 & sizeof ( long ) - 1 ) == 0 && VAR4 == 0 && VAR3 <= 1024 && 1024 != 0
  ORIGINAL[1]: memset(_vstart,_val,_len)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 , VAR3 )
  ORIGINAL[2]: _len
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: _len
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: _len
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: _vstart
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: _val
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: _len
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 47244640372
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477451
FRAGMENT_COUNT: 6
  ORIGINAL[0]: hashp -> isfixed
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: elementSize = ((((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1))) + (((intptr_t )(hctlv -> entrysize)) + (8 - 1) & ~((intptr_t )(8 - 1))))
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = ( ( ( ( VAR2 ) ( sizeof ( VAR3 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR2 ) ( 8 - 1 ) ) ) + ( ( ( VAR2 ) ( VAR4 -> VAR5 ) ) + ( 8 - 1 ) & ~ ( ( VAR2 ) ( 8 - 1 ) ) ) )
  ORIGINAL[2]: (((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1))) + (((intptr_t )(hctlv -> entrysize)) + (8 - 1) & ~((intptr_t )(8 - 1)))
  TYPE[2]: CALL
  TOKENIZED[2]: ( ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR1 ) ( 8 - 1 ) ) ) + ( ( ( VAR1 ) ( VAR3 -> VAR4 ) ) + ( 8 - 1 ) & ~ ( ( VAR1 ) ( 8 - 1 ) ) )
  ORIGINAL[3]: elementSize
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: intptr_t
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: elementSize
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719476757
FRAGMENT_COUNT: 5
  ORIGINAL[0]: data_size = mg_get_var(conn, \
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = FUN1 ( VAR2 , \
  ORIGINAL[1]: data_size < buffer_size
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 < VAR2
  ORIGINAL[2]: data_size
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: data_size
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: buffer_size
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771617
FRAGMENT_COUNT: 4
  ORIGINAL[0]: (hashp -> hash)(keyPtr,hashp -> keysize)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 -> VAR2 ) ( VAR3 , VAR1 -> VAR4 )
  ORIGINAL[1]: hashp -> hash
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hashp -> keysize
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: keyPtr
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772014
FRAGMENT_COUNT: 10
  ORIGINAL[0]: hashp -> isshared
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: elog_finish(20,\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( 20 , \
  ORIGINAL[2]: hashp -> tabname
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hashp -> tabname
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: tabname
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: hashp
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: hashp
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 30064772452
FRAGMENT_COUNT: 1
  ORIGINAL[0]: *seq_scan_tables[100]
  TYPE[0]: CALL
  TOKENIZED[0]: *seq_scan_tables [ 100 ]

CENTER_NODE: 47244640374
FRAGMENT_COUNT: 3
  ORIGINAL[0]: _start < _stop
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: *(_start++) = 0
  TYPE[1]: CALL
  TOKENIZED[1]: * ( _start++ ) = 0
  ORIGINAL[2]: while (_start < _stop)
  TYPE[2]: CONTROL_STRUCTURE
  TOKENIZED[2]: while ( VAR1 < VAR2 )

CENTER_NODE: 47244640265
FRAGMENT_COUNT: 1
  ORIGINAL[0]: while (1)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: while ( 1 )

CENTER_NODE: 30064771912
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hashp -> hctl -> nentries
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[1]: hashp -> hctl
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: nentries
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064772305
FRAGMENT_COUNT: 5
  ORIGINAL[0]: num > 9223372036854775807L / 2
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > 9223372036854775807L / 2
  ORIGINAL[1]: num = 9223372036854775807L / 2
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = 9223372036854775807L / 2
  ORIGINAL[2]: 9223372036854775807L / 2
  TYPE[2]: CALL
  TOKENIZED[2]: 9223372036854775807L / 2
  ORIGINAL[3]: num
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: i
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477517
FRAGMENT_COUNT: 10
  ORIGINAL[0]: i = num_seq_scans - 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = VAR2 - 1
  ORIGINAL[1]: i >= 0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 >= 0
  ORIGINAL[2]: i--
  TYPE[2]: CALL
  TOKENIZED[2]: i--
  ORIGINAL[3]: seq_scan_tables[i] == hashp
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 [ VAR2 ] == VAR3
  ORIGINAL[4]: i
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: i
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: i
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: i
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: i
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: i
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 68719476964
FRAGMENT_COUNT: 3
  ORIGINAL[0]: ((intptr_t )entrysize) + (8 - 1)
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( VAR1 ) VAR2 ) + ( 8 - 1 )
  ORIGINAL[1]: (intptr_t )(8 - 1)
  TYPE[1]: CALL
  TOKENIZED[1]: ( VAR1 ) ( 8 - 1 )
  ORIGINAL[2]: intptr_t
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771768
FRAGMENT_COUNT: 4
  ORIGINAL[0]: currBucket != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: (void *)0
  TYPE[1]: CALL
  TOKENIZED[1]: ( void * ) 0
  ORIGINAL[2]: action
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: currBucket
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640377
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640344
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477224
FRAGMENT_COUNT: 12
  ORIGINAL[0]: hctlv -> mutex
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hctlv -> mutex
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctlv -> mutex
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hctlv -> nelem_alloc
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hctlv -> freeList
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hctlv -> nentries
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: hctlv -> num_partitions != 0
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2 != 0
  ORIGINAL[7]: hctlv -> num_partitions
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2
  ORIGINAL[8]: hctlv -> mutex
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: mutex
  TYPE[9]: FIELD_IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: hctlv
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: hctlv
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1

CENTER_NODE: 47244640342
FRAGMENT_COUNT: 1
  ORIGINAL[0]: for (;;)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: for ( ; ; )

CENTER_NODE: 68719477291
FRAGMENT_COUNT: 4
  ORIGINAL[0]: !status -> hashp -> frozen
  TYPE[0]: CALL
  TOKENIZED[0]: !status -> VAR1 -> VAR2
  ORIGINAL[1]: status -> hashp
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hashp
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: status
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

