# Tokenized code fragments for 152024-v1.0.0-bad
# Total center nodes processed: 43
# Total code fragments found: 199

CENTER_NODE: 30064772284
FRAGMENT_COUNT: 9
  ORIGINAL[0]: curBucket > max_bucket
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > VAR2
  ORIGINAL[1]: curBucket >> hashp -> sshift
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 >> VAR2 -> VAR3
  ORIGINAL[2]: hashp -> sshift
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: sshift
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: curBucket
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: hashp
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 30064771177
FRAGMENT_COUNT: 4
  ORIGINAL[0]: strncmp(key1,key2,keysize - 1)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 - 1 )
  ORIGINAL[1]: keysize - 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 - 1
  ORIGINAL[2]: key1
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: key2
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640304
FRAGMENT_COUNT: 0

CENTER_NODE: 68719476835
FRAGMENT_COUNT: 2
  ORIGINAL[0]: fclose(f)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 )
  ORIGINAL[1]: f
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1

CENTER_NODE: 68719477003
FRAGMENT_COUNT: 8
  ORIGINAL[0]: (((intptr_t )_vstart) & sizeof(long ) - 1) == 0 && (_len & sizeof(long ) - 1) == 0 && _val == 0 && _len <= 1024 && 1024 != 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( ( VAR1 ) VAR2 ) & sizeof ( long ) - 1 ) == 0 && ( VAR3 & sizeof ( long ) - 1 ) == 0 && VAR4 == 0 && VAR3 <= 1024 && 1024 != 0
  ORIGINAL[1]: memset(_vstart,_val,_len)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 , VAR3 )
  ORIGINAL[2]: _len
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: _len
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: _len
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: _vstart
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: _val
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: _len
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 47244640336
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771982
FRAGMENT_COUNT: 4
  ORIGINAL[0]: !(hctl -> num_partitions != 0) && !hashp -> frozen && hctl -> nentries / ((long )(hctl -> max_bucket + 1)) >= hctl -> ffactor && !has_seq_scans(hashp)
  TYPE[0]: CALL
  TOKENIZED[0]: ! ( VAR1 -> VAR2 != 0 ) && !hashp -> VAR3 && VAR1 -> VAR4 / ( ( long ) ( VAR1 -> VAR5 + 1 ) ) >= VAR1 -> VAR6 && !has_seq_scans ( VAR7 )
  ORIGINAL[1]: (void )(expand_table(hashp))
  TYPE[1]: CALL
  TOKENIZED[1]: ( void ) ( FUN1 ( VAR1 ) )
  ORIGINAL[2]: expand_table(hashp)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 )
  ORIGINAL[3]: bucket
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772211
FRAGMENT_COUNT: 11
  ORIGINAL[0]: newElement != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: hctlv -> num_partitions != 0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 != 0
  ORIGINAL[2]: hctlv -> num_partitions
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: *((volatile slock_t *)(&hctlv -> mutex)) = 0
  TYPE[3]: CALL
  TOKENIZED[3]: * ( ( volatile VAR1 * ) ( &hctlv -> VAR2 ) ) = 0
  ORIGINAL[4]: *((volatile slock_t *)(&hctlv -> mutex))
  TYPE[4]: CALL
  TOKENIZED[4]: * ( ( volatile VAR1 * ) ( &hctlv -> VAR2 ) )
  ORIGINAL[5]: (volatile slock_t *)(&hctlv -> mutex)
  TYPE[5]: CALL
  TOKENIZED[5]: ( volatile VAR1 * ) ( &hctlv -> VAR2 )
  ORIGINAL[6]: &hctlv -> mutex
  TYPE[6]: CALL
  TOKENIZED[6]: &hctlv -> VAR1
  ORIGINAL[7]: hctlv -> mutex
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2
  ORIGINAL[8]: mutex
  TYPE[8]: FIELD_IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: hctlv
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: hashp
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1

CENTER_NODE: 30064772647
FRAGMENT_COUNT: 5
  ORIGINAL[0]: seq_scan_tables[num_seq_scans] = hashp
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ VAR2 ] = VAR3
  ORIGINAL[1]: seq_scan_tables[num_seq_scans]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ VAR2 ]
  ORIGINAL[2]: <global> seq_scan_tables
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: <global> VAR1
  ORIGINAL[3]: <global> num_seq_scans
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: <global> VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772331
FRAGMENT_COUNT: 3
  ORIGINAL[0]: !status -> hashp -> frozen
  TYPE[0]: CALL
  TOKENIZED[0]: !status -> VAR1 -> VAR2
  ORIGINAL[1]: deregister_seq_scan(status -> hashp)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 -> VAR2 )
  ORIGINAL[2]: status -> hashp
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2

CENTER_NODE: 30064772234
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hashp -> hctl -> nentries
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[1]: hashp -> hctl
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: nentries
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771676
FRAGMENT_COUNT: 6
  ORIGINAL[0]: hashp != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: MemoryContextDelete(hashp -> hcxt)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 -> VAR2 )
  ORIGINAL[2]: hashp -> hcxt
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hcxt
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064772791
FRAGMENT_COUNT: 5
  ORIGINAL[0]: stonesoup_i < stonesoup_num_files
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: stonesoup_filearray[stonesoup_i] = 0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ VAR2 ] = 0
  ORIGINAL[2]: stonesoup_filearray[stonesoup_i]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ VAR2 ]
  ORIGINAL[3]: for (stonesoup_i = 0;stonesoup_i < stonesoup_num_files;++stonesoup_i)
  TYPE[3]: CONTROL_STRUCTURE
  TOKENIZED[3]: for ( VAR1 = 0 ; VAR1 < VAR2 ; ++stonesoup_i )
  ORIGINAL[4]: stonesoup_i
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772639
FRAGMENT_COUNT: 4
  ORIGINAL[0]: num > (2147483647 / 2)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > ( 2147483647 / 2 )
  ORIGINAL[1]: num = (2147483647 / 2)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = ( 2147483647 / 2 )
  ORIGINAL[2]: 2147483647 / 2
  TYPE[2]: CALL
  TOKENIZED[2]: 2147483647 / 2
  ORIGINAL[3]: num
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772603
FRAGMENT_COUNT: 8
  ORIGINAL[0]: tas(&hctlv -> mutex)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( &hctlv -> VAR1 )
  ORIGINAL[1]: &hctlv -> mutex
  TYPE[1]: CALL
  TOKENIZED[1]: &hctlv -> VAR1
  ORIGINAL[2]: hctlv -> mutex
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: s_lock(&hctlv -> mutex,\
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( &hctlv -> VAR1 , \
  ORIGINAL[4]: &hctlv -> mutex
  TYPE[4]: CALL
  TOKENIZED[4]: &hctlv -> VAR1
  ORIGINAL[5]: hctlv -> mutex
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: &hctlv -> mutex
  TYPE[6]: CALL
  TOKENIZED[6]: &hctlv -> VAR1
  ORIGINAL[7]: hctlv -> mutex
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2

CENTER_NODE: 30064772927
FRAGMENT_COUNT: 1
  ORIGINAL[0]: *seq_scan_tables[100]
  TYPE[0]: CALL
  TOKENIZED[0]: *seq_scan_tables [ 100 ]

CENTER_NODE: 30064772434
FRAGMENT_COUNT: 6
  ORIGINAL[0]: new_segnum >= hctl -> nsegs
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= VAR2 -> VAR3
  ORIGINAL[1]: !(hashp -> dir[new_segnum] = seg_alloc(hashp))
  TYPE[1]: CALL
  TOKENIZED[1]: ! ( VAR1 -> VAR2 [ VAR3 ] = FUN1 ( VAR1 ) )
  ORIGINAL[2]: *newlink = ((void *)0)
  TYPE[2]: CALL
  TOKENIZED[2]: *newlink = ( ( void * ) 0 )
  ORIGINAL[3]: *newlink
  TYPE[3]: CALL
  TOKENIZED[3]: *newlink
  ORIGINAL[4]: (void *)0
  TYPE[4]: CALL
  TOKENIZED[4]: ( void * ) 0
  ORIGINAL[5]: newlink
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 47244640274
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771956
FRAGMENT_COUNT: 6
  ORIGINAL[0]: hash_search_with_hash_value(hashp,keyPtr,((hashp -> hash)(keyPtr,hashp -> keysize)),action,foundPtr)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , ( ( VAR1 -> VAR3 ) ( VAR2 , VAR1 -> VAR4 ) ) , VAR5 , VAR6 )
  ORIGINAL[1]: (hashp -> hash)(keyPtr,hashp -> keysize)
  TYPE[1]: CALL
  TOKENIZED[1]: ( VAR1 -> VAR2 ) ( VAR3 , VAR1 -> VAR4 )
  ORIGINAL[2]: hashp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: keyPtr
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: action
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: foundPtr
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064772519
FRAGMENT_COUNT: 5
  ORIGINAL[0]: !segp
  TYPE[0]: CALL
  TOKENIZED[0]: !segp
  ORIGINAL[1]: *_vstart = (void *)segp
  TYPE[1]: CALL
  TOKENIZED[1]: *_vstart = ( void * ) VAR1
  ORIGINAL[2]: (void *)segp
  TYPE[2]: CALL
  TOKENIZED[2]: ( void * ) VAR1
  ORIGINAL[3]: _vstart
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: _val
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772340
FRAGMENT_COUNT: 4
  ORIGINAL[0]: !hashp -> frozen && has_seq_scans(hashp)
  TYPE[0]: CALL
  TOKENIZED[0]: !hashp -> VAR1 && FUN1 ( VAR2 )
  ORIGINAL[1]: !hashp -> frozen
  TYPE[1]: CALL
  TOKENIZED[1]: !hashp -> VAR1
  ORIGINAL[2]: has_seq_scans(hashp)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 )
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477123
FRAGMENT_COUNT: 4
  ORIGINAL[0]: sizeof(HASHHDR )
  TYPE[0]: CALL
  TOKENIZED[0]: sizeof ( VAR1 )
  ORIGINAL[1]: info -> dsize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: dsize
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: info
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477758
FRAGMENT_COUNT: 4
  ORIGINAL[0]: hashp -> isshared
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hashp -> tabname
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: tabname
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477898
FRAGMENT_COUNT: 4
  ORIGINAL[0]: ++stonesoup_global_variable
  TYPE[0]: CALL
  TOKENIZED[0]: ++stonesoup_global_variable
  ORIGINAL[1]: delocalizing_sclat(unprime_sidky,aeronef_acromion)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[2]: unprime_sidky
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: aeronef_acromion
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640375
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772625
FRAGMENT_COUNT: 7
  ORIGINAL[0]: num > 9223372036854775807L / 2
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > 9223372036854775807L / 2
  ORIGINAL[1]: 9223372036854775807L / 2
  TYPE[1]: CALL
  TOKENIZED[1]: 9223372036854775807L / 2
  ORIGINAL[2]: num = 9223372036854775807L / 2
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = 9223372036854775807L / 2
  ORIGINAL[3]: 9223372036854775807L / 2
  TYPE[3]: CALL
  TOKENIZED[3]: 9223372036854775807L / 2
  ORIGINAL[4]: num
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: num
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: i
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 30064772635
FRAGMENT_COUNT: 3
  ORIGINAL[0]: 1L << my_log2(num)
  TYPE[0]: CALL
  TOKENIZED[0]: 1L << FUN1 ( VAR1 )
  ORIGINAL[1]: my_log2(num)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )
  ORIGINAL[2]: num
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771318
FRAGMENT_COUNT: 11
  ORIGINAL[0]: !hashp -> hctl
  TYPE[0]: CALL
  TOKENIZED[0]: !hashp -> VAR1
  ORIGINAL[1]: hashp -> hctl
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: !hashp -> hctl
  TYPE[2]: CALL
  TOKENIZED[2]: !hashp -> VAR1
  ORIGINAL[3]: hashp -> hctl
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hashp -> hctl
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hctl
  TYPE[5]: FIELD_IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: hashp
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: hashp
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: hashp
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1

CENTER_NODE: 68719477128
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hashp -> hash
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hash
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: hashp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064772495
FRAGMENT_COUNT: 9
  ORIGINAL[0]: (((intptr_t )_vstart) & sizeof(long ) - 1) == 0 && (_len & sizeof(long ) - 1) == 0 && _val == 0 && _len <= 1024 && 1024 != 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( ( VAR1 ) VAR2 ) & sizeof ( long ) - 1 ) == 0 && ( VAR3 & sizeof ( long ) - 1 ) == 0 && VAR4 == 0 && VAR3 <= 1024 && 1024 != 0
  ORIGINAL[1]: _start < _stop
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 < VAR2
  ORIGINAL[2]: *(_start++) = 0
  TYPE[2]: CALL
  TOKENIZED[2]: * ( _start++ ) = 0
  ORIGINAL[3]: *(_start++)
  TYPE[3]: CALL
  TOKENIZED[3]: * ( _start++ )
  ORIGINAL[4]: _start++
  TYPE[4]: CALL
  TOKENIZED[4]: _start++
  ORIGINAL[5]: while (_start < _stop)
  TYPE[5]: CONTROL_STRUCTURE
  TOKENIZED[5]: while ( VAR1 < VAR2 )
  ORIGINAL[6]: _start
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: _stop
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: _start
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 68719477120
FRAGMENT_COUNT: 4
  ORIGINAL[0]: nDirEntries < nSegments
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: nDirEntries <<= 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 <<= 1
  ORIGINAL[2]: nDirEntries
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: nDirEntries
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771093
FRAGMENT_COUNT: 20
  ORIGINAL[0]: ss_tc_root != NULL
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != VAR2
  ORIGINAL[1]: dirpath != NULL
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 != VAR2
  ORIGINAL[2]: sprintf(dirpath, \
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , \
  ORIGINAL[3]: retval = 0
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 = 0
  ORIGINAL[4]: stat(dirpath, &st) == -1
  TYPE[4]: CALL
  TOKENIZED[4]: FUN1 ( VAR1 , &st ) == -1
  ORIGINAL[5]: stat(dirpath, &st)
  TYPE[5]: CALL
  TOKENIZED[5]: FUN1 ( VAR1 , &st )
  ORIGINAL[6]: &st
  TYPE[6]: CALL
  TOKENIZED[6]: &st
  ORIGINAL[7]: -1
  TYPE[7]: CALL
  TOKENIZED[7]: -1
  ORIGINAL[8]: retval == 0
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 == 0
  ORIGINAL[9]: free(dirpath)
  TYPE[9]: CALL
  TOKENIZED[9]: FUN1 ( VAR1 )
  ORIGINAL[10]: dirpath
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: NULL
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1
  ORIGINAL[12]: dirpath
  TYPE[12]: IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: ss_tc_root
  TYPE[13]: IDENTIFIER
  TOKENIZED[13]: VAR1
  ORIGINAL[14]: retval
  TYPE[14]: IDENTIFIER
  TOKENIZED[14]: VAR1
  ORIGINAL[15]: dirpath
  TYPE[15]: IDENTIFIER
  TOKENIZED[15]: VAR1
  ORIGINAL[16]: st
  TYPE[16]: IDENTIFIER
  TOKENIZED[16]: VAR1
  ORIGINAL[17]: retval
  TYPE[17]: IDENTIFIER
  TOKENIZED[17]: VAR1
  ORIGINAL[18]: dirpath
  TYPE[18]: IDENTIFIER
  TOKENIZED[18]: VAR1
  ORIGINAL[19]: <global> stonesoup_printf_context
  TYPE[19]: IDENTIFIER
  TOKENIZED[19]: <global> VAR1

CENTER_NODE: 47244640347
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771509
FRAGMENT_COUNT: 3
  ORIGINAL[0]: allocSize = (32 * 4)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = ( 32 * 4 )
  ORIGINAL[1]: 32 * 4
  TYPE[1]: CALL
  TOKENIZED[1]: 32 * 4
  ORIGINAL[2]: allocSize
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477284
FRAGMENT_COUNT: 7
  ORIGINAL[0]: daleville_outsettler != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != 0
  ORIGINAL[1]: formalesque_mohock = &invertedly_petune
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = &invertedly_petune
  ORIGINAL[2]: malinda_chronologist = &formalesque_mohock
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = &formalesque_mohock
  ORIGINAL[3]: &formalesque_mohock
  TYPE[3]: CALL
  TOKENIZED[3]: &formalesque_mohock
  ORIGINAL[4]: malinda_chronologist
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: formalesque_mohock
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: malinda_chronologist
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 47244640380
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477069
FRAGMENT_COUNT: 6
  ORIGINAL[0]: hctl -> nsegs < nsegs
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 < VAR2
  ORIGINAL[1]: *segp
  TYPE[1]: CALL
  TOKENIZED[1]: *segp
  ORIGINAL[2]: *segp
  TYPE[2]: CALL
  TOKENIZED[2]: *segp
  ORIGINAL[3]: segp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: segp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: segp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064772245
FRAGMENT_COUNT: 3
  ORIGINAL[0]: !hashp -> frozen
  TYPE[0]: CALL
  TOKENIZED[0]: !hashp -> VAR1
  ORIGINAL[1]: register_seq_scan(hashp)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )
  ORIGINAL[2]: hashp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640388
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771174
FRAGMENT_COUNT: 3
  ORIGINAL[0]: !f
  TYPE[0]: CALL
  TOKENIZED[0]: !f
  ORIGINAL[1]: f
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: f
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640345
FRAGMENT_COUNT: 1
  ORIGINAL[0]: for (;;)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: for ( ; ; )

CENTER_NODE: 30064771636
FRAGMENT_COUNT: 6
  ORIGINAL[0]: ((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1))
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR1 ) ( 8 - 1 ) )
  ORIGINAL[1]: ((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1)
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 )
  ORIGINAL[2]: (intptr_t )(sizeof(HASHELEMENT ))
  TYPE[2]: CALL
  TOKENIZED[2]: ( VAR1 ) ( sizeof ( VAR2 ) )
  ORIGINAL[3]: 8 - 1
  TYPE[3]: CALL
  TOKENIZED[3]: 8 - 1
  ORIGINAL[4]: ~((intptr_t )(8 - 1))
  TYPE[4]: CALL
  TOKENIZED[4]: ~ ( ( VAR1 ) ( 8 - 1 ) )
  ORIGINAL[5]: intptr_t
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719476798
FRAGMENT_COUNT: 7
  ORIGINAL[0]: getenv(\
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( \
  ORIGINAL[1]: stonesoup_lsize = 0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = 0
  ORIGINAL[2]: stonesoup_tainted_file_name = getenv(stonesoup_env_var_name)
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = FUN1 ( VAR2 )
  ORIGINAL[3]: getenv(stonesoup_env_var_name)
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( VAR1 )
  ORIGINAL[4]: stonesoup_tainted_file_name
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: stonesoup_env_var_name
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: stonesoup_tainted_file_name
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

