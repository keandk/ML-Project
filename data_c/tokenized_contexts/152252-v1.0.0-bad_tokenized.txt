# Tokenized code fragments for 152252-v1.0.0-bad
# Total center nodes processed: 25
# Total code fragments found: 169

CENTER_NODE: 30064772175
FRAGMENT_COUNT: 5
  ORIGINAL[0]: g_slist_find_custom(conv -> data_list,((gpointer *)(&temp)),p_compare)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , ( ( VAR3 * ) ( &temp ) ) , VAR4 )
  ORIGINAL[1]: conv -> data_list
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: (gpointer *)(&temp)
  TYPE[2]: CALL
  TOKENIZED[2]: ( VAR1 * ) ( &temp )
  ORIGINAL[3]: data_list
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: conv
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771982
FRAGMENT_COUNT: 10
  ORIGINAL[0]: key . addr2 =  *addr2
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . VAR2 = *addr2
  ORIGINAL[1]: key . addr2
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2
  ORIGINAL[2]: *addr2
  TYPE[2]: CALL
  TOKENIZED[2]: *addr2
  ORIGINAL[3]: addr2
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: key
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: addr2
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: key
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: key
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: key
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: key
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 30064772151
FRAGMENT_COUNT: 8
  ORIGINAL[0]: ap -> proto > bp -> proto
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 > VAR3 -> VAR2
  ORIGINAL[1]: ap -> proto
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: bp -> proto
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: bp -> proto
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: proto
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: bp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: bp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: bp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 68719477063
FRAGMENT_COUNT: 2
  ORIGINAL[0]: (conversation_t *)value
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 * ) VAR2
  ORIGINAL[1]: value
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1

CENTER_NODE: 68719476861
FRAGMENT_COUNT: 5
  ORIGINAL[0]: *key = (const conversation_key *)v
  TYPE[0]: CALL
  TOKENIZED[0]: *key = ( const VAR1 * ) VAR2
  ORIGINAL[1]: hash_val = 0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = 0
  ORIGINAL[2]: hash_val
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hash_val
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hash_val
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771612
FRAGMENT_COUNT: 4
  ORIGINAL[0]: *v1 = (const conversation_key *)v
  TYPE[0]: CALL
  TOKENIZED[0]: *v1 = ( const VAR1 * ) VAR2
  ORIGINAL[1]: (const conversation_key *)v
  TYPE[1]: CALL
  TOKENIZED[1]: ( const VAR1 * ) VAR2
  ORIGINAL[2]: v1
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: v
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772195
FRAGMENT_COUNT: 5
  ORIGINAL[0]: conv -> data_list = g_slist_remove(conv -> data_list,(item -> data))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = FUN1 ( VAR1 -> VAR2 , ( VAR3 -> VAR4 ) )
  ORIGINAL[1]: conv -> data_list
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: g_slist_remove(conv -> data_list,(item -> data))
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 -> VAR2 , ( VAR3 -> VAR4 ) )
  ORIGINAL[3]: item
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: item
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771101
FRAGMENT_COUNT: 5
  ORIGINAL[0]: stat(dirpath, &st) == -1
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , &st ) == -1
  ORIGINAL[1]: retval = mkdir(dirpath, 0700)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = FUN1 ( VAR2 , 0700 )
  ORIGINAL[2]: mkdir(dirpath, 0700)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , 0700 )
  ORIGINAL[3]: retval
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: dirpath
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771148
FRAGMENT_COUNT: 5
  ORIGINAL[0]: stonesoup_tainted_file != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != 0
  ORIGINAL[1]: *stonesoup_tainted_buff = ((char *)(malloc(sizeof(char ) * (stonesoup_lsize + 1))))
  TYPE[1]: CALL
  TOKENIZED[1]: *stonesoup_tainted_buff = ( ( char * ) ( FUN1 ( sizeof ( char ) * ( VAR1 + 1 ) ) ) )
  ORIGINAL[2]: *stonesoup_tainted_buff
  TYPE[2]: CALL
  TOKENIZED[2]: *stonesoup_tainted_buff
  ORIGINAL[3]: (char *)(malloc(sizeof(char ) * (stonesoup_lsize + 1)))
  TYPE[3]: CALL
  TOKENIZED[3]: ( char * ) ( FUN1 ( sizeof ( char ) * ( VAR1 + 1 ) ) )
  ORIGINAL[4]: stonesoup_tainted_buff
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772156
FRAGMENT_COUNT: 4
  ORIGINAL[0]: *p1 = (se_alloc(sizeof(conv_proto_data )))
  TYPE[0]: CALL
  TOKENIZED[0]: *p1 = ( FUN1 ( sizeof ( VAR1 ) ) )
  ORIGINAL[1]: se_alloc(sizeof(conv_proto_data ))
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( sizeof ( VAR1 ) )
  ORIGINAL[2]: p1
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: p1
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772300
FRAGMENT_COUNT: 4
  ORIGINAL[0]: ((char *)((void *)soixantine_brakemaker)) != 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( char * ) ( ( void * ) VAR1 ) ) != 0
  ORIGINAL[1]: (char *)((void *)soixantine_brakemaker)
  TYPE[1]: CALL
  TOKENIZED[1]: ( char * ) ( ( void * ) VAR1 )
  ORIGINAL[2]: (void *)soixantine_brakemaker
  TYPE[2]: CALL
  TOKENIZED[2]: ( void * ) VAR1
  ORIGINAL[3]: soixantine_brakemaker
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771483
FRAGMENT_COUNT: 4
  ORIGINAL[0]: ADD_ADDRESS_TO_HASH_index = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = 0
  ORIGINAL[1]: for (ADD_ADDRESS_TO_HASH_index = 0;ADD_ADDRESS_TO_HASH_index < (&key -> addr1) -> len;ADD_ADDRESS_TO_HASH_index++)
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: for ( VAR1 = 0 ; VAR1 < ( &key -> VAR2 ) -> VAR3 ; ADD_ADDRESS_TO_HASH_index++ )
  ORIGINAL[2]: ADD_ADDRESS_TO_HASH_index
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: ADD_ADDRESS_TO_HASH_index
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477442
FRAGMENT_COUNT: 6
  ORIGINAL[0]: !(conversation -> options & 0x08)
  TYPE[0]: CALL
  TOKENIZED[0]: ! ( VAR1 -> VAR2 & 0x08 )
  ORIGINAL[1]: conversation -> options
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: conversation_create_from_template(conversation,0,port_a)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , 0 , VAR2 )
  ORIGINAL[3]: conversation
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: conversation
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: conversation
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771696
FRAGMENT_COUNT: 3
  ORIGINAL[0]: conversation_hashtable_no_port2 = g_hash_table_new(conversation_hash_no_port2,conversation_match_no_port2)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = FUN1 ( VAR2 , VAR3 )
  ORIGINAL[1]: g_hash_table_new(conversation_hash_no_port2,conversation_match_no_port2)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[2]: <global> conversation_hashtable_no_port2
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: <global> VAR1

CENTER_NODE: 30064771591
FRAGMENT_COUNT: 6
  ORIGINAL[0]: mkdir(\
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( \
  ORIGINAL[1]: stonesoup_read_taint(&thebaines_paddlefishes,\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( &thebaines_paddlefishes , \
  ORIGINAL[2]: &thebaines_paddlefishes
  TYPE[2]: CALL
  TOKENIZED[2]: &thebaines_paddlefishes
  ORIGINAL[3]: thebaines_paddlefishes
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: thebaines_paddlefishes
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: thebaines_paddlefishes
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477558
FRAGMENT_COUNT: 4
  ORIGINAL[0]: pinfo -> fd -> num
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[1]: pinfo -> src
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: src
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: pinfo
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771565
FRAGMENT_COUNT: 14
  ORIGINAL[0]: &v1 -> addr2
  TYPE[0]: CALL
  TOKENIZED[0]: &v1 -> VAR1
  ORIGINAL[1]: v1 -> addr2
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: ((&v1 -> addr2) -> type) == AT_NONE
  TYPE[2]: CALL
  TOKENIZED[2]: ( ( &v1 -> VAR1 ) -> VAR2 ) == VAR3
  ORIGINAL[3]: (&v1 -> addr2) -> type
  TYPE[3]: CALL
  TOKENIZED[3]: ( &v1 -> VAR1 ) -> VAR2
  ORIGINAL[4]: &v1 -> addr2
  TYPE[4]: CALL
  TOKENIZED[4]: &v1 -> VAR1
  ORIGINAL[5]: v1 -> addr2
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: (&v1 -> addr2) -> len
  TYPE[6]: CALL
  TOKENIZED[6]: ( &v1 -> VAR1 ) -> VAR2
  ORIGINAL[7]: &v1 -> addr2
  TYPE[7]: CALL
  TOKENIZED[7]: &v1 -> VAR1
  ORIGINAL[8]: v1 -> addr2
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: &v1 -> addr2
  TYPE[9]: CALL
  TOKENIZED[9]: &v1 -> VAR1
  ORIGINAL[10]: v1 -> addr2
  TYPE[10]: CALL
  TOKENIZED[10]: VAR1 -> VAR2
  ORIGINAL[11]: &v1 -> addr2
  TYPE[11]: CALL
  TOKENIZED[11]: &v1 -> VAR1
  ORIGINAL[12]: v1 -> addr2
  TYPE[12]: CALL
  TOKENIZED[12]: VAR1 -> VAR2
  ORIGINAL[13]: len
  TYPE[13]: FIELD_IDENTIFIER
  TOKENIZED[13]: VAR1

CENTER_NODE: 68719477097
FRAGMENT_COUNT: 9
  ORIGINAL[0]: conv -> key_ptr
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: ((void *)0) == chain_head
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( void * ) 0 ) == VAR1
  ORIGINAL[2]: conv -> next
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: conv -> last
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: g_hash_table_insert(hashtable,(conv -> key_ptr),conv)
  TYPE[4]: CALL
  TOKENIZED[4]: FUN1 ( VAR1 , ( VAR2 -> VAR3 ) , VAR2 )
  ORIGINAL[5]: conv -> key_ptr
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: conv
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashtable
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: conv
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 30064771463
FRAGMENT_COUNT: 10
  ORIGINAL[0]: (&v2 -> addr1) -> type
  TYPE[0]: CALL
  TOKENIZED[0]: ( &v2 -> VAR1 ) -> VAR2
  ORIGINAL[1]: &v2 -> addr1
  TYPE[1]: CALL
  TOKENIZED[1]: &v2 -> VAR1
  ORIGINAL[2]: v2 -> addr1
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: ((&v1 -> addr1) -> type) == AT_NONE
  TYPE[3]: CALL
  TOKENIZED[3]: ( ( &v1 -> VAR1 ) -> VAR2 ) == VAR3
  ORIGINAL[4]: (&v2 -> addr1) -> len
  TYPE[4]: CALL
  TOKENIZED[4]: ( &v2 -> VAR1 ) -> VAR2
  ORIGINAL[5]: &v2 -> addr1
  TYPE[5]: CALL
  TOKENIZED[5]: &v2 -> VAR1
  ORIGINAL[6]: v2 -> addr1
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: &v2 -> addr1
  TYPE[7]: CALL
  TOKENIZED[7]: &v2 -> VAR1
  ORIGINAL[8]: v2 -> addr1
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: len
  TYPE[9]: FIELD_IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 30064772205
FRAGMENT_COUNT: 9
  ORIGINAL[0]: conversation = find_conversation(pinfo -> fd -> num,addr_a,addr_b,ptype,port_a,port_b,0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = FUN1 ( VAR2 -> VAR3 -> VAR4 , VAR5 , VAR6 , VAR7 , VAR8 , VAR9 , 0 )
  ORIGINAL[1]: find_conversation(pinfo -> fd -> num,addr_a,addr_b,ptype,port_a,port_b,0)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 -> VAR2 -> VAR3 , VAR4 , VAR5 , VAR6 , VAR7 , VAR8 , 0 )
  ORIGINAL[2]: pinfo -> fd -> num
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[3]: conversation
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: addr_a
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: addr_b
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: ptype
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: port_a
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: port_b
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 68719476941
FRAGMENT_COUNT: 9
  ORIGINAL[0]: ADD_ADDRESS_TO_HASH_index = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = 0
  ORIGINAL[1]: ADD_ADDRESS_TO_HASH_index < (&key -> addr1) -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 < ( &key -> VAR2 ) -> VAR3
  ORIGINAL[2]: (&key -> addr1) -> len
  TYPE[2]: CALL
  TOKENIZED[2]: ( &key -> VAR1 ) -> VAR2
  ORIGINAL[3]: ADD_ADDRESS_TO_HASH_index++
  TYPE[3]: CALL
  TOKENIZED[3]: ADD_ADDRESS_TO_HASH_index++
  ORIGINAL[4]: ADD_ADDRESS_TO_HASH_index
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: ADD_ADDRESS_TO_HASH_index
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: key
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: ADD_ADDRESS_TO_HASH_index
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: ADD_ADDRESS_TO_HASH_index
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 30064771314
FRAGMENT_COUNT: 19
  ORIGINAL[0]: ((&v1 -> addr2) -> type) == ((&v2 -> addr2) -> type)
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( &v1 -> VAR1 ) -> VAR2 ) == ( ( &v2 -> VAR1 ) -> VAR2 )
  ORIGINAL[1]: ((&v1 -> addr2) -> type) == AT_NONE || (&v1 -> addr2) -> len == (&v2 -> addr2) -> len && memcmp((&v1 -> addr2) -> data,(&v2 -> addr2) -> data,((&v1 -> addr2) -> len)) == 0
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( &v1 -> VAR1 ) -> VAR2 ) == VAR3 || ( &v1 -> VAR1 ) -> VAR4 == ( &v2 -> VAR1 ) -> VAR4 && FUN1 ( ( &v1 -> VAR1 ) -> VAR5 , ( &v2 -> VAR1 ) -> VAR5 , ( ( &v1 -> VAR1 ) -> VAR4 ) ) == 0
  ORIGINAL[2]: ((&v1 -> addr2) -> type) == AT_NONE
  TYPE[2]: CALL
  TOKENIZED[2]: ( ( &v1 -> VAR1 ) -> VAR2 ) == VAR3
  ORIGINAL[3]: (&v1 -> addr2) -> type
  TYPE[3]: CALL
  TOKENIZED[3]: ( &v1 -> VAR1 ) -> VAR2
  ORIGINAL[4]: (&v1 -> addr2) -> len == (&v2 -> addr2) -> len && memcmp((&v1 -> addr2) -> data,(&v2 -> addr2) -> data,((&v1 -> addr2) -> len)) == 0
  TYPE[4]: CALL
  TOKENIZED[4]: ( &v1 -> VAR1 ) -> VAR2 == ( &v2 -> VAR1 ) -> VAR2 && FUN1 ( ( &v1 -> VAR1 ) -> VAR3 , ( &v2 -> VAR1 ) -> VAR3 , ( ( &v1 -> VAR1 ) -> VAR2 ) ) == 0
  ORIGINAL[5]: (&v1 -> addr2) -> len == (&v2 -> addr2) -> len
  TYPE[5]: CALL
  TOKENIZED[5]: ( &v1 -> VAR1 ) -> VAR2 == ( &v2 -> VAR1 ) -> VAR2
  ORIGINAL[6]: (&v1 -> addr2) -> len
  TYPE[6]: CALL
  TOKENIZED[6]: ( &v1 -> VAR1 ) -> VAR2
  ORIGINAL[7]: &v1 -> addr2
  TYPE[7]: CALL
  TOKENIZED[7]: &v1 -> VAR1
  ORIGINAL[8]: v1 -> addr2
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: (&v2 -> addr2) -> len
  TYPE[9]: CALL
  TOKENIZED[9]: ( &v2 -> VAR1 ) -> VAR2
  ORIGINAL[10]: &v2 -> addr2
  TYPE[10]: CALL
  TOKENIZED[10]: &v2 -> VAR1
  ORIGINAL[11]: v2 -> addr2
  TYPE[11]: CALL
  TOKENIZED[11]: VAR1 -> VAR2
  ORIGINAL[12]: addr2
  TYPE[12]: FIELD_IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: len
  TYPE[13]: FIELD_IDENTIFIER
  TOKENIZED[13]: VAR1
  ORIGINAL[14]: addr2
  TYPE[14]: FIELD_IDENTIFIER
  TOKENIZED[14]: VAR1
  ORIGINAL[15]: len
  TYPE[15]: FIELD_IDENTIFIER
  TOKENIZED[15]: VAR1
  ORIGINAL[16]: AT_NONE
  TYPE[16]: IDENTIFIER
  TOKENIZED[16]: VAR1
  ORIGINAL[17]: v1
  TYPE[17]: IDENTIFIER
  TOKENIZED[17]: VAR1
  ORIGINAL[18]: v2
  TYPE[18]: IDENTIFIER
  TOKENIZED[18]: VAR1

CENTER_NODE: 30064772202
FRAGMENT_COUNT: 3
  ORIGINAL[0]: conversation -> dissector_handle = handle
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = VAR3
  ORIGINAL[1]: conversation -> dissector_handle
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: handle
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719476845
FRAGMENT_COUNT: 5
  ORIGINAL[0]: conversation -> options & 0x02
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 & 0x02
  ORIGINAL[1]: conversation -> options
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: conversation -> options
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: options
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: conversation
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477164
FRAGMENT_COUNT: 11
  ORIGINAL[0]: conv -> key_ptr
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: conv == chain_head
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 == VAR2
  ORIGINAL[2]: cur != conv
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 != VAR2
  ORIGINAL[3]: conv -> next
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: conv -> next
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: conv
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: cur
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: conv
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: conv
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: conv
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: conv
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1

