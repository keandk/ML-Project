# Tokenized code fragments for 152833-v1.0.0-bad
# Total center nodes processed: 41
# Total code fragments found: 176

CENTER_NODE: 30064771495
FRAGMENT_COUNT: 2
  ORIGINAL[0]: ((intptr_t )entrysize) + (8 - 1)
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( VAR1 ) VAR2 ) + ( 8 - 1 )
  ORIGINAL[1]: 8 - 1
  TYPE[1]: CALL
  TOKENIZED[1]: 8 - 1

CENTER_NODE: 30064772379
FRAGMENT_COUNT: 4
  ORIGINAL[0]: num > (2147483647 / 2)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > ( 2147483647 / 2 )
  ORIGINAL[1]: num = (2147483647 / 2)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = ( 2147483647 / 2 )
  ORIGINAL[2]: 2147483647 / 2
  TYPE[2]: CALL
  TOKENIZED[2]: 2147483647 / 2
  ORIGINAL[3]: num
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477141
FRAGMENT_COUNT: 4
  ORIGINAL[0]: hashp -> hash
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hash
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: keyPtr
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477270
FRAGMENT_COUNT: 11
  ORIGINAL[0]: hctlv -> num_partitions
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hctlv -> mutex
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctlv -> mutex
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hctlv -> freeList
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: newElement != ((void *)0)
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[5]: hctlv -> num_partitions
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: hctlv -> mutex
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: hctlv -> nelem_alloc
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2
  ORIGINAL[8]: num_partitions
  TYPE[8]: FIELD_IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: hctlv
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: hctlv
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1

CENTER_NODE: 47244640334
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772460
FRAGMENT_COUNT: 5
  ORIGINAL[0]: pygidid_becumber > 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > 0
  ORIGINAL[1]: tracepoint(stonesoup_trace, weakness_start, \
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 , \
  ORIGINAL[2]: stonesoup_trace
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: weakness_start
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: stonesoup_trace
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772527
FRAGMENT_COUNT: 3
  ORIGINAL[0]: ++stonesoup_global_variable
  TYPE[0]: CALL
  TOKENIZED[0]: ++stonesoup_global_variable
  ORIGINAL[1]: <global> stonesoup_global_variable
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: <global> VAR1
  ORIGINAL[2]: skeined_uncomplacently
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640272
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640387
FRAGMENT_COUNT: 1
  ORIGINAL[0]: tas(&hctlv -> mutex)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( &hctlv -> VAR1 )

CENTER_NODE: 47244640302
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771663
FRAGMENT_COUNT: 9
  ORIGINAL[0]: hashp != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: (void *)0
  TYPE[1]: CALL
  TOKENIZED[1]: ( void * ) 0
  ORIGINAL[2]: hash_stats(\
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( \
  ORIGINAL[3]: MemoryContextDelete(hashp -> hcxt)
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( VAR1 -> VAR2 )
  ORIGINAL[4]: hashp -> hcxt
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hcxt
  TYPE[5]: FIELD_IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: hashp
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 30064772414
FRAGMENT_COUNT: 6
  ORIGINAL[0]: i < num_seq_scans
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: seq_scan_tables[i] == hashp
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ VAR2 ] == VAR3
  ORIGINAL[2]: seq_scan_tables[i]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ VAR2 ]
  ORIGINAL[3]: <global> seq_scan_tables
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: <global> VAR1
  ORIGINAL[4]: i
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771466
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hctl -> keysize = sizeof(char *)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = sizeof ( char * )
  ORIGINAL[1]: sizeof(char *)
  TYPE[1]: CALL
  TOKENIZED[1]: sizeof ( char * )
  ORIGINAL[2]: char
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: char

CENTER_NODE: 30064771981
FRAGMENT_COUNT: 5
  ORIGINAL[0]: status -> curEntry = ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = ( ( void * ) 0 )
  ORIGINAL[1]: status -> curEntry
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: (void *)0
  TYPE[2]: CALL
  TOKENIZED[2]: ( void * ) 0
  ORIGINAL[3]: curEntry
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: status
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477457
FRAGMENT_COUNT: 6
  ORIGINAL[0]: (((intptr_t )_vstart) & sizeof(long ) - 1) == 0 && (_len & sizeof(long ) - 1) == 0 && _val == 0 && _len <= 1024 && 1024 != 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( ( VAR1 ) VAR2 ) & sizeof ( long ) - 1 ) == 0 && ( VAR3 & sizeof ( long ) - 1 ) == 0 && VAR4 == 0 && VAR3 <= 1024 && 1024 != 0
  ORIGINAL[1]: *_start = (long *)_vstart
  TYPE[1]: CALL
  TOKENIZED[1]: *_start = ( long * ) VAR1
  ORIGINAL[2]: *_stop = (long *)(((char *)_start) + _len)
  TYPE[2]: CALL
  TOKENIZED[2]: *_stop = ( long * ) ( ( ( char * ) VAR1 ) + VAR2 )
  ORIGINAL[3]: (long *)(((char *)_start) + _len)
  TYPE[3]: CALL
  TOKENIZED[3]: ( long * ) ( ( ( char * ) VAR1 ) + VAR2 )
  ORIGINAL[4]: _stop
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: _stop
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477542
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hashp -> isshared
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: isshared
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: hashp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771669
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (hashp -> hash)(keyPtr,hashp -> keysize)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 -> VAR2 ) ( VAR3 , VAR1 -> VAR4 )
  ORIGINAL[1]: hashp -> hash
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hash
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: keyPtr
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477083
FRAGMENT_COUNT: 6
  ORIGINAL[0]: elementAllocCnt = (choose_nelem_alloc(entrysize))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = ( FUN1 ( VAR2 ) )
  ORIGINAL[1]: nElementAllocs = (num_entries - 1) / elementAllocCnt + 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = ( VAR2 - 1 ) / VAR3 + 1
  ORIGINAL[2]: (num_entries - 1) / elementAllocCnt + 1
  TYPE[2]: CALL
  TOKENIZED[2]: ( VAR1 - 1 ) / VAR2 + 1
  ORIGINAL[3]: nElementAllocs
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: num_entries
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: nElementAllocs
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719476855
FRAGMENT_COUNT: 7
  ORIGINAL[0]: (((intptr_t )_vstart) & sizeof(long ) - 1) == 0 && (_len & sizeof(long ) - 1) == 0 && _val == 0 && _len <= 1024 && 1024 != 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( ( VAR1 ) VAR2 ) & sizeof ( long ) - 1 ) == 0 && ( VAR3 & sizeof ( long ) - 1 ) == 0 && VAR4 == 0 && VAR3 <= 1024 && 1024 != 0
  ORIGINAL[1]: ((char *)_start) + _len
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( char * ) VAR1 ) + VAR2
  ORIGINAL[2]: (char *)_start
  TYPE[2]: CALL
  TOKENIZED[2]: ( char * ) VAR1
  ORIGINAL[3]: _len
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: _len
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: _len
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: _len
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 30064771168
FRAGMENT_COUNT: 4
  ORIGINAL[0]: strncmp(key1,key2,keysize - 1)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 - 1 )
  ORIGINAL[1]: keysize - 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 - 1
  ORIGINAL[2]: key1
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: key2
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640373
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477107
FRAGMENT_COUNT: 2
  ORIGINAL[0]: sizeof(HASHHDR )
  TYPE[0]: CALL
  TOKENIZED[0]: sizeof ( VAR1 )
  ORIGINAL[1]: HASHHDR
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1

CENTER_NODE: 47244640386
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771645
FRAGMENT_COUNT: 4
  ORIGINAL[0]: nBuckets = next_pow2_long((num_entries - 1) / 1 + 1)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = FUN1 ( ( VAR2 - 1 ) / 1 + 1 )
  ORIGINAL[1]: next_pow2_long((num_entries - 1) / 1 + 1)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( ( VAR1 - 1 ) / 1 + 1 )
  ORIGINAL[2]: nBuckets
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: nSegments
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771554
FRAGMENT_COUNT: 6
  ORIGINAL[0]: !hashp -> dir
  TYPE[0]: CALL
  TOKENIZED[0]: !hashp -> VAR1
  ORIGINAL[1]: (hashp -> alloc)((hctl -> dsize) * sizeof(HASHSEGMENT ))
  TYPE[1]: CALL
  TOKENIZED[1]: ( VAR1 -> VAR2 ) ( ( VAR3 -> VAR4 ) * sizeof ( VAR5 ) )
  ORIGINAL[2]: hashp -> alloc
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: alloc
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hctl
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771751
FRAGMENT_COUNT: 23
  ORIGINAL[0]: currBucket != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: currBucket -> hashvalue == hashvalue && match((((char *)currBucket) + (((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1)))),keyPtr,keysize) == 0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 == VAR2 && FUN1 ( ( ( ( char * ) VAR1 ) + ( ( ( VAR3 ) ( sizeof ( VAR4 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR3 ) ( 8 - 1 ) ) ) ) , VAR5 , VAR6 ) == 0
  ORIGINAL[2]: currBucket -> hashvalue == hashvalue
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 == VAR2
  ORIGINAL[3]: currBucket -> hashvalue
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: match((((char *)currBucket) + (((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1)))),keyPtr,keysize) == 0
  TYPE[4]: CALL
  TOKENIZED[4]: FUN1 ( ( ( ( char * ) VAR1 ) + ( ( ( VAR2 ) ( sizeof ( VAR3 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR2 ) ( 8 - 1 ) ) ) ) , VAR4 , VAR5 ) == 0
  ORIGINAL[5]: match((((char *)currBucket) + (((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1)))),keyPtr,keysize)
  TYPE[5]: CALL
  TOKENIZED[5]: FUN1 ( ( ( ( char * ) VAR1 ) + ( ( ( VAR2 ) ( sizeof ( VAR3 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR2 ) ( 8 - 1 ) ) ) ) , VAR4 , VAR5 )
  ORIGINAL[6]: ((char *)currBucket) + (((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1)))
  TYPE[6]: CALL
  TOKENIZED[6]: ( ( char * ) VAR1 ) + ( ( ( VAR2 ) ( sizeof ( VAR3 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR2 ) ( 8 - 1 ) ) )
  ORIGINAL[7]: (char *)currBucket
  TYPE[7]: CALL
  TOKENIZED[7]: ( char * ) VAR1
  ORIGINAL[8]: ((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1))
  TYPE[8]: CALL
  TOKENIZED[8]: ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR1 ) ( 8 - 1 ) )
  ORIGINAL[9]: ((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1)
  TYPE[9]: CALL
  TOKENIZED[9]: ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 )
  ORIGINAL[10]: (intptr_t )(sizeof(HASHELEMENT ))
  TYPE[10]: CALL
  TOKENIZED[10]: ( VAR1 ) ( sizeof ( VAR2 ) )
  ORIGINAL[11]: sizeof(HASHELEMENT )
  TYPE[11]: CALL
  TOKENIZED[11]: sizeof ( VAR1 )
  ORIGINAL[12]: 8 - 1
  TYPE[12]: CALL
  TOKENIZED[12]: 8 - 1
  ORIGINAL[13]: ~((intptr_t )(8 - 1))
  TYPE[13]: CALL
  TOKENIZED[13]: ~ ( ( VAR1 ) ( 8 - 1 ) )
  ORIGINAL[14]: (intptr_t )(8 - 1)
  TYPE[14]: CALL
  TOKENIZED[14]: ( VAR1 ) ( 8 - 1 )
  ORIGINAL[15]: 8 - 1
  TYPE[15]: CALL
  TOKENIZED[15]: 8 - 1
  ORIGINAL[16]: hashvalue
  TYPE[16]: IDENTIFIER
  TOKENIZED[16]: VAR1
  ORIGINAL[17]: currBucket
  TYPE[17]: IDENTIFIER
  TOKENIZED[17]: VAR1
  ORIGINAL[18]: intptr_t
  TYPE[18]: IDENTIFIER
  TOKENIZED[18]: VAR1
  ORIGINAL[19]: HASHELEMENT
  TYPE[19]: IDENTIFIER
  TOKENIZED[19]: VAR1
  ORIGINAL[20]: intptr_t
  TYPE[20]: IDENTIFIER
  TOKENIZED[20]: VAR1
  ORIGINAL[21]: keyPtr
  TYPE[21]: IDENTIFIER
  TOKENIZED[21]: VAR1
  ORIGINAL[22]: keysize
  TYPE[22]: IDENTIFIER
  TOKENIZED[22]: VAR1

CENTER_NODE: 68719477553
FRAGMENT_COUNT: 5
  ORIGINAL[0]: limit < num
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: i++
  TYPE[1]: CALL
  TOKENIZED[1]: i++
  ORIGINAL[2]: i
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: i
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: i
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476785
FRAGMENT_COUNT: 6
  ORIGINAL[0]: vfprintf(stonesoup_printf_context, format, argptr)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 )
  ORIGINAL[1]: argptr
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: <global> stonesoup_printf_context
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: <global> VAR1
  ORIGINAL[3]: format
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: argptr
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: argptr
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 47244640343
FRAGMENT_COUNT: 1
  ORIGINAL[0]: for (;;)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: for ( ; ; )

CENTER_NODE: 30064772255
FRAGMENT_COUNT: 5
  ORIGINAL[0]: sizeof(HASHBUCKET ) * (hashp -> ssize)
  TYPE[0]: CALL
  TOKENIZED[0]: sizeof ( VAR1 ) * ( VAR2 -> VAR3 )
  ORIGINAL[1]: sizeof(HASHBUCKET )
  TYPE[1]: CALL
  TOKENIZED[1]: sizeof ( VAR1 )
  ORIGINAL[2]: hashp -> ssize
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: HASHBUCKET
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640345
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772091
FRAGMENT_COUNT: 10
  ORIGINAL[0]: hctl -> max_bucket + 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 + 1
  ORIGINAL[1]: hctl -> max_bucket
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctl -> max_bucket
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: max_bucket
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hctl
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hctl
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hctl
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hctl
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: hctl
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: hctl
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 30064772083
FRAGMENT_COUNT: 8
  ORIGINAL[0]: hashp -> tabname
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: !hashp -> frozen && has_seq_scans(hashp)
  TYPE[1]: CALL
  TOKENIZED[1]: !hashp -> VAR1 && FUN1 ( VAR2 )
  ORIGINAL[2]: elog_finish(20,\
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( 20 , \
  ORIGINAL[3]: hashp -> tabname
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: tabname
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 30064772068
FRAGMENT_COUNT: 6
  ORIGINAL[0]: !status -> hashp -> frozen
  TYPE[0]: CALL
  TOKENIZED[0]: !status -> VAR1 -> VAR2
  ORIGINAL[1]: status -> hashp -> frozen
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[2]: deregister_seq_scan(status -> hashp)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 -> VAR2 )
  ORIGINAL[3]: status -> hashp
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hashp
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: status
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064772375
FRAGMENT_COUNT: 3
  ORIGINAL[0]: 1L << my_log2(num)
  TYPE[0]: CALL
  TOKENIZED[0]: 1L << FUN1 ( VAR1 )
  ORIGINAL[1]: my_log2(num)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )
  ORIGINAL[2]: num
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771087
FRAGMENT_COUNT: 3
  ORIGINAL[0]: ss_tc_root != NULL
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != VAR2
  ORIGINAL[1]: strlen(ss_tc_root) + strlen(\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 ) + FUN1 ( \
  ORIGINAL[2]: strlen(\
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( \

CENTER_NODE: 30064772533
FRAGMENT_COUNT: 1
  ORIGINAL[0]: *seq_scan_tables[100]
  TYPE[0]: CALL
  TOKENIZED[0]: *seq_scan_tables [ 100 ]

CENTER_NODE: 30064772058
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (curElem = segp[segment_ndx]) == ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 = VAR2 [ VAR3 ] ) == ( ( void * ) 0 )
  ORIGINAL[1]: (void *)(((char *)curElem) + (((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1))))
  TYPE[1]: CALL
  TOKENIZED[1]: ( void * ) ( ( ( char * ) VAR1 ) + ( ( ( VAR2 ) ( sizeof ( VAR3 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR2 ) ( 8 - 1 ) ) ) )
  ORIGINAL[2]: ((char *)curElem) + (((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1)))
  TYPE[2]: CALL
  TOKENIZED[2]: ( ( char * ) VAR1 ) + ( ( ( VAR2 ) ( sizeof ( VAR3 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR2 ) ( 8 - 1 ) ) )
  ORIGINAL[3]: (char *)curElem
  TYPE[3]: CALL
  TOKENIZED[3]: ( char * ) VAR1
  ORIGINAL[4]: ((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1))
  TYPE[4]: CALL
  TOKENIZED[4]: ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR1 ) ( 8 - 1 ) )

CENTER_NODE: 47244640266
FRAGMENT_COUNT: 1
  ORIGINAL[0]: stonesoup_tainted_file != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != 0

CENTER_NODE: 47244640378
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771974
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hashp -> hctl -> nentries
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[1]: hashp -> hctl
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: nentries
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1

