# Tokenized code fragments for 152086-v1.0.0-bad
# Total center nodes processed: 33
# Total code fragments found: 180

CENTER_NODE: 68719476738
FRAGMENT_COUNT: 2
  ORIGINAL[0]: (struct mg_connection*) stonesoup_printf_context
  TYPE[0]: CALL
  TOKENIZED[0]: ( struct mg_connection* ) VAR1
  ORIGINAL[1]: <global> stonesoup_printf_context
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: <global> VAR1

CENTER_NODE: 30064772119
FRAGMENT_COUNT: 10
  ORIGINAL[0]: ap -> proto > bp -> proto
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 > VAR3 -> VAR2
  ORIGINAL[1]: ap -> proto
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: bp -> proto
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: ap -> proto == bp -> proto
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2 == VAR3 -> VAR2
  ORIGINAL[4]: ap -> proto
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: bp -> proto
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: proto
  TYPE[6]: FIELD_IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: proto
  TYPE[7]: FIELD_IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: ap
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: bp
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 68719477530
FRAGMENT_COUNT: 6
  ORIGINAL[0]: conversation -> dissector_handle == ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == ( ( void * ) 0 )
  ORIGINAL[1]: call_dissector_only(conversation -> dissector_handle,tvb,pinfo,tree)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 -> VAR2 , VAR3 , VAR4 , VAR5 )
  ORIGINAL[2]: conversation -> dissector_handle
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: tvb
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: pinfo
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: tree
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477287
FRAGMENT_COUNT: 9
  ORIGINAL[0]: chain_head -> last && chain_head -> last -> setup_frame <= frame_num
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 && VAR1 -> VAR2 -> VAR3 <= VAR4
  ORIGINAL[1]: chain_head -> latest_found && chain_head -> latest_found -> setup_frame <= frame_num
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 && VAR1 -> VAR2 -> VAR3 <= VAR4
  ORIGINAL[2]: match = chain_head -> latest_found
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = VAR2 -> VAR3
  ORIGINAL[3]: convo = match
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 = VAR2
  ORIGINAL[4]: convo -> setup_frame
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: convo
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: match
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: convo
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: convo
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 30064771142
FRAGMENT_COUNT: 6
  ORIGINAL[0]: index < size_param
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: stonesoup_isalnum(str[index])
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 [ VAR2 ] )
  ORIGINAL[2]: str[index]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ VAR2 ]
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: index
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: str
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771223
FRAGMENT_COUNT: 9
  ORIGINAL[0]: &key -> addr2
  TYPE[0]: CALL
  TOKENIZED[0]: &key -> VAR1
  ORIGINAL[1]: key -> addr2
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: &key -> addr2
  TYPE[2]: CALL
  TOKENIZED[2]: &key -> VAR1
  ORIGINAL[3]: key -> addr2
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: addr2
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: key
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: key
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: key
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: key
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 30064771627
FRAGMENT_COUNT: 6
  ORIGINAL[0]: g_slist_free(conv -> data_list)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 )
  ORIGINAL[1]: conv -> data_list
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: data_list
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: conv
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: conv
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: conv
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477509
FRAGMENT_COUNT: 4
  ORIGINAL[0]: conv -> data_list
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: data_list
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: item
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: conv
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640268
FRAGMENT_COUNT: 3
  ORIGINAL[0]: i < size
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: ++i
  TYPE[1]: CALL
  TOKENIZED[1]: ++i
  ORIGINAL[2]: for (;i < size;++i)
  TYPE[2]: CONTROL_STRUCTURE
  TOKENIZED[2]: for ( ; VAR1 < VAR2 ; ++i )

CENTER_NODE: 47244640274
FRAGMENT_COUNT: 2
  ORIGINAL[0]: conversation -> options & 0x01 && conversation -> options & 0x02
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 & 0x01 && VAR1 -> VAR2 & 0x02
  ORIGINAL[1]: else
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: else

CENTER_NODE: 30064771731
FRAGMENT_COUNT: 23
  ORIGINAL[0]: conv == chain_head
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 == VAR2
  ORIGINAL[1]: ((void *)0) == conv -> next
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( void * ) 0 ) == VAR1 -> VAR2
  ORIGINAL[2]: (void *)0
  TYPE[2]: CALL
  TOKENIZED[2]: ( void * ) 0
  ORIGINAL[3]: conv -> next
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: cur = chain_head -> next
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 = VAR2 -> VAR3
  ORIGINAL[5]: chain_head -> next
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: prev = chain_head
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 = VAR2
  ORIGINAL[7]: cur != conv && cur -> next
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 != VAR2 && VAR1 -> VAR3
  ORIGINAL[8]: cur != conv
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 != VAR2
  ORIGINAL[9]: cur != conv
  TYPE[9]: CALL
  TOKENIZED[9]: VAR1 != VAR2
  ORIGINAL[10]: next
  TYPE[10]: FIELD_IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: next
  TYPE[11]: FIELD_IDENTIFIER
  TOKENIZED[11]: VAR1
  ORIGINAL[12]: conv
  TYPE[12]: IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: chain_head
  TYPE[13]: IDENTIFIER
  TOKENIZED[13]: VAR1
  ORIGINAL[14]: conv
  TYPE[14]: IDENTIFIER
  TOKENIZED[14]: VAR1
  ORIGINAL[15]: cur
  TYPE[15]: IDENTIFIER
  TOKENIZED[15]: VAR1
  ORIGINAL[16]: chain_head
  TYPE[16]: IDENTIFIER
  TOKENIZED[16]: VAR1
  ORIGINAL[17]: prev
  TYPE[17]: IDENTIFIER
  TOKENIZED[17]: VAR1
  ORIGINAL[18]: chain_head
  TYPE[18]: IDENTIFIER
  TOKENIZED[18]: VAR1
  ORIGINAL[19]: cur
  TYPE[19]: IDENTIFIER
  TOKENIZED[19]: VAR1
  ORIGINAL[20]: conv
  TYPE[20]: IDENTIFIER
  TOKENIZED[20]: VAR1
  ORIGINAL[21]: cur
  TYPE[21]: IDENTIFIER
  TOKENIZED[21]: VAR1
  ORIGINAL[22]: conv
  TYPE[22]: IDENTIFIER
  TOKENIZED[22]: VAR1

CENTER_NODE: 68719477557
FRAGMENT_COUNT: 2
  ORIGINAL[0]: (*semitist_pitressin)(char **) = portside_aparthrosis
  TYPE[0]: CALL
  TOKENIZED[0]: ( *semitist_pitressin ) ( char ** ) = VAR1
  ORIGINAL[1]: 
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: 

CENTER_NODE: 30064771360
FRAGMENT_COUNT: 9
  ORIGINAL[0]: (&v2 -> addr2) -> type
  TYPE[0]: CALL
  TOKENIZED[0]: ( &v2 -> VAR1 ) -> VAR2
  ORIGINAL[1]: v1 -> port2 == v2 -> port1 && v1 -> port1 == v2 -> port2 && (((&v1 -> addr2) -> type) == ((&v2 -> addr1) -> type) && (((&v1 -> addr2) -> type) == AT_NONE || (&v1 -> addr2) -> len == (&v2 -> addr1) -> len && memcmp((&v1 -> addr2) -> data,(&v2 -> addr1) -> data,((&v1 -> addr2) -> len)) == 0))
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 == VAR3 -> VAR4 && VAR1 -> VAR4 == VAR3 -> VAR2 && ( ( ( &v1 -> VAR5 ) -> VAR6 ) == ( ( &v2 -> VAR7 ) -> VAR6 ) && ( ( ( &v1 -> VAR5 ) -> VAR6 ) == VAR8 || ( &v1 -> VAR5 ) -> VAR9 == ( &v2 -> VAR7 ) -> VAR9 && FUN1 ( ( &v1 -> VAR5 ) -> VAR10 , ( &v2 -> VAR7 ) -> VAR10 , ( ( &v1 -> VAR5 ) -> VAR9 ) ) == 0 ) )
  ORIGINAL[2]: ((&v1 -> addr1) -> type) == ((&v2 -> addr2) -> type)
  TYPE[2]: CALL
  TOKENIZED[2]: ( ( &v1 -> VAR1 ) -> VAR2 ) == ( ( &v2 -> VAR3 ) -> VAR2 )
  ORIGINAL[3]: (&v1 -> addr1) -> type
  TYPE[3]: CALL
  TOKENIZED[3]: ( &v1 -> VAR1 ) -> VAR2
  ORIGINAL[4]: (&v2 -> addr2) -> type
  TYPE[4]: CALL
  TOKENIZED[4]: ( &v2 -> VAR1 ) -> VAR2
  ORIGINAL[5]: &v2 -> addr2
  TYPE[5]: CALL
  TOKENIZED[5]: &v2 -> VAR1
  ORIGINAL[6]: &v2 -> addr2
  TYPE[6]: CALL
  TOKENIZED[6]: &v2 -> VAR1
  ORIGINAL[7]: &v2 -> addr2
  TYPE[7]: CALL
  TOKENIZED[7]: &v2 -> VAR1
  ORIGINAL[8]: type
  TYPE[8]: FIELD_IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 47244640258
FRAGMENT_COUNT: 1
  ORIGINAL[0]: while (1)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: while ( 1 )

CENTER_NODE: 30064771608
FRAGMENT_COUNT: 14
  ORIGINAL[0]: &v1 -> addr1
  TYPE[0]: CALL
  TOKENIZED[0]: &v1 -> VAR1
  ORIGINAL[1]: v1 -> addr1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: ((&v1 -> addr1) -> type) == AT_NONE
  TYPE[2]: CALL
  TOKENIZED[2]: ( ( &v1 -> VAR1 ) -> VAR2 ) == VAR3
  ORIGINAL[3]: (&v1 -> addr1) -> type
  TYPE[3]: CALL
  TOKENIZED[3]: ( &v1 -> VAR1 ) -> VAR2
  ORIGINAL[4]: &v1 -> addr1
  TYPE[4]: CALL
  TOKENIZED[4]: &v1 -> VAR1
  ORIGINAL[5]: v1 -> addr1
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: (&v1 -> addr1) -> len
  TYPE[6]: CALL
  TOKENIZED[6]: ( &v1 -> VAR1 ) -> VAR2
  ORIGINAL[7]: &v1 -> addr1
  TYPE[7]: CALL
  TOKENIZED[7]: &v1 -> VAR1
  ORIGINAL[8]: v1 -> addr1
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: &v1 -> addr1
  TYPE[9]: CALL
  TOKENIZED[9]: &v1 -> VAR1
  ORIGINAL[10]: v1 -> addr1
  TYPE[10]: CALL
  TOKENIZED[10]: VAR1 -> VAR2
  ORIGINAL[11]: &v1 -> addr1
  TYPE[11]: CALL
  TOKENIZED[11]: &v1 -> VAR1
  ORIGINAL[12]: v1 -> addr1
  TYPE[12]: CALL
  TOKENIZED[12]: VAR1 -> VAR2
  ORIGINAL[13]: len
  TYPE[13]: FIELD_IDENTIFIER
  TOKENIZED[13]: VAR1

CENTER_NODE: 47244640306
FRAGMENT_COUNT: 2
  ORIGINAL[0]: conv -> setup_frame > cur -> setup_frame && cur -> next
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 > VAR3 -> VAR2 && VAR3 -> VAR4
  ORIGINAL[1]: for (;conv -> setup_frame > cur -> setup_frame && cur -> next;(prev = cur , cur = cur -> next))
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: for ( ; VAR1 -> VAR2 > VAR3 -> VAR2 && VAR3 -> VAR4 ; ( VAR5 = VAR3 , VAR3 = VAR3 -> VAR4 ) )

CENTER_NODE: 30064772136
FRAGMENT_COUNT: 5
  ORIGINAL[0]: g_slist_insert_sorted(conv -> data_list,((gpointer *)p1),p_compare)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , ( ( VAR3 * ) VAR4 ) , VAR5 )
  ORIGINAL[1]: conv -> data_list
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: (gpointer *)p1
  TYPE[2]: CALL
  TOKENIZED[2]: ( VAR1 * ) VAR2
  ORIGINAL[3]: data_list
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: conv
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476943
FRAGMENT_COUNT: 6
  ORIGINAL[0]: (v1 -> ptype) != (v2 -> ptype)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 -> VAR2 ) != ( VAR3 -> VAR2 )
  ORIGINAL[1]: v1 -> ptype
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: v1 -> port1
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: port1
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: v1
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: v1
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477518
FRAGMENT_COUNT: 3
  ORIGINAL[0]: conversation -> dissector_handle = handle
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = VAR3
  ORIGINAL[1]: conversation -> dissector_handle
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: handle
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064772288
FRAGMENT_COUNT: 5
  ORIGINAL[0]: stonesoup_contents == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 == 0
  ORIGINAL[1]: tracepoint(stonesoup_trace, trace_point, \
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 , \
  ORIGINAL[2]: stonesoup_trace
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: trace_point
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: stonesoup_contents
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771480
FRAGMENT_COUNT: 6
  ORIGINAL[0]: ADD_ADDRESS_TO_HASH_index < (&key -> addr2) -> len
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < ( &key -> VAR2 ) -> VAR3
  ORIGINAL[1]: hash_val += ADD_ADDRESS_TO_HASH_data[ADD_ADDRESS_TO_HASH_index]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 += VAR2 [ VAR3 ]
  ORIGINAL[2]: ADD_ADDRESS_TO_HASH_data[ADD_ADDRESS_TO_HASH_index]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ VAR2 ]
  ORIGINAL[3]: for (ADD_ADDRESS_TO_HASH_index = 0;ADD_ADDRESS_TO_HASH_index < (&key -> addr2) -> len;ADD_ADDRESS_TO_HASH_index++)
  TYPE[3]: CONTROL_STRUCTURE
  TOKENIZED[3]: for ( VAR1 = 0 ; VAR1 < ( &key -> VAR2 ) -> VAR3 ; ADD_ADDRESS_TO_HASH_index++ )
  ORIGINAL[4]: ADD_ADDRESS_TO_HASH_index
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hash_val
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 47244640265
FRAGMENT_COUNT: 1
  ORIGINAL[0]: while (1)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: while ( 1 )

CENTER_NODE: 68719477000
FRAGMENT_COUNT: 5
  ORIGINAL[0]: ((&v1 -> addr2) -> type) == ((&v2 -> addr2) -> type)
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( &v1 -> VAR1 ) -> VAR2 ) == ( ( &v2 -> VAR1 ) -> VAR2 )
  ORIGINAL[1]: ((&v1 -> addr2) -> type) == AT_NONE
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( &v1 -> VAR1 ) -> VAR2 ) == VAR3
  ORIGINAL[2]: (&v1 -> addr2) -> type
  TYPE[2]: CALL
  TOKENIZED[2]: ( &v1 -> VAR1 ) -> VAR2
  ORIGINAL[3]: AT_NONE
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: AT_NONE
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476921
FRAGMENT_COUNT: 2
  ORIGINAL[0]: (const conversation_key *)v
  TYPE[0]: CALL
  TOKENIZED[0]: ( const VAR1 * ) VAR2
  ORIGINAL[1]: v
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1

CENTER_NODE: 47244640321
FRAGMENT_COUNT: 1
  ORIGINAL[0]: else
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: else

CENTER_NODE: 30064771108
FRAGMENT_COUNT: 5
  ORIGINAL[0]: getenv(\
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( \
  ORIGINAL[1]: *stonesoup_server = mg_create_server(NULL, stonesoup_ev_handler)
  TYPE[1]: CALL
  TOKENIZED[1]: *stonesoup_server = FUN1 ( VAR1 , VAR2 )
  ORIGINAL[2]: mg_create_server(NULL, stonesoup_ev_handler)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[3]: stonesoup_server
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: NULL
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772202
FRAGMENT_COUNT: 3
  ORIGINAL[0]: (conv = find_conversation(pinfo -> fd -> num,(&pinfo -> src),(&pinfo -> dst),pinfo -> ptype,pinfo -> srcport,pinfo -> destport,0)) == ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 = FUN1 ( VAR2 -> VAR3 -> VAR4 , ( &pinfo -> VAR5 ) , ( &pinfo -> VAR6 ) , VAR2 -> VAR7 , VAR2 -> VAR8 , VAR2 -> VAR9 , 0 ) ) == ( ( void * ) 0 )
  ORIGINAL[1]: conv = find_conversation(pinfo -> fd -> num,(&pinfo -> src),(&pinfo -> dst),pinfo -> ptype,pinfo -> srcport,pinfo -> destport,0)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = FUN1 ( VAR2 -> VAR3 -> VAR4 , ( &pinfo -> VAR5 ) , ( &pinfo -> VAR6 ) , VAR2 -> VAR7 , VAR2 -> VAR8 , VAR2 -> VAR9 , 0 )
  ORIGINAL[2]: (void *)0
  TYPE[2]: CALL
  TOKENIZED[2]: ( void * ) 0

CENTER_NODE: 30064771663
FRAGMENT_COUNT: 4
  ORIGINAL[0]: conversation_hashtable_no_addr2 = g_hash_table_new(conversation_hash_no_addr2,conversation_match_no_addr2)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = FUN1 ( VAR2 , VAR3 )
  ORIGINAL[1]: g_hash_table_new(conversation_hash_no_addr2,conversation_match_no_addr2)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[2]: <global> conversation_hashtable_no_addr2
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: <global> VAR1
  ORIGINAL[3]: <global> conversation_hashtable_no_port2
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: <global> VAR1

CENTER_NODE: 30064771090
FRAGMENT_COUNT: 6
  ORIGINAL[0]: data_size = mg_get_var(conn, \
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = FUN1 ( VAR2 , \
  ORIGINAL[1]: mg_get_var(conn, \
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , \
  ORIGINAL[2]: buffer_size * sizeof(char)
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 * sizeof ( char )
  ORIGINAL[3]: data_size
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: conn
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: stonesoup_tainted_buff
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477019
FRAGMENT_COUNT: 6
  ORIGINAL[0]: ADD_ADDRESS_TO_HASH_index < (&key -> addr1) -> len
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < ( &key -> VAR2 ) -> VAR3
  ORIGINAL[1]: ADD_ADDRESS_TO_HASH_data[ADD_ADDRESS_TO_HASH_index]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ VAR2 ]
  ORIGINAL[2]: ADD_ADDRESS_TO_HASH_data
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hash_val
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: ADD_ADDRESS_TO_HASH_data
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: ADD_ADDRESS_TO_HASH_index
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477495
FRAGMENT_COUNT: 3
  ORIGINAL[0]: temp . proto
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . VAR2
  ORIGINAL[1]: proto
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: temp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477554
FRAGMENT_COUNT: 4
  ORIGINAL[0]: motors_luxembourg != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != 0
  ORIGINAL[1]: tryptic_ochroleucous[34]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ 34 ]
  ORIGINAL[2]: tryptic_ochroleucous
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: tryptic_ochroleucous
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719476790
FRAGMENT_COUNT: 7
  ORIGINAL[0]: c >= 97 && c <= 122
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= 97 && VAR1 <= 122
  ORIGINAL[1]: c >= 65
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 >= 65
  ORIGINAL[2]: c
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: c
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: c
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: c
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: c
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

