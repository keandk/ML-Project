# Tokenized code fragments for 153290-v1.0.0-bad
# Total center nodes processed: 43
# Total code fragments found: 182

CENTER_NODE: 68719477118
FRAGMENT_COUNT: 5
  ORIGINAL[0]: nBuckets = next_pow2_long((num_entries - 1) / 1 + 1)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = FUN1 ( ( VAR2 - 1 ) / 1 + 1 )
  ORIGINAL[1]: next_pow2_long((num_entries - 1) / 1 + 1)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( ( VAR1 - 1 ) / 1 + 1 )
  ORIGINAL[2]: nBuckets
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: num_entries
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: nBuckets
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476822
FRAGMENT_COUNT: 3
  ORIGINAL[0]: tracepoint(stonesoup_trace, trace_location, \
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , \
  ORIGINAL[1]: stonesoup_trace
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: trace_location
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064772018
FRAGMENT_COUNT: 4
  ORIGINAL[0]: status -> curEntry = ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = ( ( void * ) 0 )
  ORIGINAL[1]: status -> curEntry
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: (void *)0
  TYPE[2]: CALL
  TOKENIZED[2]: ( void * ) 0
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640338
FRAGMENT_COUNT: 0

CENTER_NODE: 68719476964
FRAGMENT_COUNT: 13
  ORIGINAL[0]: hctl -> num_partitions
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hctl -> ssize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctl -> sshift
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hctl -> ffactor
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hctl -> max_dsize
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hctl -> dsize
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: flags & 0x020
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 & 0x020
  ORIGINAL[7]: hctl -> keysize = info -> keysize
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2 = VAR3 -> VAR2
  ORIGINAL[8]: hctl -> keysize
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: hctl -> entrysize
  TYPE[9]: CALL
  TOKENIZED[9]: VAR1 -> VAR2
  ORIGINAL[10]: entrysize
  TYPE[10]: FIELD_IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: hctl
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1
  ORIGINAL[12]: hctl
  TYPE[12]: IDENTIFIER
  TOKENIZED[12]: VAR1

CENTER_NODE: 30064772114
FRAGMENT_COUNT: 10
  ORIGINAL[0]: hashp -> isshared
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: elog_finish(20,\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( 20 , \
  ORIGINAL[2]: hashp -> tabname
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hashp -> tabname
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: tabname
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: hashp
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: hashp
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 30064771189
FRAGMENT_COUNT: 5
  ORIGINAL[0]: strncmp(key1,key2,keysize - 1)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 - 1 )
  ORIGINAL[1]: keysize - 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 - 1
  ORIGINAL[2]: key1
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: key2
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: keysize
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640276
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772083
FRAGMENT_COUNT: 5
  ORIGINAL[0]: ++segment_ndx >= ssize
  TYPE[0]: CALL
  TOKENIZED[0]: ++segment_ndx >= VAR1
  ORIGINAL[1]: segp = hashp -> dir[segment_num]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = VAR2 -> VAR3 [ VAR4 ]
  ORIGINAL[2]: hashp -> dir[segment_num]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 [ VAR3 ]
  ORIGINAL[3]: curElem
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: segp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772109
FRAGMENT_COUNT: 3
  ORIGINAL[0]: !status -> hashp -> frozen
  TYPE[0]: CALL
  TOKENIZED[0]: !status -> VAR1 -> VAR2
  ORIGINAL[1]: deregister_seq_scan(status -> hashp)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 -> VAR2 )
  ORIGINAL[2]: status -> hashp
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2

CENTER_NODE: 47244640382
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771561
FRAGMENT_COUNT: 13
  ORIGINAL[0]: nsegs > hctl -> dsize
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > VAR2 -> VAR3
  ORIGINAL[1]: !hashp -> dir
  TYPE[1]: CALL
  TOKENIZED[1]: !hashp -> VAR1
  ORIGINAL[2]: hashp -> dir
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hctl -> dsize = nsegs
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2 = VAR3
  ORIGINAL[4]: hctl -> dsize
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: (bool )0
  TYPE[5]: CALL
  TOKENIZED[5]: ( VAR1 ) 0
  ORIGINAL[6]: !hashp -> dir
  TYPE[6]: CALL
  TOKENIZED[6]: !hashp -> VAR1
  ORIGINAL[7]: hashp -> dir
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2
  ORIGINAL[8]: dsize
  TYPE[8]: FIELD_IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: dir
  TYPE[9]: FIELD_IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: hctl
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: nsegs
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1
  ORIGINAL[12]: hashp
  TYPE[12]: IDENTIFIER
  TOKENIZED[12]: VAR1

CENTER_NODE: 30064772374
FRAGMENT_COUNT: 5
  ORIGINAL[0]: i < nelem
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: ((char *)tmpElement) + elementSize
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( char * ) VAR1 ) + VAR2
  ORIGINAL[2]: (char *)tmpElement
  TYPE[2]: CALL
  TOKENIZED[2]: ( char * ) VAR1
  ORIGINAL[3]: tmpElement
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: elementSize
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640397
FRAGMENT_COUNT: 1
  ORIGINAL[0]: num > (2147483647 / 2)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > ( 2147483647 / 2 )

CENTER_NODE: 30064771131
FRAGMENT_COUNT: 6
  ORIGINAL[0]: *stonesoup_tainted_buff = NULL
  TYPE[0]: CALL
  TOKENIZED[0]: *stonesoup_tainted_buff = VAR1
  ORIGINAL[1]: *stonesoup_tainted_buff
  TYPE[1]: CALL
  TOKENIZED[1]: *stonesoup_tainted_buff
  ORIGINAL[2]: stonesoup_tainted_buff
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: NULL
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: stonesoup_tainted_buff
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: stonesoup_tainted_buff
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477517
FRAGMENT_COUNT: 4
  ORIGINAL[0]: !segp
  TYPE[0]: CALL
  TOKENIZED[0]: !segp
  ORIGINAL[1]: sizeof(HASHBUCKET )
  TYPE[1]: CALL
  TOKENIZED[1]: sizeof ( VAR1 )
  ORIGINAL[2]: _len
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: HASHBUCKET
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477591
FRAGMENT_COUNT: 2
  ORIGINAL[0]: my_log2(num)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 )
  ORIGINAL[1]: num
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1

CENTER_NODE: 30064772271
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (((intptr_t )_vstart) & sizeof(long ) - 1) == 0 && (_len & sizeof(long ) - 1) == 0 && _val == 0 && _len <= 1024 && 1024 != 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( ( VAR1 ) VAR2 ) & sizeof ( long ) - 1 ) == 0 && ( VAR3 & sizeof ( long ) - 1 ) == 0 && VAR4 == 0 && VAR3 <= 1024 && 1024 != 0
  ORIGINAL[1]: (long *)(((char *)_start) + _len)
  TYPE[1]: CALL
  TOKENIZED[1]: ( long * ) ( ( ( char * ) VAR1 ) + VAR2 )
  ORIGINAL[2]: ((char *)_start) + _len
  TYPE[2]: CALL
  TOKENIZED[2]: ( ( char * ) VAR1 ) + VAR2
  ORIGINAL[3]: (char *)_start
  TYPE[3]: CALL
  TOKENIZED[3]: ( char * ) VAR1
  ORIGINAL[4]: _len
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772512
FRAGMENT_COUNT: 1
  ORIGINAL[0]: *seq_scan_tables[100]
  TYPE[0]: CALL
  TOKENIZED[0]: *seq_scan_tables [ 100 ]

CENTER_NODE: 68719476830
FRAGMENT_COUNT: 6
  ORIGINAL[0]: strlen(set_param_str) > 10U
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 ) > 10U
  ORIGINAL[1]: set_param_data_struct -> func_member = stonesoup_modulus_function
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 = VAR3
  ORIGINAL[2]: set_param_data_struct -> func_member
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: set_param_data_struct -> str_member
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: str_member
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: set_param_data_struct
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771109
FRAGMENT_COUNT: 5
  ORIGINAL[0]: retval == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 == 0
  ORIGINAL[1]: filepath = (char*) malloc (size_filepath * sizeof(char))
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = ( char* ) FUN1 ( VAR2 * sizeof ( char ) )
  ORIGINAL[2]: (char*) malloc (size_filepath * sizeof(char))
  TYPE[2]: CALL
  TOKENIZED[2]: ( char* ) FUN1 ( VAR1 * sizeof ( char ) )
  ORIGINAL[3]: malloc (size_filepath * sizeof(char))
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( VAR1 * sizeof ( char ) )
  ORIGINAL[4]: filepath
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640390
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477579
FRAGMENT_COUNT: 4
  ORIGINAL[0]: hashp -> isshared
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hashp -> tabname
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: tabname
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771651
FRAGMENT_COUNT: 4
  ORIGINAL[0]: ((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1))
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR1 ) ( 8 - 1 ) )
  ORIGINAL[1]: ((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1)
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 )
  ORIGINAL[2]: ~((intptr_t )(8 - 1))
  TYPE[2]: CALL
  TOKENIZED[2]: ~ ( ( VAR1 ) ( 8 - 1 ) )
  ORIGINAL[3]: (intptr_t )(8 - 1)
  TYPE[3]: CALL
  TOKENIZED[3]: ( VAR1 ) ( 8 - 1 )

CENTER_NODE: 47244640347
FRAGMENT_COUNT: 1
  ORIGINAL[0]: for (;;)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: for ( ; ; )

CENTER_NODE: 68719477300
FRAGMENT_COUNT: 10
  ORIGINAL[0]: hctlv -> num_partitions != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 != 0
  ORIGINAL[1]: hctlv -> num_partitions
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctlv -> mutex
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hctlv -> mutex
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hctlv -> freeList
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hctlv -> mutex
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: hctlv -> nelem_alloc
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: mutex
  TYPE[7]: FIELD_IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: hctlv
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: hctlv
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 47244640374
FRAGMENT_COUNT: 1
  ORIGINAL[0]: else
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: else

CENTER_NODE: 68719477315
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hashp -> hctl
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hctl
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: hashp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640377
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640306
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477007
FRAGMENT_COUNT: 7
  ORIGINAL[0]: (((intptr_t )_vstart) & sizeof(long ) - 1) == 0 && (_len & sizeof(long ) - 1) == 0 && _val == 0 && _len <= 1024 && 1024 != 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( ( VAR1 ) VAR2 ) & sizeof ( long ) - 1 ) == 0 && ( VAR3 & sizeof ( long ) - 1 ) == 0 && VAR4 == 0 && VAR3 <= 1024 && 1024 != 0
  ORIGINAL[1]: memset(_vstart,_val,_len)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 , VAR3 )
  ORIGINAL[2]: _val
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: _val
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: _vstart
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: _val
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: _len
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 30064772514
FRAGMENT_COUNT: 2
  ORIGINAL[0]: num_seq_scans = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = 0
  ORIGINAL[1]: num_seq_scans
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1

CENTER_NODE: 68719477664
FRAGMENT_COUNT: 3
  ORIGINAL[0]: tracepoint(stonesoup_trace, weakness_start, \
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , \
  ORIGINAL[1]: stonesoup_trace
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: weakness_start
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640349
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477028
FRAGMENT_COUNT: 5
  ORIGINAL[0]: elementSize = ((((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1))) + (((intptr_t )entrysize) + (8 - 1) & ~((intptr_t )(8 - 1))))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = ( ( ( ( VAR2 ) ( sizeof ( VAR3 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR2 ) ( 8 - 1 ) ) ) + ( ( ( VAR2 ) VAR4 ) + ( 8 - 1 ) & ~ ( ( VAR2 ) ( 8 - 1 ) ) ) )
  ORIGINAL[1]: allocSize = (32 * 4)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = ( 32 * 4 )
  ORIGINAL[2]: 32 * 4
  TYPE[2]: CALL
  TOKENIZED[2]: 32 * 4
  ORIGINAL[3]: allocSize
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: allocSize
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771681
FRAGMENT_COUNT: 6
  ORIGINAL[0]: (info -> dsize) * sizeof(HASHSEGMENT )
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 -> VAR2 ) * sizeof ( VAR3 )
  ORIGINAL[1]: info -> dsize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: sizeof(HASHSEGMENT )
  TYPE[2]: CALL
  TOKENIZED[2]: sizeof ( VAR1 )
  ORIGINAL[3]: dsize
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: info
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: HASHSEGMENT
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477720
FRAGMENT_COUNT: 6
  ORIGINAL[0]: shmid = shmget(key, shmsz, IPC_CREAT | 0666)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = FUN1 ( VAR2 , VAR3 , VAR4 | 0666 )
  ORIGINAL[1]: shmget(key, shmsz, IPC_CREAT | 0666)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 , VAR3 | 0666 )
  ORIGINAL[2]: errors
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: shmid
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: key
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: shmid
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477584
FRAGMENT_COUNT: 6
  ORIGINAL[0]: num > 9223372036854775807L / 2
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > 9223372036854775807L / 2
  ORIGINAL[1]: num = 9223372036854775807L / 2
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = 9223372036854775807L / 2
  ORIGINAL[2]: i = 0
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = 0
  ORIGINAL[3]: i
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: i
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: i
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771736
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (hashp -> hash)(keyPtr,hashp -> keysize)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 -> VAR2 ) ( VAR3 , VAR1 -> VAR4 )
  ORIGINAL[1]: hashp -> hash
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hash
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: keyPtr
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771768
FRAGMENT_COUNT: 5
  ORIGINAL[0]: segment_ndx = bucket & hashp -> ssize - 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = VAR2 & VAR3 -> VAR4 - 1
  ORIGINAL[1]: bucket & hashp -> ssize - 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 & VAR2 -> VAR3 - 1
  ORIGINAL[2]: hashp -> ssize - 1
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 - 1
  ORIGINAL[3]: segment_ndx
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: bucket
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477144
FRAGMENT_COUNT: 4
  ORIGINAL[0]: transportee_gavia = 40
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = 40
  ORIGINAL[1]: &badmash_huther
  TYPE[1]: CALL
  TOKENIZED[1]: &badmash_huther
  ORIGINAL[2]: <global> badmash_huther
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: <global> VAR1
  ORIGINAL[3]: badmash_huther
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771690
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (hashp -> hash)(keyPtr,hashp -> keysize)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 -> VAR2 ) ( VAR3 , VAR1 -> VAR4 )
  ORIGINAL[1]: hashp -> keysize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: keysize
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: keyPtr
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771683
FRAGMENT_COUNT: 9
  ORIGINAL[0]: hashp != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: (void *)0
  TYPE[1]: CALL
  TOKENIZED[1]: ( void * ) 0
  ORIGINAL[2]: hash_stats(\
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( \
  ORIGINAL[3]: MemoryContextDelete(hashp -> hcxt)
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( VAR1 -> VAR2 )
  ORIGINAL[4]: hashp -> hcxt
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hcxt
  TYPE[5]: FIELD_IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: hashp
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

