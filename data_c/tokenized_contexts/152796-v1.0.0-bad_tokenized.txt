# Tokenized code fragments for 152796-v1.0.0-bad
# Total center nodes processed: 43
# Total code fragments found: 199

CENTER_NODE: 68719477596
FRAGMENT_COUNT: 4
  ORIGINAL[0]: hashp -> isshared
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hashp -> tabname
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: tabname
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640393
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772287
FRAGMENT_COUNT: 6
  ORIGINAL[0]: p != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: _len = (new_dirsize - old_dirsize)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = ( VAR2 - VAR3 )
  ORIGINAL[2]: new_dirsize - old_dirsize
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 - VAR2
  ORIGINAL[3]: _len
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: new_dirsize
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: old_dirsize
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477369
FRAGMENT_COUNT: 9
  ORIGINAL[0]: hashp -> hctl
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hashp -> ssize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: curBucket > max_bucket
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 > VAR2
  ORIGINAL[3]: hashp -> sshift
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hashp -> dir
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: dir
  TYPE[5]: FIELD_IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: segp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: hashp
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 30064771181
FRAGMENT_COUNT: 5
  ORIGINAL[0]: a >= 'a'
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= 'a'
  ORIGINAL[1]: a -= 'a'-'A'
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -= 'a'-'A'
  ORIGINAL[2]: 'a'-'A'
  TYPE[2]: CALL
  TOKENIZED[2]: 'a'-'A'
  ORIGINAL[3]: a
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: a
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477609
FRAGMENT_COUNT: 4
  ORIGINAL[0]: num > (2147483647 / 2)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > ( 2147483647 / 2 )
  ORIGINAL[1]: 2147483647 / 2
  TYPE[1]: CALL
  TOKENIZED[1]: 2147483647 / 2
  ORIGINAL[2]: num
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: num
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640322
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477572
FRAGMENT_COUNT: 7
  ORIGINAL[0]: !firstElement
  TYPE[0]: CALL
  TOKENIZED[0]: !firstElement
  ORIGINAL[1]: prevElement = ((void *)0)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = ( ( void * ) 0 )
  ORIGINAL[2]: tmpElement = firstElement
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = VAR2
  ORIGINAL[3]: tmpElement
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: firstElement
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: tmpElement
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: tmpElement
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 47244640365
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477116
FRAGMENT_COUNT: 6
  ORIGINAL[0]: add_size(size,mul_size(nDirEntries,sizeof(HASHSEGMENT )))
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , FUN2 ( VAR2 , sizeof ( VAR3 ) ) )
  ORIGINAL[1]: mul_size(nDirEntries,sizeof(HASHSEGMENT ))
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , sizeof ( VAR2 ) )
  ORIGINAL[2]: size
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: size
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: size
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: nDirEntries
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771082
FRAGMENT_COUNT: 22
  ORIGINAL[0]: ss_tc_root != NULL
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != VAR2
  ORIGINAL[1]: size_dirpath = strlen(ss_tc_root) + strlen(\
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = FUN1 ( VAR2 ) + FUN1 ( \
  ORIGINAL[2]: strlen(ss_tc_root) + strlen(\
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 ) + FUN1 ( \
  ORIGINAL[3]: strlen(ss_tc_root) + strlen(\
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( VAR1 ) + FUN1 ( \
  ORIGINAL[4]: strlen(ss_tc_root)
  TYPE[4]: CALL
  TOKENIZED[4]: FUN1 ( VAR1 )
  ORIGINAL[5]: strlen(\
  TYPE[5]: CALL
  TOKENIZED[5]: FUN1 ( \
  ORIGINAL[6]: dirpath = (char*) malloc (size_dirpath * sizeof(char))
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 = ( char* ) FUN1 ( VAR2 * sizeof ( char ) )
  ORIGINAL[7]: (char*) malloc (size_dirpath * sizeof(char))
  TYPE[7]: CALL
  TOKENIZED[7]: ( char* ) FUN1 ( VAR1 * sizeof ( char ) )
  ORIGINAL[8]: malloc (size_dirpath * sizeof(char))
  TYPE[8]: CALL
  TOKENIZED[8]: FUN1 ( VAR1 * sizeof ( char ) )
  ORIGINAL[9]: size_dirpath * sizeof(char)
  TYPE[9]: CALL
  TOKENIZED[9]: VAR1 * sizeof ( char )
  ORIGINAL[10]: sizeof(char)
  TYPE[10]: CALL
  TOKENIZED[10]: sizeof ( char )
  ORIGINAL[11]: dirpath != NULL
  TYPE[11]: CALL
  TOKENIZED[11]: VAR1 != VAR2
  ORIGINAL[12]: ss_tc_root
  TYPE[12]: IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: NULL
  TYPE[13]: IDENTIFIER
  TOKENIZED[13]: VAR1
  ORIGINAL[14]: size_dirpath
  TYPE[14]: IDENTIFIER
  TOKENIZED[14]: VAR1
  ORIGINAL[15]: ss_tc_root
  TYPE[15]: IDENTIFIER
  TOKENIZED[15]: VAR1
  ORIGINAL[16]: dirpath
  TYPE[16]: IDENTIFIER
  TOKENIZED[16]: VAR1
  ORIGINAL[17]: size_dirpath
  TYPE[17]: IDENTIFIER
  TOKENIZED[17]: VAR1
  ORIGINAL[18]: char
  TYPE[18]: IDENTIFIER
  TOKENIZED[18]: char
  ORIGINAL[19]: dirpath
  TYPE[19]: IDENTIFIER
  TOKENIZED[19]: VAR1
  ORIGINAL[20]: NULL
  TYPE[20]: IDENTIFIER
  TOKENIZED[20]: VAR1
  ORIGINAL[21]: <global> stonesoup_printf_context
  TYPE[21]: IDENTIFIER
  TOKENIZED[21]: <global> VAR1

CENTER_NODE: 30064772365
FRAGMENT_COUNT: 4
  ORIGINAL[0]: _start < _stop
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: *(_start++) = 0
  TYPE[1]: CALL
  TOKENIZED[1]: * ( _start++ ) = 0
  ORIGINAL[2]: *(_start++)
  TYPE[2]: CALL
  TOKENIZED[2]: * ( _start++ )
  ORIGINAL[3]: _start++
  TYPE[3]: CALL
  TOKENIZED[3]: _start++

CENTER_NODE: 68719477608
FRAGMENT_COUNT: 2
  ORIGINAL[0]: my_log2(num)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 )
  ORIGINAL[1]: num
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1

CENTER_NODE: 68719477156
FRAGMENT_COUNT: 6
  ORIGINAL[0]: hashp != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: hash_stats(\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( \
  ORIGINAL[2]: hashp -> hcxt
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hcxt
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477157
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hashp -> hash
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hash
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: hashp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640292
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771488
FRAGMENT_COUNT: 5
  ORIGINAL[0]: *hctl = hashp -> hctl
  TYPE[0]: CALL
  TOKENIZED[0]: *hctl = VAR1 -> VAR2
  ORIGINAL[1]: hashp -> hctl
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctl
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hctl
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771559
FRAGMENT_COUNT: 6
  ORIGINAL[0]: ((intptr_t )entrysize) + (8 - 1) & ~((intptr_t )(8 - 1))
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( VAR1 ) VAR2 ) + ( 8 - 1 ) & ~ ( ( VAR1 ) ( 8 - 1 ) )
  ORIGINAL[1]: ((intptr_t )entrysize) + (8 - 1)
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( VAR1 ) VAR2 ) + ( 8 - 1 )
  ORIGINAL[2]: (intptr_t )entrysize
  TYPE[2]: CALL
  TOKENIZED[2]: ( VAR1 ) VAR2
  ORIGINAL[3]: 8 - 1
  TYPE[3]: CALL
  TOKENIZED[3]: 8 - 1
  ORIGINAL[4]: ~((intptr_t )(8 - 1))
  TYPE[4]: CALL
  TOKENIZED[4]: ~ ( ( VAR1 ) ( 8 - 1 ) )
  ORIGINAL[5]: intptr_t
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 47244640406
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771771
FRAGMENT_COUNT: 6
  ORIGINAL[0]: hash_search_with_hash_value(hashp,keyPtr,((hashp -> hash)(keyPtr,hashp -> keysize)),action,foundPtr)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , ( ( VAR1 -> VAR3 ) ( VAR2 , VAR1 -> VAR4 ) ) , VAR5 , VAR6 )
  ORIGINAL[1]: (hashp -> hash)(keyPtr,hashp -> keysize)
  TYPE[1]: CALL
  TOKENIZED[1]: ( VAR1 -> VAR2 ) ( VAR3 , VAR1 -> VAR4 )
  ORIGINAL[2]: hashp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: keyPtr
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: action
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: foundPtr
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771928
FRAGMENT_COUNT: 6
  ORIGINAL[0]: hashp -> isshared
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: errstart(20,\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( 20 , \
  ORIGINAL[2]: errstart(20,\
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( 20 , \
  ORIGINAL[3]: errfinish(errcode(('5' - 48 & 0x3F) + (('3' - 48 & 0x3F) << 6) + (('2' - 48 & 0x3F) << 12) + ((48 - 48 & 0x3F) << 18) + ((48 - 48 & 0x3F) << 24)),errmsg(\
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( FUN2 ( ( '5' - 48 & 0x3F ) + ( ( '3' - 48 & 0x3F ) << 6 ) + ( ( '2' - 48 & 0x3F ) << 12 ) + ( ( 48 - 48 & 0x3F ) << 18 ) + ( ( 48 - 48 & 0x3F ) << 24 ) ) , FUN3 ( \
  ORIGINAL[4]: (void )0
  TYPE[4]: CALL
  TOKENIZED[4]: ( void ) 0
  ORIGINAL[5]: prevBucketPtr
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 47244640398
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771714
FRAGMENT_COUNT: 3
  ORIGINAL[0]: (num_entries - 1) / 1 + 1
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 - 1 ) / 1 + 1
  ORIGINAL[1]: (num_entries - 1) / 1
  TYPE[1]: CALL
  TOKENIZED[1]: ( VAR1 - 1 ) / 1
  ORIGINAL[2]: num_entries - 1
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 - 1

CENTER_NODE: 30064772050
FRAGMENT_COUNT: 5
  ORIGINAL[0]: hashp -> hctl -> nentries
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[1]: hashp -> hctl
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctl
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: nentries
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771212
FRAGMENT_COUNT: 10
  ORIGINAL[0]: src[i] != '\\0'
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ VAR2 ] != '\\0'
  ORIGINAL[1]: src[i]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ VAR2 ]
  ORIGINAL[2]: src[i] == ';'
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ VAR2 ] == ' ; '
  ORIGINAL[3]: src[i]
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 [ VAR2 ]
  ORIGINAL[4]: src[i]
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 [ VAR2 ]
  ORIGINAL[5]: src
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: src
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: i
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: src
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: src
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 47244640354
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477607
FRAGMENT_COUNT: 4
  ORIGINAL[0]: limit < num
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: i
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: i
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: i
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772567
FRAGMENT_COUNT: 3
  ORIGINAL[0]: ((char *)( *(unscoring_resistance - 5))) != 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( char * ) ( * ( VAR1 - 5 ) ) ) != 0
  ORIGINAL[1]: free(((char *)((char *)( *(unscoring_resistance - 5)))))
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( ( ( char * ) ( ( char * ) ( * ( VAR1 - 5 ) ) ) ) )
  ORIGINAL[2]: stonesoup_close_printf_context()
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( )

CENTER_NODE: 30064772572
FRAGMENT_COUNT: 1
  ORIGINAL[0]: *seq_scan_tables[100]
  TYPE[0]: CALL
  TOKENIZED[0]: *seq_scan_tables [ 100 ]

CENTER_NODE: 68719477768
FRAGMENT_COUNT: 8
  ORIGINAL[0]: (shm = shmat(shmid, NULL, 0)) == (char *) -1
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 = FUN1 ( VAR2 , VAR3 , 0 ) ) == ( char * ) -1
  ORIGINAL[1]: fprintf(stderr, \
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , \
  ORIGINAL[2]: stderr
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: stderr
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: stderr
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: stderr
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: stderr
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: stderr
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 30064771728
FRAGMENT_COUNT: 4
  ORIGINAL[0]: (info -> dsize) * sizeof(HASHSEGMENT )
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 -> VAR2 ) * sizeof ( VAR3 )
  ORIGINAL[1]: info -> dsize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: sizeof(HASHSEGMENT )
  TYPE[2]: CALL
  TOKENIZED[2]: sizeof ( VAR1 )
  ORIGINAL[3]: HASHSEGMENT
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771404
FRAGMENT_COUNT: 5
  ORIGINAL[0]: flags & 0x040
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 & 0x040
  ORIGINAL[1]: flags & 0x080
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 & 0x080
  ORIGINAL[2]: hdefault(hashp)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 )
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hctl
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477618
FRAGMENT_COUNT: 4
  ORIGINAL[0]: seq_scan_tables[num_seq_scans] = hashp
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ VAR2 ] = VAR3
  ORIGINAL[1]: seq_scan_level[num_seq_scans]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ VAR2 ]
  ORIGINAL[2]: <global> seq_scan_level
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: <global> VAR1
  ORIGINAL[3]: <global> num_seq_scans
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: <global> VAR1

CENTER_NODE: 47244640342
FRAGMENT_COUNT: 1
  ORIGINAL[0]: bucket > hctl -> max_bucket
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > VAR2 -> VAR3

CENTER_NODE: 30064772041
FRAGMENT_COUNT: 8
  ORIGINAL[0]: newElement != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: hctlv -> nentries++
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> nentries++
  ORIGINAL[2]: hctlv -> nentries
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: nentries
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hctlv
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hctlv
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hctlv
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hctlv
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 68719477462
FRAGMENT_COUNT: 7
  ORIGINAL[0]: currElement != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: currElement -> link
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: currElement -> hashvalue
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hashvalue
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: currElement
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hctl
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: currElement
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 30064772160
FRAGMENT_COUNT: 5
  ORIGINAL[0]: hashp -> frozen = ((bool )1)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = ( ( VAR3 ) 1 )
  ORIGINAL[1]: hashp -> frozen
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: (bool )1
  TYPE[2]: CALL
  TOKENIZED[2]: ( VAR1 ) 1
  ORIGINAL[3]: frozen
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771573
FRAGMENT_COUNT: 10
  ORIGINAL[0]: hctl -> num_partitions != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 != 0
  ORIGINAL[1]: hctl -> num_partitions
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: *((volatile slock_t *)(&hctl -> mutex)) = 0
  TYPE[2]: CALL
  TOKENIZED[2]: * ( ( volatile VAR1 * ) ( &hctl -> VAR2 ) ) = 0
  ORIGINAL[3]: *((volatile slock_t *)(&hctl -> mutex))
  TYPE[3]: CALL
  TOKENIZED[3]: * ( ( volatile VAR1 * ) ( &hctl -> VAR2 ) )
  ORIGINAL[4]: (volatile slock_t *)(&hctl -> mutex)
  TYPE[4]: CALL
  TOKENIZED[4]: ( volatile VAR1 * ) ( &hctl -> VAR2 )
  ORIGINAL[5]: &hctl -> mutex
  TYPE[5]: CALL
  TOKENIZED[5]: &hctl -> VAR1
  ORIGINAL[6]: hctl -> mutex
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: mutex
  TYPE[7]: FIELD_IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: hctl
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: nbuckets
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 47244640363
FRAGMENT_COUNT: 1
  ORIGINAL[0]: for (;;)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: for ( ; ; )

CENTER_NODE: 68719477334
FRAGMENT_COUNT: 6
  ORIGINAL[0]: status -> hashp = hashp
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = VAR2
  ORIGINAL[1]: status -> hashp
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hashp -> frozen
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719476869
FRAGMENT_COUNT: 3
  ORIGINAL[0]: keysize - 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 - 1
  ORIGINAL[1]: key2
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: keysize
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771133
FRAGMENT_COUNT: 7
  ORIGINAL[0]: getenv(\
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( \
  ORIGINAL[1]: getenv(\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( \
  ORIGINAL[2]: getenv(\
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( \
  ORIGINAL[3]: strcmp(getenv(\
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( FUN2 ( \
  ORIGINAL[4]: strcmp(getenv(\
  TYPE[4]: CALL
  TOKENIZED[4]: FUN1 ( FUN2 ( \
  ORIGINAL[5]: getenv(\
  TYPE[5]: CALL
  TOKENIZED[5]: FUN1 ( \
  ORIGINAL[6]: NULL
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 68719477396
FRAGMENT_COUNT: 3
  ORIGINAL[0]: status -> hashp
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hashp
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: status
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

