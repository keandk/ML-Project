# Tokenized code fragments for 152271-v1.0.0-bad
# Total center nodes processed: 41
# Total code fragments found: 162

CENTER_NODE: 30064772384
FRAGMENT_COUNT: 2
  ORIGINAL[0]: 1L << my_log2(num)
  TYPE[0]: CALL
  TOKENIZED[0]: 1L << FUN1 ( VAR1 )
  ORIGINAL[1]: my_log2(num)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )

CENTER_NODE: 30064771168
FRAGMENT_COUNT: 4
  ORIGINAL[0]: strncmp(key1,key2,keysize - 1)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 - 1 )
  ORIGINAL[1]: keysize - 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 - 1
  ORIGINAL[2]: key1
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: key2
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771995
FRAGMENT_COUNT: 3
  ORIGINAL[0]: !hashp -> frozen
  TYPE[0]: CALL
  TOKENIZED[0]: !hashp -> VAR1
  ORIGINAL[1]: register_seq_scan(hashp)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )
  ORIGINAL[2]: hashp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640304
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771662
FRAGMENT_COUNT: 4
  ORIGINAL[0]: (info -> dsize) * sizeof(HASHSEGMENT )
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 -> VAR2 ) * sizeof ( VAR3 )
  ORIGINAL[1]: info -> dsize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: sizeof(HASHSEGMENT )
  TYPE[2]: CALL
  TOKENIZED[2]: sizeof ( VAR1 )
  ORIGINAL[3]: HASHSEGMENT
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640392
FRAGMENT_COUNT: 1
  ORIGINAL[0]: else
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: else

CENTER_NODE: 47244640336
FRAGMENT_COUNT: 0

CENTER_NODE: 68719476826
FRAGMENT_COUNT: 4
  ORIGINAL[0]: flags & 0x040
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 & 0x040
  ORIGINAL[1]: flags
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: flags
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: flags
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772220
FRAGMENT_COUNT: 4
  ORIGINAL[0]: p != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: _val = 0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = 0
  ORIGINAL[2]: _val
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: _len
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477689
FRAGMENT_COUNT: 5
  ORIGINAL[0]: errors = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = 0
  ORIGINAL[1]: *out_filename = NULL
  TYPE[1]: CALL
  TOKENIZED[1]: *out_filename = VAR1
  ORIGINAL[2]: out_filename
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: NULL
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: out_filename
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477086
FRAGMENT_COUNT: 6
  ORIGINAL[0]: nElementAllocs = (num_entries - 1) / elementAllocCnt + 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = ( VAR2 - 1 ) / VAR3 + 1
  ORIGINAL[1]: elementSize = (((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1))) + (((intptr_t )entrysize) + (8 - 1) & ~((intptr_t )(8 - 1)))
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = ( ( ( VAR2 ) ( sizeof ( VAR3 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR2 ) ( 8 - 1 ) ) ) + ( ( ( VAR2 ) VAR4 ) + ( 8 - 1 ) & ~ ( ( VAR2 ) ( 8 - 1 ) ) )
  ORIGINAL[2]: (((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1))) + (((intptr_t )entrysize) + (8 - 1) & ~((intptr_t )(8 - 1)))
  TYPE[2]: CALL
  TOKENIZED[2]: ( ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR1 ) ( 8 - 1 ) ) ) + ( ( ( VAR1 ) VAR3 ) + ( 8 - 1 ) & ~ ( ( VAR1 ) ( 8 - 1 ) ) )
  ORIGINAL[3]: elementSize
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: intptr_t
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: elementSize
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477515
FRAGMENT_COUNT: 5
  ORIGINAL[0]: hashp -> isfixed
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: ((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1))
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR1 ) ( 8 - 1 ) )
  ORIGINAL[2]: (intptr_t )(hctlv -> entrysize)
  TYPE[2]: CALL
  TOKENIZED[2]: ( VAR1 ) ( VAR2 -> VAR3 )
  ORIGINAL[3]: intptr_t
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hctlv
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771653
FRAGMENT_COUNT: 3
  ORIGINAL[0]: (nBuckets - 1) / 256 + 1
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 - 1 ) / 256 + 1
  ORIGINAL[1]: (nBuckets - 1) / 256
  TYPE[1]: CALL
  TOKENIZED[1]: ( VAR1 - 1 ) / 256
  ORIGINAL[2]: nBuckets - 1
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 - 1

CENTER_NODE: 30064771984
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hashp -> hctl -> nentries
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[1]: hashp -> hctl
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: nentries
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064772504
FRAGMENT_COUNT: 1
  ORIGINAL[0]: *seq_scan_tables[100]
  TYPE[0]: CALL
  TOKENIZED[0]: *seq_scan_tables [ 100 ]

CENTER_NODE: 47244640371
FRAGMENT_COUNT: 2
  ORIGINAL[0]: ((long )(calc_bucket(hctl,currElement -> hashvalue))) == old_bucket
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( long ) ( FUN1 ( VAR1 , VAR2 -> VAR3 ) ) ) == VAR4
  ORIGINAL[1]: else
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: else

CENTER_NODE: 68719476803
FRAGMENT_COUNT: 7
  ORIGINAL[0]: sscanf(stonesoup_envKey, \
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , \
  ORIGINAL[1]: &stonesoup_key
  TYPE[1]: CALL
  TOKENIZED[1]: &stonesoup_key
  ORIGINAL[2]: shmget(stonesoup_key, stonesoup_shmsz, 0666)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , VAR2 , 0666 )
  ORIGINAL[3]: stonesoup_key
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: stonesoup_shmid
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: stonesoup_key
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: stonesoup_shmsz
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 68719477677
FRAGMENT_COUNT: 7
  ORIGINAL[0]: stonesoup_is_valid == 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 == 1
  ORIGINAL[1]: tracepoint(stonesoup_trace, trace_point, \
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 , \
  ORIGINAL[2]: tracepoint(stonesoup_trace, weakness_end)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[3]: stonesoup_trace
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: stonesoup_trace
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: stonesoup_trace
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: weakness_end
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 30064772285
FRAGMENT_COUNT: 4
  ORIGINAL[0]: (((intptr_t )_vstart) & sizeof(long ) - 1) == 0 && (_len & sizeof(long ) - 1) == 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( ( VAR1 ) VAR2 ) & sizeof ( long ) - 1 ) == 0 && ( VAR3 & sizeof ( long ) - 1 ) == 0
  ORIGINAL[1]: (((intptr_t )_vstart) & sizeof(long ) - 1) == 0
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( ( VAR1 ) VAR2 ) & sizeof ( long ) - 1 ) == 0
  ORIGINAL[2]: (_len & sizeof(long ) - 1) == 0
  TYPE[2]: CALL
  TOKENIZED[2]: ( VAR1 & sizeof ( long ) - 1 ) == 0
  ORIGINAL[3]: _len & sizeof(long ) - 1
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 & sizeof ( long ) - 1

CENTER_NODE: 47244640274
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640262
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771668
FRAGMENT_COUNT: 4
  ORIGINAL[0]: (hashp -> hash)(keyPtr,hashp -> keysize)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 -> VAR2 ) ( VAR3 , VAR1 -> VAR4 )
  ORIGINAL[1]: hashp -> hash
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hashp -> keysize
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: keyPtr
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719476766
FRAGMENT_COUNT: 6
  ORIGINAL[0]: retval == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 == 0
  ORIGINAL[1]: size_filepath = strlen(dirpath) + strlen(\
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = FUN1 ( VAR2 ) + FUN1 ( \
  ORIGINAL[2]: filepath = (char*) malloc (size_filepath * sizeof(char))
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = ( char* ) FUN1 ( VAR2 * sizeof ( char ) )
  ORIGINAL[3]: (char*) malloc (size_filepath * sizeof(char))
  TYPE[3]: CALL
  TOKENIZED[3]: ( char* ) FUN1 ( VAR1 * sizeof ( char ) )
  ORIGINAL[4]: filepath
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: filepath
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477563
FRAGMENT_COUNT: 4
  ORIGINAL[0]: num > (2147483647 / 2)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > ( 2147483647 / 2 )
  ORIGINAL[1]: 2147483647 / 2
  TYPE[1]: CALL
  TOKENIZED[1]: 2147483647 / 2
  ORIGINAL[2]: num
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: num
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477145
FRAGMENT_COUNT: 4
  ORIGINAL[0]: hash_search_with_hash_value(hashp,keyPtr,((hashp -> hash)(keyPtr,hashp -> keysize)),action,foundPtr)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , ( ( VAR1 -> VAR3 ) ( VAR2 , VAR1 -> VAR4 ) ) , VAR5 , VAR6 )
  ORIGINAL[1]: hashp -> keysize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hashp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: keyPtr
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477168
FRAGMENT_COUNT: 6
  ORIGINAL[0]: bucket = calc_bucket(hctl,hashvalue)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = FUN1 ( VAR2 , VAR3 )
  ORIGINAL[1]: segment_num = (bucket >> hashp -> sshift)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = ( VAR2 >> VAR3 -> VAR4 )
  ORIGINAL[2]: bucket >> hashp -> sshift
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 >> VAR2 -> VAR3
  ORIGINAL[3]: segment_num
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: bucket
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: segment_num
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 47244640375
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477036
FRAGMENT_COUNT: 12
  ORIGINAL[0]: hctl -> mutex
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hctl -> ffactor
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctl -> num_partitions
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hctl -> max_bucket
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hctl -> low_mask
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hctl -> high_mask
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: hctl -> ssize
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: hctl -> dsize
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2
  ORIGINAL[8]: dsize
  TYPE[8]: FIELD_IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: hctl
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: nsegs
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: hctl
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1

CENTER_NODE: 30064771502
FRAGMENT_COUNT: 5
  ORIGINAL[0]: nelem_alloc = (allocSize / elementSize)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = ( VAR2 / VAR3 )
  ORIGINAL[1]: allocSize / elementSize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 / VAR2
  ORIGINAL[2]: nelem_alloc < 32
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 < 32
  ORIGINAL[3]: nelem_alloc
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: nelem_alloc
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477356
FRAGMENT_COUNT: 8
  ORIGINAL[0]: hashp -> tabname
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: !hashp -> frozen
  TYPE[1]: CALL
  TOKENIZED[1]: !hashp -> VAR1
  ORIGINAL[2]: hashp -> frozen
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: has_seq_scans(hashp)
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( VAR1 )
  ORIGINAL[4]: hashp -> tabname
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 30064772379
FRAGMENT_COUNT: 3
  ORIGINAL[0]: i = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = 0
  ORIGINAL[1]: i
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: limit
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477305
FRAGMENT_COUNT: 16
  ORIGINAL[0]: (curElem = status -> curEntry) != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 = VAR2 -> VAR3 ) != ( ( void * ) 0 )
  ORIGINAL[1]: curBucket = status -> curBucket
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = VAR2 -> VAR1
  ORIGINAL[2]: hashp = status -> hashp
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = VAR2 -> VAR1
  ORIGINAL[3]: status -> hashp
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hashp -> hctl
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hashp -> ssize
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: hashp -> sshift
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: hashp -> dir
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2
  ORIGINAL[8]: hashp -> dir
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: hashp
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: status
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: hashp
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1
  ORIGINAL[12]: hashp
  TYPE[12]: IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: hashp
  TYPE[13]: IDENTIFIER
  TOKENIZED[13]: VAR1
  ORIGINAL[14]: hashp
  TYPE[14]: IDENTIFIER
  TOKENIZED[14]: VAR1
  ORIGINAL[15]: hashp
  TYPE[15]: IDENTIFIER
  TOKENIZED[15]: VAR1

CENTER_NODE: 47244640347
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771663
FRAGMENT_COUNT: 9
  ORIGINAL[0]: hashp != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: (void *)0
  TYPE[1]: CALL
  TOKENIZED[1]: ( void * ) 0
  ORIGINAL[2]: hash_stats(\
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( \
  ORIGINAL[3]: MemoryContextDelete(hashp -> hcxt)
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( VAR1 -> VAR2 )
  ORIGINAL[4]: hashp -> hcxt
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hcxt
  TYPE[5]: FIELD_IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: hashp
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 47244640380
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772080
FRAGMENT_COUNT: 5
  ORIGINAL[0]: status -> hashp -> frozen
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[1]: status -> hashp
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hashp
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: frozen
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: status
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640388
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771947
FRAGMENT_COUNT: 4
  ORIGINAL[0]: *hctlv = (hashp -> hctl)
  TYPE[0]: CALL
  TOKENIZED[0]: *hctlv = ( VAR1 -> VAR2 )
  ORIGINAL[1]: hashp -> hctl
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctlv
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hctlv
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477620
FRAGMENT_COUNT: 7
  ORIGINAL[0]: seq_scan_level[i] >= nestDepth
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ VAR2 ] >= VAR3
  ORIGINAL[1]: seq_scan_tables[i]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ VAR2 ]
  ORIGINAL[2]: seq_scan_tables[i]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ VAR2 ]
  ORIGINAL[3]: seq_scan_tables[num_seq_scans - 1]
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 [ VAR2 - 1 ]
  ORIGINAL[4]: isCommit
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: <global> seq_scan_tables
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: <global> VAR1
  ORIGINAL[6]: i
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 47244640345
FRAGMENT_COUNT: 1
  ORIGINAL[0]: for (;;)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: for ( ; ; )

CENTER_NODE: 68719476994
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hctl -> keysize
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: sizeof(char *)
  TYPE[1]: CALL
  TOKENIZED[1]: sizeof ( char * )
  ORIGINAL[2]: char
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: char

