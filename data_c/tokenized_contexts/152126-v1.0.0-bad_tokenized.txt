# Tokenized code fragments for 152126-v1.0.0-bad
# Total center nodes processed: 78
# Total code fragments found: 345

CENTER_NODE: 30064771319
FRAGMENT_COUNT: 3
  ORIGINAL[0]: (svn_string_t *)(&strbuf -> data)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 * ) ( &strbuf -> VAR2 )
  ORIGINAL[1]: &strbuf -> data
  TYPE[1]: CALL
  TOKENIZED[1]: &strbuf -> VAR1
  ORIGINAL[2]: strbuf -> data
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2

CENTER_NODE: 68719477149
FRAGMENT_COUNT: 7
  ORIGINAL[0]: bytes + count > (str -> data) && bytes < (str -> data + str -> blocksize)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 + VAR2 > ( VAR3 -> VAR4 ) && VAR1 < ( VAR3 -> VAR4 + VAR3 -> VAR5 )
  ORIGINAL[1]: str -> data + pos
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 + VAR3
  ORIGINAL[2]: str -> data
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: pos
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: pos
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: pos
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: pos
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 68719477424
FRAGMENT_COUNT: 6
  ORIGINAL[0]: &endptr
  TYPE[0]: CALL
  TOKENIZED[0]: &endptr
  ORIGINAL[1]: *__errno_location() == 22 || endptr == str || str[0] == '\\0'
  TYPE[1]: CALL
  TOKENIZED[1]: *__errno_location ( ) == 22 || VAR1 == VAR2 || VAR2 [ 0 ] == '\\0'
  ORIGINAL[2]: *endptr
  TYPE[2]: CALL
  TOKENIZED[2]: *endptr
  ORIGINAL[3]: endptr
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: endptr
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: endptr
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477379
FRAGMENT_COUNT: 7
  ORIGINAL[0]: svn_ctype_casecmp(a,b)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[1]: cmp || !a || !b
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 || !a || !b
  ORIGINAL[2]: a
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: cmp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: a
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: b
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: a
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 68719477297
FRAGMENT_COUNT: 3
  ORIGINAL[0]: svn_cstring_split_append(a,input,sep_chars,chop_whitespace,pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 , VAR4 , VAR5 )
  ORIGINAL[1]: a
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: a
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771298
FRAGMENT_COUNT: 4
  ORIGINAL[0]: __builtin_va_start(ap,fmt)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[1]: ap
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: fmt
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771665
FRAGMENT_COUNT: 4
  ORIGINAL[0]: i = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = 0
  ORIGINAL[1]: for (i = 0;i < list -> nelts;i++)
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: for ( VAR1 = 0 ; VAR1 < VAR2 -> VAR3 ; i++ )
  ORIGINAL[2]: i
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: i
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771416
FRAGMENT_COUNT: 6
  ORIGINAL[0]: membuf_ensure(&mem,&str -> blocksize,minimum_size,str -> pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( &mem , &str -> VAR1 , VAR2 , VAR3 -> VAR4 )
  ORIGINAL[1]: &mem
  TYPE[1]: CALL
  TOKENIZED[1]: &mem
  ORIGINAL[2]: &str -> blocksize
  TYPE[2]: CALL
  TOKENIZED[2]: &str -> VAR1
  ORIGINAL[3]: str -> pool
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: minimum_size
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: mem
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477440
FRAGMENT_COUNT: 3
  ORIGINAL[0]: svn_cstring_strtoi64(n,str,- 9223372036854775807L - 1,9223372036854775807L,10)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , - 9223372036854775807L - 1 , 9223372036854775807L , 10 )
  ORIGINAL[1]: n
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: str
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771408
FRAGMENT_COUNT: 4
  ORIGINAL[0]: str -> data[str -> len] = '\\0'
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 [ VAR1 -> VAR3 ] = '\\0'
  ORIGINAL[1]: str -> data[str -> len]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 [ VAR1 -> VAR3 ]
  ORIGINAL[2]: str -> data
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: str -> len
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2

CENTER_NODE: 30064771815
FRAGMENT_COUNT: 3
  ORIGINAL[0]: svn_cstring_strtoui64(n,str,0,18446744073709551615UL,10)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , 0 , 18446744073709551615UL , 10 )
  ORIGINAL[1]: n
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: str
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477457
FRAGMENT_COUNT: 5
  ORIGINAL[0]: number < 10
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < 10
  ORIGINAL[1]: memcpy(dest,decimal_table[(apr_size_t )number],2)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 [ ( VAR3 ) VAR4 ] , 2 )
  ORIGINAL[2]: dest[2]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ 2 ]
  ORIGINAL[3]: dest
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: dest
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477401
FRAGMENT_COUNT: 5
  ORIGINAL[0]: *__errno_location() == 34 && (val == - 9223372036854775807L - 1 || val == 9223372036854775807L) || val < 0 || ((apr_uint64_t )val) < minval
  TYPE[0]: CALL
  TOKENIZED[0]: *__errno_location ( ) == 34 && ( VAR1 == - 9223372036854775807L - 1 || VAR1 == 9223372036854775807L ) || VAR1 < 0 || ( ( VAR2 ) VAR1 ) < VAR3
  ORIGINAL[1]: ((apr_uint64_t )val) > maxval
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( VAR1 ) VAR2 ) > VAR3
  ORIGINAL[2]: (apr_uint64_t )val
  TYPE[2]: CALL
  TOKENIZED[2]: ( VAR1 ) VAR2
  ORIGINAL[3]: maxval
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: maxval
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476921
FRAGMENT_COUNT: 6
  ORIGINAL[0]: *new_string = (apr_palloc(pool,sizeof(( *new_string))))
  TYPE[0]: CALL
  TOKENIZED[0]: *new_string = ( FUN1 ( VAR1 , sizeof ( ( *new_string ) ) ) )
  ORIGINAL[1]: *new_string
  TYPE[1]: CALL
  TOKENIZED[1]: *new_string
  ORIGINAL[2]: new_string -> data
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: data
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: new_string
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: new_string
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 47244640284
FRAGMENT_COUNT: 2
  ORIGINAL[0]: i != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != 0
  ORIGINAL[1]: while (i != 0)
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: while ( VAR1 != 0 )

CENTER_NODE: 68719476864
FRAGMENT_COUNT: 7
  ORIGINAL[0]: membuf -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: membuf -> size
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: &membuf -> data
  TYPE[2]: CALL
  TOKENIZED[2]: &membuf -> VAR1
  ORIGINAL[3]: membuf -> data
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: membuf -> size
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: size
  TYPE[5]: FIELD_IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: membuf
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 68719476998
FRAGMENT_COUNT: 8
  ORIGINAL[0]: new_string -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: new_string -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: new_string -> blocksize
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: *new_string
  TYPE[3]: CALL
  TOKENIZED[3]: *new_string
  ORIGINAL[4]: new_string -> pool = pool
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2 = VAR2
  ORIGINAL[5]: new_string -> pool
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: new_string
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: new_string
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 30064771411
FRAGMENT_COUNT: 2
  ORIGINAL[0]: str -> len == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == 0
  ORIGINAL[1]: str -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2

CENTER_NODE: 68719477531
FRAGMENT_COUNT: 4
  ORIGINAL[0]: ui64toa_sep(number,seperator,buffer)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 )
  ORIGINAL[1]: apr_pstrdup(pool,buffer)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[2]: pool
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: buffer
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477121
FRAGMENT_COUNT: 3
  ORIGINAL[0]: svn_stringbuf_appendbytes(targetstr,(appendstr -> data),appendstr -> len)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , ( VAR2 -> VAR3 ) , VAR2 -> VAR4 )
  ORIGINAL[1]: targetstr
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: appendstr
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771168
FRAGMENT_COUNT: 5
  ORIGINAL[0]: minimum_size = minimum_size + (8 - 1) & (~(8 - 1))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = VAR1 + ( 8 - 1 ) & ( ~ ( 8 - 1 ) )
  ORIGINAL[1]: minimum_size + (8 - 1) & (~(8 - 1))
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 + ( 8 - 1 ) & ( ~ ( 8 - 1 ) )
  ORIGINAL[2]: minimum_size + (8 - 1)
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 + ( 8 - 1 )
  ORIGINAL[3]: ~(8 - 1)
  TYPE[3]: CALL
  TOKENIZED[3]: ~ ( 8 - 1 )
  ORIGINAL[4]: minimum_size
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771303
FRAGMENT_COUNT: 4
  ORIGINAL[0]: str -> len == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == 0
  ORIGINAL[1]: str -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477233
FRAGMENT_COUNT: 4
  ORIGINAL[0]: original_string -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: original_string -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: original_string
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771281
FRAGMENT_COUNT: 4
  ORIGINAL[0]: new_string = mem
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = VAR2
  ORIGINAL[1]: new_string
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: mem
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: new_string
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477516
FRAGMENT_COUNT: 6
  ORIGINAL[0]: i > 3
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > 3
  ORIGINAL[1]: i - 2
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 - 2
  ORIGINAL[2]: i
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: buffer
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: i
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: i
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 47244640275
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477026
FRAGMENT_COUNT: 4
  ORIGINAL[0]: new_string -> data = data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = VAR2
  ORIGINAL[1]: new_string -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: data
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: data
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771466
FRAGMENT_COUNT: 5
  ORIGINAL[0]: start_address = (str -> data + str -> len)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = ( VAR2 -> VAR3 + VAR2 -> VAR4 )
  ORIGINAL[1]: str -> data + str -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 + VAR1 -> VAR3
  ORIGINAL[2]: str -> data
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: str -> len
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: start_address
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476979
FRAGMENT_COUNT: 2
  ORIGINAL[0]: svn_stringbuf_create_ensure(0,pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( 0 , VAR1 )
  ORIGINAL[1]: pool
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1

CENTER_NODE: 68719477270
FRAGMENT_COUNT: 25
  ORIGINAL[0]: 0 != (svn_ctype_table[(unsigned char )( *p)] & 0x0002)
  TYPE[0]: CALL
  TOKENIZED[0]: 0 != ( VAR1 [ ( unsigned char ) ( *p ) ] & 0x0002 )
  ORIGINAL[1]: svn_ctype_table[(unsigned char )( *p)] & 0x0002
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ ( unsigned char ) ( *p ) ] & 0x0002
  ORIGINAL[2]: svn_ctype_table[(unsigned char )( *p)]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ ( unsigned char ) ( *p ) ]
  ORIGINAL[3]: (unsigned char )( *p)
  TYPE[3]: CALL
  TOKENIZED[3]: ( unsigned char ) ( *p )
  ORIGINAL[4]: *p
  TYPE[4]: CALL
  TOKENIZED[4]: *p
  ORIGINAL[5]: *e = p + (strlen(p) - 1)
  TYPE[5]: CALL
  TOKENIZED[5]: *e = VAR1 + ( FUN1 ( VAR1 ) - 1 )
  ORIGINAL[6]: p + (strlen(p) - 1)
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 + ( FUN1 ( VAR1 ) - 1 )
  ORIGINAL[7]: strlen(p) - 1
  TYPE[7]: CALL
  TOKENIZED[7]: FUN1 ( VAR1 ) - 1
  ORIGINAL[8]: strlen(p)
  TYPE[8]: CALL
  TOKENIZED[8]: FUN1 ( VAR1 )
  ORIGINAL[9]: e >= p && 0 != (svn_ctype_table[(unsigned char )( *e)] & 0x0002)
  TYPE[9]: CALL
  TOKENIZED[9]: VAR1 >= VAR2 && 0 != ( VAR3 [ ( unsigned char ) ( *e ) ] & 0x0002 )
  ORIGINAL[10]: e >= p
  TYPE[10]: CALL
  TOKENIZED[10]: VAR1 >= VAR2
  ORIGINAL[11]: *(++e) = '\\0'
  TYPE[11]: CALL
  TOKENIZED[11]: * ( ++e ) = '\\0'
  ORIGINAL[12]: *(++e)
  TYPE[12]: CALL
  TOKENIZED[12]: * ( ++e )
  ORIGINAL[13]: ++e
  TYPE[13]: CALL
  TOKENIZED[13]: ++e
  ORIGINAL[14]: p
  TYPE[14]: IDENTIFIER
  TOKENIZED[14]: VAR1
  ORIGINAL[15]: chop_whitespace
  TYPE[15]: IDENTIFIER
  TOKENIZED[15]: VAR1
  ORIGINAL[16]: svn_ctype_table
  TYPE[16]: IDENTIFIER
  TOKENIZED[16]: VAR1
  ORIGINAL[17]: p
  TYPE[17]: IDENTIFIER
  TOKENIZED[17]: VAR1
  ORIGINAL[18]: e
  TYPE[18]: IDENTIFIER
  TOKENIZED[18]: VAR1
  ORIGINAL[19]: p
  TYPE[19]: IDENTIFIER
  TOKENIZED[19]: VAR1
  ORIGINAL[20]: p
  TYPE[20]: IDENTIFIER
  TOKENIZED[20]: VAR1
  ORIGINAL[21]: e
  TYPE[21]: IDENTIFIER
  TOKENIZED[21]: VAR1
  ORIGINAL[22]: p
  TYPE[22]: IDENTIFIER
  TOKENIZED[22]: VAR1
  ORIGINAL[23]: e
  TYPE[23]: IDENTIFIER
  TOKENIZED[23]: VAR1
  ORIGINAL[24]: p
  TYPE[24]: IDENTIFIER
  TOKENIZED[24]: VAR1

CENTER_NODE: 68719476897
FRAGMENT_COUNT: 3
  ORIGINAL[0]: i < len
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: svn_ctype_table[(unsigned char )str[i]]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ ( unsigned char ) VAR2 [ VAR3 ] ]
  ORIGINAL[2]: svn_ctype_table
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771234
FRAGMENT_COUNT: 4
  ORIGINAL[0]: _s_z_ > _m_b_f_ -> size
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > VAR2 -> VAR3
  ORIGINAL[1]: memset(_m_b_f_ -> data,0,_s_z_)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 -> VAR2 , 0 , VAR3 )
  ORIGINAL[2]: _m_b_f_ -> data
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: _s_z_
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477035
FRAGMENT_COUNT: 4
  ORIGINAL[0]: __builtin_va_start(ap,fmt)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[1]: ap
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: fmt
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: fmt
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772111
FRAGMENT_COUNT: 4
  ORIGINAL[0]: reimmerge_parlante = ((char *)((aedicule_mikir )boulez_hunter))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = ( ( char * ) ( ( VAR2 ) VAR3 ) )
  ORIGINAL[1]: (char *)((aedicule_mikir )boulez_hunter)
  TYPE[1]: CALL
  TOKENIZED[1]: ( char * ) ( ( VAR1 ) VAR2 )
  ORIGINAL[2]: reimmerge_parlante
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: stonesoup_trace
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477009
FRAGMENT_COUNT: 5
  ORIGINAL[0]: strbuf -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: strbuf -> len = size
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 = VAR3
  ORIGINAL[2]: strbuf -> len
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: strbuf
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: strbuf
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640334
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771293
FRAGMENT_COUNT: 5
  ORIGINAL[0]: svn_string_ncreate((strbuf -> data),strbuf -> len,pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( ( VAR1 -> VAR2 ) , VAR1 -> VAR3 , VAR4 )
  ORIGINAL[1]: strbuf -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: strbuf
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: pool
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771755
FRAGMENT_COUNT: 6
  ORIGINAL[0]: i < strings -> nelts
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2 -> VAR3
  ORIGINAL[1]: *string = ((const char **)(strings -> elts))[i]
  TYPE[1]: CALL
  TOKENIZED[1]: *string = ( ( const char ** ) ( VAR1 -> VAR2 ) ) [ VAR3 ]
  ORIGINAL[2]: ((const char **)(strings -> elts))[i]
  TYPE[2]: CALL
  TOKENIZED[2]: ( ( const char ** ) ( VAR1 -> VAR2 ) ) [ VAR3 ]
  ORIGINAL[3]: (const char **)(strings -> elts)
  TYPE[3]: CALL
  TOKENIZED[3]: ( const char ** ) ( VAR1 -> VAR2 )
  ORIGINAL[4]: string
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: i
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477415
FRAGMENT_COUNT: 2
  ORIGINAL[0]: *n
  TYPE[0]: CALL
  TOKENIZED[0]: *n
  ORIGINAL[1]: n
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1

CENTER_NODE: 68719477261
FRAGMENT_COUNT: 4
  ORIGINAL[0]: str2 -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: str1 -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str1
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771867
FRAGMENT_COUNT: 3
  ORIGINAL[0]: *n = ((int )val)
  TYPE[0]: CALL
  TOKENIZED[0]: *n = ( ( int ) VAR1 )
  ORIGINAL[1]: *n
  TYPE[1]: CALL
  TOKENIZED[1]: *n
  ORIGINAL[2]: (int )val
  TYPE[2]: CALL
  TOKENIZED[2]: ( int ) VAR1

CENTER_NODE: 30064771678
FRAGMENT_COUNT: 7
  ORIGINAL[0]: i < list -> nelts
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2 -> VAR3
  ORIGINAL[1]: list -> nelts
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: strcmp(this_str,str) == 0
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , VAR2 ) == 0
  ORIGINAL[3]: nelts
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: i
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: list
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: list
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 30064771255
FRAGMENT_COUNT: 5
  ORIGINAL[0]: new_string = (apr_palloc(pool,sizeof(( *new_string))))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = ( FUN1 ( VAR2 , sizeof ( ( *new_string ) ) ) )
  ORIGINAL[1]: apr_palloc(pool,sizeof(( *new_string)))
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , sizeof ( ( *new_string ) ) )
  ORIGINAL[2]: sizeof(( *new_string))
  TYPE[2]: CALL
  TOKENIZED[2]: sizeof ( ( *new_string ) )
  ORIGINAL[3]: new_string
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: pool
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771317
FRAGMENT_COUNT: 5
  ORIGINAL[0]: find_char_backward(str -> data,str -> len,ch)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , VAR1 -> VAR3 , VAR4 )
  ORIGINAL[1]: str -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: ch
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640330
FRAGMENT_COUNT: 1
  ORIGINAL[0]: for (;;)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: for ( ; ; )

CENTER_NODE: 30064771447
FRAGMENT_COUNT: 4
  ORIGINAL[0]: str -> blocksize > old_len + 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 > VAR3 + 1
  ORIGINAL[1]: svn_stringbuf_appendbytes(str,(&b),1)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , ( &b ) , 1 )
  ORIGINAL[2]: &b
  TYPE[2]: CALL
  TOKENIZED[2]: &b
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477015
FRAGMENT_COUNT: 3
  ORIGINAL[0]: svn_stringbuf_ncreate(str -> data,str -> len,pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , VAR1 -> VAR3 , VAR4 )
  ORIGINAL[1]: str -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: pool
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719476855
FRAGMENT_COUNT: 3
  ORIGINAL[0]: membuf -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: data
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: membuf
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477010
FRAGMENT_COUNT: 3
  ORIGINAL[0]: svn_stringbuf_ncreate(cstring,strlen(cstring),pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , FUN2 ( VAR1 ) , VAR2 )
  ORIGINAL[1]: cstring
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: cstring
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477238
FRAGMENT_COUNT: 4
  ORIGINAL[0]: str1 -> len
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: str2 -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str2
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719476969
FRAGMENT_COUNT: 3
  ORIGINAL[0]: str1 -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: data
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: str1
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477054
FRAGMENT_COUNT: 3
  ORIGINAL[0]: str -> len
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: len
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: str
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771224
FRAGMENT_COUNT: 3
  ORIGINAL[0]: memset(_m_b_f_ -> data,0,_m_b_f_ -> size)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , 0 , VAR1 -> VAR3 )
  ORIGINAL[1]: _m_b_f_ -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: _m_b_f_ -> size
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2

CENTER_NODE: 30064771591
FRAGMENT_COUNT: 5
  ORIGINAL[0]: string_first_non_whitespace((str -> data),str -> len)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( ( VAR1 -> VAR2 ) , VAR1 -> VAR3 )
  ORIGINAL[1]: str -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: data
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: str
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771383
FRAGMENT_COUNT: 4
  ORIGINAL[0]: memset((str -> data),c,str -> len)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( ( VAR1 -> VAR2 ) , VAR3 , VAR1 -> VAR4 )
  ORIGINAL[1]: str -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: str -> len
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: c
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771306
FRAGMENT_COUNT: 5
  ORIGINAL[0]: svn_string_ncreate(original_string -> data,original_string -> len,pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , VAR1 -> VAR3 , VAR4 )
  ORIGINAL[1]: original_string -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: original_string
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: pool
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476952
FRAGMENT_COUNT: 4
  ORIGINAL[0]: apr_pvsprintf(pool,fmt,ap)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 )
  ORIGINAL[1]: pool
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: fmt
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: ap
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771480
FRAGMENT_COUNT: 3
  ORIGINAL[0]: svn_stringbuf_appendbytes(targetstr,cstr,strlen(cstr))
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , FUN2 ( VAR2 ) )
  ORIGINAL[1]: strlen(cstr)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )
  ORIGINAL[2]: cstr
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771314
FRAGMENT_COUNT: 4
  ORIGINAL[0]: string_first_non_whitespace(str -> data,str -> len)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , VAR1 -> VAR3 )
  ORIGINAL[1]: str -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477534
FRAGMENT_COUNT: 5
  ORIGINAL[0]: buffer[2 * 21]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ 2 * 21 ]
  ORIGINAL[1]: number < 0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 < 0
  ORIGINAL[2]: number
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: number
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: number
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771620
FRAGMENT_COUNT: 5
  ORIGINAL[0]: find_char_backward((str -> data),str -> len,ch)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( ( VAR1 -> VAR2 ) , VAR1 -> VAR3 , VAR4 )
  ORIGINAL[1]: str -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: ch
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772083
FRAGMENT_COUNT: 7
  ORIGINAL[0]: ( *pstr) == strb[i - 1]
  TYPE[0]: CALL
  TOKENIZED[0]: ( *pstr ) == VAR1 [ VAR2 - 1 ]
  ORIGINAL[1]: curr[i] = (curr[i - 1] > prev[i]?curr[i - 1] : prev[i])
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ VAR2 ] = ( VAR1 [ VAR2 - 1 ] > VAR3 [ VAR2 ] ?curr [ VAR2 - 1 ] : VAR3 [ VAR2 ] )
  ORIGINAL[2]: curr[i]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ VAR2 ]
  ORIGINAL[3]: curr[i - 1] > prev[i]?curr[i - 1] : prev[i]
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 [ VAR2 - 1 ] > VAR3 [ VAR2 ] ?curr [ VAR2 - 1 ] : VAR3 [ VAR2 ]
  ORIGINAL[4]: curr[i - 1] > prev[i]
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 [ VAR2 - 1 ] > VAR3 [ VAR2 ]
  ORIGINAL[5]: curr[i - 1]
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 [ VAR2 - 1 ]
  ORIGINAL[6]: prev[i]
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 [ VAR2 ]

CENTER_NODE: 30064771289
FRAGMENT_COUNT: 4
  ORIGINAL[0]: svn_string_ncreate(cstring,strlen(cstring),pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , FUN2 ( VAR1 ) , VAR2 )
  ORIGINAL[1]: strlen(cstring)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )
  ORIGINAL[2]: cstring
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: pool
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719476854
FRAGMENT_COUNT: 4
  ORIGINAL[0]: membuf -> pool = pool
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = VAR2
  ORIGINAL[1]: membuf -> pool
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: pool
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: pool
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719476829
FRAGMENT_COUNT: 6
  ORIGINAL[0]: minimum_size >  *size
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > *size
  ORIGINAL[1]: *size
  TYPE[1]: CALL
  TOKENIZED[1]: *size
  ORIGINAL[2]: minimum_size
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: size
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: minimum_size
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: minimum_size
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477052
FRAGMENT_COUNT: 6
  ORIGINAL[0]: memcpy((str -> data),value,amt + 1)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( ( VAR1 -> VAR2 ) , VAR3 , VAR4 + 1 )
  ORIGINAL[1]: str -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: str -> len
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: len
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: str
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: str
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 47244640338
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771106
FRAGMENT_COUNT: 4
  ORIGINAL[0]: retval == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 == 0
  ORIGINAL[1]: strlen(dirpath) + strlen(\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 ) + FUN1 ( \
  ORIGINAL[2]: strlen(dirpath)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 )
  ORIGINAL[3]: dirpath
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640276
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477183
FRAGMENT_COUNT: 4
  ORIGINAL[0]: bytes + new_count
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 + VAR2
  ORIGINAL[1]: str -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: data
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477502
FRAGMENT_COUNT: 3
  ORIGINAL[0]: number >= 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= 0
  ORIGINAL[1]: svn__ui64toa(dest,((apr_uint64_t )number))
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , ( ( VAR2 ) VAR3 ) )
  ORIGINAL[2]: dest
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477251
FRAGMENT_COUNT: 8
  ORIGINAL[0]: str -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: str -> blocksize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: str -> len > 0
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 > 0
  ORIGINAL[3]: str -> len
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: str -> data
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: data
  TYPE[5]: FIELD_IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: str
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: str
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 30064771719
FRAGMENT_COUNT: 6
  ORIGINAL[0]: next == ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 == ( ( void * ) 0 )
  ORIGINAL[1]: *str = token + strlen(token)
  TYPE[1]: CALL
  TOKENIZED[1]: *str = VAR1 + FUN1 ( VAR1 )
  ORIGINAL[2]: *str
  TYPE[2]: CALL
  TOKENIZED[2]: *str
  ORIGINAL[3]: token + strlen(token)
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 + FUN1 ( VAR1 )
  ORIGINAL[4]: strlen(token)
  TYPE[4]: CALL
  TOKENIZED[4]: FUN1 ( VAR1 )
  ORIGINAL[5]: token
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477171
FRAGMENT_COUNT: 7
  ORIGINAL[0]: pos + count > str -> len
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 + VAR2 > VAR3 -> VAR4
  ORIGINAL[1]: str -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: count = str -> len - pos
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = VAR2 -> VAR3 - VAR4
  ORIGINAL[3]: str -> len
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: str -> data
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: data
  TYPE[5]: FIELD_IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: str
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 47244640266
FRAGMENT_COUNT: 1
  ORIGINAL[0]: stonesoup_tainted_file != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != 0

CENTER_NODE: 68719476888
FRAGMENT_COUNT: 4
  ORIGINAL[0]: len1 != len2
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != VAR2
  ORIGINAL[1]: len1
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: len2
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: len1
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477351
FRAGMENT_COUNT: 6
  ORIGINAL[0]: ( *p) == 10
  TYPE[0]: CALL
  TOKENIZED[0]: ( *p ) == 10
  ORIGINAL[1]: *p
  TYPE[1]: CALL
  TOKENIZED[1]: *p
  ORIGINAL[2]: *p
  TYPE[2]: CALL
  TOKENIZED[2]: *p
  ORIGINAL[3]: p
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: p
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: p
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477553
FRAGMENT_COUNT: 5
  ORIGINAL[0]: stringa . data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . VAR2
  ORIGINAL[1]: stringa . len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2
  ORIGINAL[2]: stringb . len = strlen(strb)
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 . VAR2 = FUN1 ( VAR3 )
  ORIGINAL[3]: &stringa
  TYPE[3]: CALL
  TOKENIZED[3]: &stringa
  ORIGINAL[4]: stringa
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

