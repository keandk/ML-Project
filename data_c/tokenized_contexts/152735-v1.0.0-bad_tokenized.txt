# Tokenized code fragments for 152735-v1.0.0-bad
# Total center nodes processed: 39
# Total code fragments found: 206

CENTER_NODE: 47244640390
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477582
FRAGMENT_COUNT: 6
  ORIGINAL[0]: hashp -> isfixed
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hctlv -> entrysize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: entrysize
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hctlv
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: intptr_t
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hctlv
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771676
FRAGMENT_COUNT: 3
  ORIGINAL[0]: *unswaggeringly_ungreased = 0
  TYPE[0]: CALL
  TOKENIZED[0]: *unswaggeringly_ungreased = 0
  ORIGINAL[1]: unswaggeringly_ungreased
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: allegiance_shored
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477616
FRAGMENT_COUNT: 4
  ORIGINAL[0]: hashp -> isshared
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hashp -> tabname
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: tabname
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640349
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640272
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640302
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477024
FRAGMENT_COUNT: 4
  ORIGINAL[0]: nbuckets < hctl -> num_partitions
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2 -> VAR3
  ORIGINAL[1]: nbuckets <<= 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 <<= 1
  ORIGINAL[2]: nbuckets
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: nbuckets
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771115
FRAGMENT_COUNT: 5
  ORIGINAL[0]: filepath != NULL
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != VAR2
  ORIGINAL[1]: stonesoup_printf_context = fopen(filepath, \
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = FUN1 ( VAR2 , \
  ORIGINAL[2]: fopen(filepath, \
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , \
  ORIGINAL[3]: <global> stonesoup_printf_context
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: <global> VAR1
  ORIGINAL[4]: filepath
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477212
FRAGMENT_COUNT: 5
  ORIGINAL[0]: hash_search_with_hash_value(hashp,keyPtr,((hashp -> hash)(keyPtr,hashp -> keysize)),action,foundPtr)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , ( ( VAR1 -> VAR3 ) ( VAR2 , VAR1 -> VAR4 ) ) , VAR5 , VAR6 )
  ORIGINAL[1]: hashp
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: keyPtr
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: keyPtr
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772043
FRAGMENT_COUNT: 22
  ORIGINAL[0]: &hctlv -> mutex
  TYPE[0]: CALL
  TOKENIZED[0]: &hctlv -> VAR1
  ORIGINAL[1]: hctlv -> mutex
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: &hctlv -> mutex
  TYPE[2]: CALL
  TOKENIZED[2]: &hctlv -> VAR1
  ORIGINAL[3]: hctlv -> mutex
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hctlv -> num_partitions != 0
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2 != 0
  ORIGINAL[5]: &hctlv -> mutex
  TYPE[5]: CALL
  TOKENIZED[5]: &hctlv -> VAR1
  ORIGINAL[6]: hctlv -> mutex
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: &hctlv -> mutex
  TYPE[7]: CALL
  TOKENIZED[7]: &hctlv -> VAR1
  ORIGINAL[8]: hctlv -> mutex
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: mutex
  TYPE[9]: FIELD_IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: hctlv
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: hctlv
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1
  ORIGINAL[12]: hctlv
  TYPE[12]: IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: hctlv
  TYPE[13]: IDENTIFIER
  TOKENIZED[13]: VAR1
  ORIGINAL[14]: hctlv
  TYPE[14]: IDENTIFIER
  TOKENIZED[14]: VAR1
  ORIGINAL[15]: hctlv
  TYPE[15]: IDENTIFIER
  TOKENIZED[15]: VAR1
  ORIGINAL[16]: hctlv
  TYPE[16]: IDENTIFIER
  TOKENIZED[16]: VAR1
  ORIGINAL[17]: hctlv
  TYPE[17]: IDENTIFIER
  TOKENIZED[17]: VAR1
  ORIGINAL[18]: hctlv
  TYPE[18]: IDENTIFIER
  TOKENIZED[18]: VAR1
  ORIGINAL[19]: hctlv
  TYPE[19]: IDENTIFIER
  TOKENIZED[19]: VAR1
  ORIGINAL[20]: hctlv
  TYPE[20]: IDENTIFIER
  TOKENIZED[20]: VAR1
  ORIGINAL[21]: hctlv
  TYPE[21]: IDENTIFIER
  TOKENIZED[21]: VAR1

CENTER_NODE: 47244640382
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772060
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hashp -> hctl -> nentries
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[1]: hashp -> hctl
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: nentries
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771261
FRAGMENT_COUNT: 5
  ORIGINAL[0]: flags & 0x040
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 & 0x040
  ORIGINAL[1]: hashp -> hctl = info -> hctl
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 = VAR3 -> VAR2
  ORIGINAL[2]: hashp -> hctl
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: info -> hctl
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772532
FRAGMENT_COUNT: 1
  ORIGINAL[0]: *seq_scan_tables[100]
  TYPE[0]: CALL
  TOKENIZED[0]: *seq_scan_tables [ 100 ]

CENTER_NODE: 30064772203
FRAGMENT_COUNT: 7
  ORIGINAL[0]: new_segnum >= hctl -> nsegs
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= VAR2 -> VAR3
  ORIGINAL[1]: !(hashp -> dir[new_segnum] = seg_alloc(hashp))
  TYPE[1]: CALL
  TOKENIZED[1]: ! ( VAR1 -> VAR2 [ VAR3 ] = FUN1 ( VAR1 ) )
  ORIGINAL[2]: old_bucket = new_bucket & (hctl -> low_mask)
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = VAR2 & ( VAR3 -> VAR4 )
  ORIGINAL[3]: new_bucket & (hctl -> low_mask)
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 & ( VAR2 -> VAR3 )
  ORIGINAL[4]: hctl -> low_mask
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: old_bucket
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: new_bucket
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 30064771484
FRAGMENT_COUNT: 4
  ORIGINAL[0]: (((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1))) + (((intptr_t )entrysize) + (8 - 1) & ~((intptr_t )(8 - 1)))
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR1 ) ( 8 - 1 ) ) ) + ( ( ( VAR1 ) VAR3 ) + ( 8 - 1 ) & ~ ( ( VAR1 ) ( 8 - 1 ) ) )
  ORIGINAL[1]: ((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1))
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR1 ) ( 8 - 1 ) )
  ORIGINAL[2]: ((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1)
  TYPE[2]: CALL
  TOKENIZED[2]: ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 )
  ORIGINAL[3]: ~((intptr_t )(8 - 1))
  TYPE[3]: CALL
  TOKENIZED[3]: ~ ( ( VAR1 ) ( 8 - 1 ) )

CENTER_NODE: 30064771168
FRAGMENT_COUNT: 4
  ORIGINAL[0]: strncmp(key1,key2,keysize - 1)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 - 1 )
  ORIGINAL[1]: keysize - 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 - 1
  ORIGINAL[2]: key1
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: key2
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772147
FRAGMENT_COUNT: 7
  ORIGINAL[0]: (curElem = segp[segment_ndx]) == ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 = VAR2 [ VAR3 ] ) == ( ( void * ) 0 )
  ORIGINAL[1]: ((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1))
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR1 ) ( 8 - 1 ) )
  ORIGINAL[2]: ((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1)
  TYPE[2]: CALL
  TOKENIZED[2]: ( ( VAR1 ) ( sizeof ( VAR2 ) ) ) + ( 8 - 1 )
  ORIGINAL[3]: (intptr_t )(sizeof(HASHELEMENT ))
  TYPE[3]: CALL
  TOKENIZED[3]: ( VAR1 ) ( sizeof ( VAR2 ) )
  ORIGINAL[4]: 8 - 1
  TYPE[4]: CALL
  TOKENIZED[4]: 8 - 1
  ORIGINAL[5]: ~((intptr_t )(8 - 1))
  TYPE[5]: CALL
  TOKENIZED[5]: ~ ( ( VAR1 ) ( 8 - 1 ) )
  ORIGINAL[6]: intptr_t
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 68719477107
FRAGMENT_COUNT: 2
  ORIGINAL[0]: sizeof(HASHHDR )
  TYPE[0]: CALL
  TOKENIZED[0]: sizeof ( VAR1 )
  ORIGINAL[1]: HASHHDR
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1

CENTER_NODE: 30064771159
FRAGMENT_COUNT: 9
  ORIGINAL[0]: *stonesoup_tainted_buff != 0
  TYPE[0]: CALL
  TOKENIZED[0]: *stonesoup_tainted_buff != 0
  ORIGINAL[1]: *stonesoup_tainted_buff
  TYPE[1]: CALL
  TOKENIZED[1]: *stonesoup_tainted_buff
  ORIGINAL[2]: fread(*stonesoup_tainted_buff,1,stonesoup_lsize,stonesoup_tainted_file)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( *stonesoup_tainted_buff , 1 , VAR1 , VAR2 )
  ORIGINAL[3]: *stonesoup_tainted_buff
  TYPE[3]: CALL
  TOKENIZED[3]: *stonesoup_tainted_buff
  ORIGINAL[4]: *stonesoup_tainted_buff
  TYPE[4]: CALL
  TOKENIZED[4]: *stonesoup_tainted_buff
  ORIGINAL[5]: stonesoup_tainted_buff
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: stonesoup_lsize
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: stonesoup_tainted_file
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: stonesoup_tainted_buff
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 68719477422
FRAGMENT_COUNT: 8
  ORIGINAL[0]: hashp -> tabname
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: !hashp -> frozen
  TYPE[1]: CALL
  TOKENIZED[1]: !hashp -> VAR1
  ORIGINAL[2]: hashp -> frozen
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: has_seq_scans(hashp)
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( VAR1 )
  ORIGINAL[4]: hashp -> tabname
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 30064772461
FRAGMENT_COUNT: 3
  ORIGINAL[0]: 1L << my_log2(num)
  TYPE[0]: CALL
  TOKENIZED[0]: 1L << FUN1 ( VAR1 )
  ORIGINAL[1]: my_log2(num)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )
  ORIGINAL[2]: num
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064772454
FRAGMENT_COUNT: 4
  ORIGINAL[0]: num > 9223372036854775807L / 2
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > 9223372036854775807L / 2
  ORIGINAL[1]: num = 9223372036854775807L / 2
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = 9223372036854775807L / 2
  ORIGINAL[2]: 9223372036854775807L / 2
  TYPE[2]: CALL
  TOKENIZED[2]: 9223372036854775807L / 2
  ORIGINAL[3]: num
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772373
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (((intptr_t )_vstart) & sizeof(long ) - 1) == 0 && (_len & sizeof(long ) - 1) == 0 && _val == 0 && _len <= 1024 && 1024 != 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( ( VAR1 ) VAR2 ) & sizeof ( long ) - 1 ) == 0 && ( VAR3 & sizeof ( long ) - 1 ) == 0 && VAR4 == 0 && VAR3 <= 1024 && 1024 != 0
  ORIGINAL[1]: ((char *)_start) + _len
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( char * ) VAR1 ) + VAR2
  ORIGINAL[2]: (char *)_start
  TYPE[2]: CALL
  TOKENIZED[2]: ( char * ) VAR1
  ORIGINAL[3]: _start
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: _len
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772070
FRAGMENT_COUNT: 6
  ORIGINAL[0]: !hashp -> frozen
  TYPE[0]: CALL
  TOKENIZED[0]: !hashp -> VAR1
  ORIGINAL[1]: hashp -> frozen
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: frozen
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 47244640338
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477644
FRAGMENT_COUNT: 4
  ORIGINAL[0]: i--
  TYPE[0]: CALL
  TOKENIZED[0]: i--
  ORIGINAL[1]: seq_scan_tables[i] == hashp
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ VAR2 ] == VAR3
  ORIGINAL[2]: i
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: i
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719476977
FRAGMENT_COUNT: 7
  ORIGINAL[0]: (((intptr_t )_vstart) & sizeof(long ) - 1) == 0 && (_len & sizeof(long ) - 1) == 0 && _val == 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( ( VAR1 ) VAR2 ) & sizeof ( long ) - 1 ) == 0 && ( VAR3 & sizeof ( long ) - 1 ) == 0 && VAR4 == 0
  ORIGINAL[1]: _len <= 1024
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 <= 1024
  ORIGINAL[2]: _len
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: _len
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: _len
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: _len
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: _len
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 47244640318
FRAGMENT_COUNT: 1
  ORIGINAL[0]: hashp != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )

CENTER_NODE: 30064771994
FRAGMENT_COUNT: 6
  ORIGINAL[0]: currBucket == ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 == ( ( void * ) 0 )
  ORIGINAL[1]: action == HASH_ENTER_NULL
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 == VAR2
  ORIGINAL[2]: currBucket -> hashvalue = hashvalue
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 = VAR2
  ORIGINAL[3]: currBucket -> hashvalue
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hashvalue
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 47244640347
FRAGMENT_COUNT: 1
  ORIGINAL[0]: for (;;)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: for ( ; ; )

CENTER_NODE: 47244640377
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771602
FRAGMENT_COUNT: 4
  ORIGINAL[0]: size = add_size(size,mul_size(nDirEntries,sizeof(HASHSEGMENT )))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = FUN1 ( VAR1 , FUN2 ( VAR2 , sizeof ( VAR3 ) ) )
  ORIGINAL[1]: add_size(size,mul_size(nDirEntries,sizeof(HASHSEGMENT )))
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , FUN2 ( VAR2 , sizeof ( VAR3 ) ) )
  ORIGINAL[2]: size
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: size
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772263
FRAGMENT_COUNT: 50
  ORIGINAL[0]: hashp -> hctl -> max_dsize != (- 1)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3 != ( - 1 )
  ORIGINAL[1]: hashp -> hctl -> max_dsize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[2]: - 1
  TYPE[2]: CALL
  TOKENIZED[2]: - 1
  ORIGINAL[3]: (bool )0
  TYPE[3]: CALL
  TOKENIZED[3]: ( VAR1 ) 0
  ORIGINAL[4]: new_dsize = hashp -> hctl -> dsize << 1
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 = VAR2 -> VAR3 -> VAR4 << 1
  ORIGINAL[5]: hashp -> hctl -> dsize << 1
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2 -> VAR3 << 1
  ORIGINAL[6]: hashp -> hctl -> dsize
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[7]: hashp -> hctl
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2
  ORIGINAL[8]: old_dirsize = ((hashp -> hctl -> dsize) * sizeof(HASHSEGMENT ))
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 = ( ( VAR2 -> VAR3 -> VAR4 ) * sizeof ( VAR5 ) )
  ORIGINAL[9]: (hashp -> hctl -> dsize) * sizeof(HASHSEGMENT )
  TYPE[9]: CALL
  TOKENIZED[9]: ( VAR1 -> VAR2 -> VAR3 ) * sizeof ( VAR4 )
  ORIGINAL[10]: hashp -> hctl -> dsize
  TYPE[10]: CALL
  TOKENIZED[10]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[11]: hashp -> hctl
  TYPE[11]: CALL
  TOKENIZED[11]: VAR1 -> VAR2
  ORIGINAL[12]: sizeof(HASHSEGMENT )
  TYPE[12]: CALL
  TOKENIZED[12]: sizeof ( VAR1 )
  ORIGINAL[13]: new_dirsize = (new_dsize * sizeof(HASHSEGMENT ))
  TYPE[13]: CALL
  TOKENIZED[13]: VAR1 = ( VAR2 * sizeof ( VAR3 ) )
  ORIGINAL[14]: new_dsize * sizeof(HASHSEGMENT )
  TYPE[14]: CALL
  TOKENIZED[14]: VAR1 * sizeof ( VAR2 )
  ORIGINAL[15]: sizeof(HASHSEGMENT )
  TYPE[15]: CALL
  TOKENIZED[15]: sizeof ( VAR1 )
  ORIGINAL[16]: old_p = hashp -> dir
  TYPE[16]: CALL
  TOKENIZED[16]: VAR1 = VAR2 -> VAR3
  ORIGINAL[17]: hashp -> dir
  TYPE[17]: CALL
  TOKENIZED[17]: VAR1 -> VAR2
  ORIGINAL[18]: CurrentDynaHashCxt = hashp -> hcxt
  TYPE[18]: CALL
  TOKENIZED[18]: VAR1 = VAR2 -> VAR3
  ORIGINAL[19]: hashp -> hcxt
  TYPE[19]: CALL
  TOKENIZED[19]: VAR1 -> VAR2
  ORIGINAL[20]: p = ((HASHSEGMENT *)((hashp -> alloc)(((Size )new_dirsize))))
  TYPE[20]: CALL
  TOKENIZED[20]: VAR1 = ( ( VAR2 * ) ( ( VAR3 -> VAR4 ) ( ( ( VAR5 ) VAR6 ) ) ) )
  ORIGINAL[21]: (HASHSEGMENT *)((hashp -> alloc)(((Size )new_dirsize)))
  TYPE[21]: CALL
  TOKENIZED[21]: ( VAR1 * ) ( ( VAR2 -> VAR3 ) ( ( ( VAR4 ) VAR5 ) ) )
  ORIGINAL[22]: (hashp -> alloc)(((Size )new_dirsize))
  TYPE[22]: CALL
  TOKENIZED[22]: ( VAR1 -> VAR2 ) ( ( ( VAR3 ) VAR4 ) )
  ORIGINAL[23]: hashp -> alloc
  TYPE[23]: CALL
  TOKENIZED[23]: VAR1 -> VAR2
  ORIGINAL[24]: (Size )new_dirsize
  TYPE[24]: CALL
  TOKENIZED[24]: ( VAR1 ) VAR2
  ORIGINAL[25]: p != ((void *)0)
  TYPE[25]: CALL
  TOKENIZED[25]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[26]: (void *)0
  TYPE[26]: CALL
  TOKENIZED[26]: ( void * ) 0
  ORIGINAL[27]: hctl
  TYPE[27]: FIELD_IDENTIFIER
  TOKENIZED[27]: VAR1
  ORIGINAL[28]: dsize
  TYPE[28]: FIELD_IDENTIFIER
  TOKENIZED[28]: VAR1
  ORIGINAL[29]: hctl
  TYPE[29]: FIELD_IDENTIFIER
  TOKENIZED[29]: VAR1
  ORIGINAL[30]: dsize
  TYPE[30]: FIELD_IDENTIFIER
  TOKENIZED[30]: VAR1
  ORIGINAL[31]: dir
  TYPE[31]: FIELD_IDENTIFIER
  TOKENIZED[31]: VAR1
  ORIGINAL[32]: hcxt
  TYPE[32]: FIELD_IDENTIFIER
  TOKENIZED[32]: VAR1
  ORIGINAL[33]: alloc
  TYPE[33]: FIELD_IDENTIFIER
  TOKENIZED[33]: VAR1
  ORIGINAL[34]: new_dsize
  TYPE[34]: IDENTIFIER
  TOKENIZED[34]: VAR1
  ORIGINAL[35]: hashp
  TYPE[35]: IDENTIFIER
  TOKENIZED[35]: VAR1
  ORIGINAL[36]: old_dirsize
  TYPE[36]: IDENTIFIER
  TOKENIZED[36]: VAR1
  ORIGINAL[37]: hashp
  TYPE[37]: IDENTIFIER
  TOKENIZED[37]: VAR1
  ORIGINAL[38]: HASHSEGMENT
  TYPE[38]: IDENTIFIER
  TOKENIZED[38]: VAR1
  ORIGINAL[39]: new_dirsize
  TYPE[39]: IDENTIFIER
  TOKENIZED[39]: VAR1
  ORIGINAL[40]: new_dsize
  TYPE[40]: IDENTIFIER
  TOKENIZED[40]: VAR1
  ORIGINAL[41]: HASHSEGMENT
  TYPE[41]: IDENTIFIER
  TOKENIZED[41]: VAR1
  ORIGINAL[42]: old_p
  TYPE[42]: IDENTIFIER
  TOKENIZED[42]: VAR1
  ORIGINAL[43]: hashp
  TYPE[43]: IDENTIFIER
  TOKENIZED[43]: VAR1
  ORIGINAL[44]: <global> CurrentDynaHashCxt
  TYPE[44]: IDENTIFIER
  TOKENIZED[44]: <global> VAR1
  ORIGINAL[45]: hashp
  TYPE[45]: IDENTIFIER
  TOKENIZED[45]: VAR1
  ORIGINAL[46]: p
  TYPE[46]: IDENTIFIER
  TOKENIZED[46]: VAR1
  ORIGINAL[47]: hashp
  TYPE[47]: IDENTIFIER
  TOKENIZED[47]: VAR1
  ORIGINAL[48]: new_dirsize
  TYPE[48]: IDENTIFIER
  TOKENIZED[48]: VAR1
  ORIGINAL[49]: p
  TYPE[49]: IDENTIFIER
  TOKENIZED[49]: VAR1

CENTER_NODE: 30064772158
FRAGMENT_COUNT: 6
  ORIGINAL[0]: !status -> hashp -> frozen
  TYPE[0]: CALL
  TOKENIZED[0]: !status -> VAR1 -> VAR2
  ORIGINAL[1]: status -> hashp -> frozen
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[2]: deregister_seq_scan(status -> hashp)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 -> VAR2 )
  ORIGINAL[3]: status -> hashp
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hashp
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: status
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477105
FRAGMENT_COUNT: 4
  ORIGINAL[0]: nDirEntries < nSegments
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: nDirEntries <<= 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 <<= 1
  ORIGINAL[2]: nDirEntries
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: nDirEntries
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477114
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (hashp -> hash)(keyPtr,hashp -> keysize)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 -> VAR2 ) ( VAR3 , VAR1 -> VAR4 )
  ORIGINAL[1]: hashp -> hash
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hashp -> keysize
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: keyPtr
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640397
FRAGMENT_COUNT: 1
  ORIGINAL[0]: num > (2147483647 / 2)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > ( 2147483647 / 2 )

