# Tokenized code fragments for 152946-v1.0.0-bad
# Total center nodes processed: 32
# Total code fragments found: 162

CENTER_NODE: 30064771697
FRAGMENT_COUNT: 3
  ORIGINAL[0]: file -> fd = - 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = - 1
  ORIGINAL[1]: file -> fd
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: - 1
  TYPE[2]: CALL
  TOKENIZED[2]: - 1

CENTER_NODE: 30064771192
FRAGMENT_COUNT: 1
  ORIGINAL[0]: window[32768]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ 32768 ]

CENTER_NODE: 68719476805
FRAGMENT_COUNT: 7
  ORIGINAL[0]: *extension != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: *extension != ( ( void * ) 0 )
  ORIGINAL[1]: *extension
  TYPE[1]: CALL
  TOKENIZED[1]: *extension
  ORIGINAL[2]: *extension
  TYPE[2]: CALL
  TOKENIZED[2]: *extension
  ORIGINAL[3]: extension
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: extension
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: gpointer
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: extension
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 30064771676
FRAGMENT_COUNT: 5
  ORIGINAL[0]: fh -> err != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 != 0
  ORIGINAL[1]: *err_info = (fh -> err_info == ((void *)0)?((void *)0) : g_strdup(fh -> err_info))
  TYPE[1]: CALL
  TOKENIZED[1]: *err_info = ( VAR1 -> VAR2 == ( ( void * ) 0 ) ? ( ( void * ) 0 ) : FUN1 ( VAR1 -> VAR2 ) )
  ORIGINAL[2]: *err_info
  TYPE[2]: CALL
  TOKENIZED[2]: *err_info
  ORIGINAL[3]: fh -> err_info == ((void *)0)?((void *)0) : g_strdup(fh -> err_info)
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2 == ( ( void * ) 0 ) ? ( ( void * ) 0 ) : FUN1 ( VAR1 -> VAR2 )
  ORIGINAL[4]: fh
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771425
FRAGMENT_COUNT: 5
  ORIGINAL[0]: stream -> fast_seek = seek
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = VAR3
  ORIGINAL[1]: stream -> fast_seek
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: fast_seek
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: stream
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: seek
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771325
FRAGMENT_COUNT: 4
  ORIGINAL[0]: state -> avail_in = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = 0
  ORIGINAL[1]: state -> avail_in
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: avail_in
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: state
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771716
FRAGMENT_COUNT: 6
  ORIGINAL[0]: file -> err = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = 0
  ORIGINAL[1]: file -> err
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: err
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: file
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: file
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: file
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477169
FRAGMENT_COUNT: 4
  ORIGINAL[0]: (fd = open(path,0 | 0,0)) == - 1
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 = FUN1 ( VAR2 , 0 | 0 , 0 ) ) == - 1
  ORIGINAL[1]: file -> fd
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: fd
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: file
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771413
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (fd = open(path,0 | 0,0)) == - 1
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 = FUN1 ( VAR2 , 0 | 0 , 0 ) ) == - 1
  ORIGINAL[1]: fd = open(path,0 | 0,0)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = FUN1 ( VAR2 , 0 | 0 , 0 )
  ORIGINAL[2]: open(path,0 | 0,0)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , 0 | 0 , 0 )
  ORIGINAL[3]: - 1
  TYPE[3]: CALL
  TOKENIZED[3]: - 1
  ORIGINAL[4]: fd
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640346
FRAGMENT_COUNT: 1
  ORIGINAL[0]: buf == str
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 == VAR2

CENTER_NODE: 68719477053
FRAGMENT_COUNT: 3
  ORIGINAL[0]: stream -> pos
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: pos
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: stream
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640305
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771129
FRAGMENT_COUNT: 1
  ORIGINAL[0]: base_path[20]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ 20 ]

CENTER_NODE: 68719476894
FRAGMENT_COUNT: 16
  ORIGINAL[0]: state -> avail_in
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: state -> avail_in
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: state -> raw_pos
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: state -> avail_in
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: state -> raw
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: state -> pos
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: state -> out
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: state -> avail_in
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2
  ORIGINAL[8]: state -> next
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: state -> have
  TYPE[9]: CALL
  TOKENIZED[9]: VAR1 -> VAR2
  ORIGINAL[10]: state -> next_in
  TYPE[10]: CALL
  TOKENIZED[10]: VAR1 -> VAR2
  ORIGINAL[11]: state -> avail_in
  TYPE[11]: CALL
  TOKENIZED[11]: VAR1 -> VAR2
  ORIGINAL[12]: avail_in
  TYPE[12]: FIELD_IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: state
  TYPE[13]: IDENTIFIER
  TOKENIZED[13]: VAR1
  ORIGINAL[14]: state
  TYPE[14]: IDENTIFIER
  TOKENIZED[14]: VAR1
  ORIGINAL[15]: state
  TYPE[15]: IDENTIFIER
  TOKENIZED[15]: VAR1

CENTER_NODE: 30064771229
FRAGMENT_COUNT: 4
  ORIGINAL[0]: file -> fast_seek -> len != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3 != 0
  ORIGINAL[1]: file -> fast_seek -> pdata[file -> fast_seek -> len - 1]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 -> VAR3 [ VAR1 -> VAR2 -> VAR4 - 1 ]
  ORIGINAL[2]: file -> fast_seek -> len - 1
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 -> VAR3 - 1
  ORIGINAL[3]: file -> fast_seek -> len
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2 -> VAR3

CENTER_NODE: 68719477156
FRAGMENT_COUNT: 6
  ORIGINAL[0]: file -> eof && file -> avail_in == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 && VAR1 -> VAR3 == 0
  ORIGINAL[1]: file -> eof
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: file -> avail_in
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: file -> have
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: have
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: file
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477045
FRAGMENT_COUNT: 7
  ORIGINAL[0]: state -> compression == 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == 1
  ORIGINAL[1]: state -> compression
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: state -> out
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: out
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: state
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: state
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: state
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 68719476745
FRAGMENT_COUNT: 3
  ORIGINAL[0]: size_filepath = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = 0
  ORIGINAL[1]: retval = 0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = 0
  ORIGINAL[2]: retval
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771534
FRAGMENT_COUNT: 3
  ORIGINAL[0]: stream -> raw_pos
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: raw_pos
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: stream
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719476794
FRAGMENT_COUNT: 6
  ORIGINAL[0]: c >= 97
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= 97
  ORIGINAL[1]: c <= 122
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 <= 122
  ORIGINAL[2]: c
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: c
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: c
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: c
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771160
FRAGMENT_COUNT: 10
  ORIGINAL[0]: ret <= 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 <= 0
  ORIGINAL[1]: state -> raw_pos += ret
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 += VAR3
  ORIGINAL[2]: state -> raw_pos
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: raw_pos
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: state
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: state
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: ret
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: state
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: state
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: state
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 30064771191
FRAGMENT_COUNT: 1
  ORIGINAL[0]: window[32768]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ 32768 ]

CENTER_NODE: 68719477052
FRAGMENT_COUNT: 2
  ORIGINAL[0]: file_seek(file,delta,1,err)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , 1 , VAR3 )
  ORIGINAL[1]: err
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1

CENTER_NODE: 30064771546
FRAGMENT_COUNT: 3
  ORIGINAL[0]: stream -> is_compressed
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: is_compressed
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: stream
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477058
FRAGMENT_COUNT: 3
  ORIGINAL[0]: fstat(stream -> fd,statb)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , VAR3 )
  ORIGINAL[1]: stream -> fd
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: statb
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477070
FRAGMENT_COUNT: 14
  ORIGINAL[0]: file -> seek
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: file -> skip
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: file -> have
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: file -> have
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: file -> next
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: file -> have
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: file -> eof
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: file -> avail_in
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2
  ORIGINAL[8]: file -> pos
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: have
  TYPE[9]: FIELD_IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: file
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: n
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1
  ORIGINAL[12]: file
  TYPE[12]: IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: file
  TYPE[13]: IDENTIFIER
  TOKENIZED[13]: VAR1

CENTER_NODE: 30064771692
FRAGMENT_COUNT: 4
  ORIGINAL[0]: stream -> eof = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = 0
  ORIGINAL[1]: stream -> eof
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: eof
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: stream
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477188
FRAGMENT_COUNT: 3
  ORIGINAL[0]: file -> pos + offset
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 + VAR3
  ORIGINAL[1]: file -> pos
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: <global> offset
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: <global> VAR1

CENTER_NODE: 68719477012
FRAGMENT_COUNT: 14
  ORIGINAL[0]: offset < 0 && file -> next
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < 0 && VAR2 -> VAR3
  ORIGINAL[1]: -offset <= had
  TYPE[1]: CALL
  TOKENIZED[1]: -offset <= VAR1
  ORIGINAL[2]: here = fast_seek_find(file,file -> pos + offset)
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = FUN1 ( VAR2 , VAR2 -> VAR3 + VAR4 )
  ORIGINAL[3]: fast_seek_find(file,file -> pos + offset)
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( VAR1 , VAR1 -> VAR2 + VAR3 )
  ORIGINAL[4]: here -> compression
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: here -> in
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: here -> out
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: here -> compression
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2
  ORIGINAL[8]: here
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: file
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: here
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: here
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1
  ORIGINAL[12]: here
  TYPE[12]: IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: here
  TYPE[13]: IDENTIFIER
  TOKENIZED[13]: VAR1

CENTER_NODE: 30064771190
FRAGMENT_COUNT: 8
  ORIGINAL[0]: raw_read(state,state -> in,state -> size,((unsigned int *)(&state -> avail_in))) == - 1
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR1 -> VAR2 , VAR1 -> VAR3 , ( ( unsigned int * ) ( &state -> VAR4 ) ) ) == - 1
  ORIGINAL[1]: state -> in
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: state -> next_in = state -> in
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 = VAR1 -> VAR3
  ORIGINAL[3]: state -> next_in
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: state -> in
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: in
  TYPE[5]: FIELD_IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: state
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: state
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 68719476840
FRAGMENT_COUNT: 9
  ORIGINAL[0]: !file -> fast_seek
  TYPE[0]: CALL
  TOKENIZED[0]: !file -> VAR1
  ORIGINAL[1]: low < max
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 < VAR2
  ORIGINAL[2]: pos < item -> out
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 < VAR2 -> VAR3
  ORIGINAL[3]: pos > item -> out
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 > VAR2 -> VAR3
  ORIGINAL[4]: max
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: low
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: max
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: max
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: max
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 47244640335
FRAGMENT_COUNT: 1
  ORIGINAL[0]: file -> have
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2

