# Tokenized code fragments for 153786-v1.0.0-bad
# Total center nodes processed: 41
# Total code fragments found: 186

CENTER_NODE: 68719476977
FRAGMENT_COUNT: 15
  ORIGINAL[0]: hctl -> nentries
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hctl -> freeList
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctl -> dsize
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hctl -> nsegs
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hctl -> keysize
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hctl -> entrysize
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: hctl -> num_partitions
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: hctl -> ffactor
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2
  ORIGINAL[8]: hctl -> max_dsize
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: hctl -> ssize = 256
  TYPE[9]: CALL
  TOKENIZED[9]: VAR1 -> VAR2 = 256
  ORIGINAL[10]: hctl -> ssize
  TYPE[10]: CALL
  TOKENIZED[10]: VAR1 -> VAR2
  ORIGINAL[11]: hctl -> sshift
  TYPE[11]: CALL
  TOKENIZED[11]: VAR1 -> VAR2
  ORIGINAL[12]: sshift
  TYPE[12]: FIELD_IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: hctl
  TYPE[13]: IDENTIFIER
  TOKENIZED[13]: VAR1
  ORIGINAL[14]: hctl
  TYPE[14]: IDENTIFIER
  TOKENIZED[14]: VAR1

CENTER_NODE: 30064772350
FRAGMENT_COUNT: 6
  ORIGINAL[0]: num > (2147483647 / 2)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > ( 2147483647 / 2 )
  ORIGINAL[1]: 2147483647 / 2
  TYPE[1]: CALL
  TOKENIZED[1]: 2147483647 / 2
  ORIGINAL[2]: num = (2147483647 / 2)
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = ( 2147483647 / 2 )
  ORIGINAL[3]: 2147483647 / 2
  TYPE[3]: CALL
  TOKENIZED[3]: 2147483647 / 2
  ORIGINAL[4]: num
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: num
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477077
FRAGMENT_COUNT: 4
  ORIGINAL[0]: nBuckets - 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 - 1
  ORIGINAL[1]: nBuckets
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: nSegments
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: nBuckets
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477331
FRAGMENT_COUNT: 8
  ORIGINAL[0]: hashp -> tabname
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: !hashp -> frozen && has_seq_scans(hashp)
  TYPE[1]: CALL
  TOKENIZED[1]: !hashp -> VAR1 && FUN1 ( VAR2 )
  ORIGINAL[2]: hashp -> frozen
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hashp -> tabname
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hashp -> frozen
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: frozen
  TYPE[5]: FIELD_IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 68719477601
FRAGMENT_COUNT: 2
  ORIGINAL[0]: ++stonesoup_global_variable
  TYPE[0]: CALL
  TOKENIZED[0]: ++stonesoup_global_variable
  ORIGINAL[1]: <global> stonesoup_global_variable
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: <global> VAR1

CENTER_NODE: 68719476799
FRAGMENT_COUNT: 4
  ORIGINAL[0]: strncmp(key1,key2,keysize - 1)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 - 1 )
  ORIGINAL[1]: keysize - 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 - 1
  ORIGINAL[2]: key1
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: key2
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477070
FRAGMENT_COUNT: 6
  ORIGINAL[0]: mul_size(nElementAllocs,mul_size(elementAllocCnt,elementSize))
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , FUN1 ( VAR2 , VAR3 ) )
  ORIGINAL[1]: mul_size(elementAllocCnt,elementSize)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[2]: nElementAllocs
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: size
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: nElementAllocs
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: elementAllocCnt
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477527
FRAGMENT_COUNT: 6
  ORIGINAL[0]: num > 9223372036854775807L / 2
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > 9223372036854775807L / 2
  ORIGINAL[1]: num = 9223372036854775807L / 2
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = 9223372036854775807L / 2
  ORIGINAL[2]: i = 0
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = 0
  ORIGINAL[3]: i
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: i
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: i
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 47244640374
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771631
FRAGMENT_COUNT: 4
  ORIGINAL[0]: hashp != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: hash_stats(\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( \
  ORIGINAL[2]: hashp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771115
FRAGMENT_COUNT: 5
  ORIGINAL[0]: filepath != NULL
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != VAR2
  ORIGINAL[1]: stonesoup_printf_context = fopen(filepath, \
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = FUN1 ( VAR2 , \
  ORIGINAL[2]: fopen(filepath, \
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , \
  ORIGINAL[3]: <global> stonesoup_printf_context
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: <global> VAR1
  ORIGINAL[4]: filepath
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477507
FRAGMENT_COUNT: 8
  ORIGINAL[0]: i < nelem
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: tmpElement -> link
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: prevElement = tmpElement
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = VAR2
  ORIGINAL[3]: tmpElement
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: prevElement
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: tmpElement
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: tmpElement
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: tmpElement
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 30064771949
FRAGMENT_COUNT: 5
  ORIGINAL[0]: hashp -> hctl -> nentries
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[1]: hashp -> hctl
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctl
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: nentries
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640330
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477644
FRAGMENT_COUNT: 3
  ORIGINAL[0]: * stonesoup_printf_context = NULL
  TYPE[0]: CALL
  TOKENIZED[0]: * VAR1 = VAR2
  ORIGINAL[1]: stonesoup_printf_context
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: NULL
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064772440
FRAGMENT_COUNT: 5
  ORIGINAL[0]: stonesoup_data != NULL
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != VAR2
  ORIGINAL[1]: stonesoup_oc_i < strlen(stonesoup_data)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 < FUN1 ( VAR2 )
  ORIGINAL[2]: strlen(stonesoup_data)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 )
  ORIGINAL[3]: stonesoup_oc_i
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: stonesoup_data
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772349
FRAGMENT_COUNT: 3
  ORIGINAL[0]: 1L << my_log2(num)
  TYPE[0]: CALL
  TOKENIZED[0]: 1L << FUN1 ( VAR1 )
  ORIGINAL[1]: my_log2(num)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )
  ORIGINAL[2]: num
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640382
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640369
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772147
FRAGMENT_COUNT: 5
  ORIGINAL[0]: new_segnum >= hctl -> nsegs
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= VAR2 -> VAR3
  ORIGINAL[1]: !(hashp -> dir[new_segnum] = seg_alloc(hashp))
  TYPE[1]: CALL
  TOKENIZED[1]: ! ( VAR1 -> VAR2 [ VAR3 ] = FUN1 ( VAR1 ) )
  ORIGINAL[2]: *newlink = ((void *)0)
  TYPE[2]: CALL
  TOKENIZED[2]: *newlink = ( ( void * ) 0 )
  ORIGINAL[3]: *newlink
  TYPE[3]: CALL
  TOKENIZED[3]: *newlink
  ORIGINAL[4]: (void *)0
  TYPE[4]: CALL
  TOKENIZED[4]: ( void * ) 0

CENTER_NODE: 68719477524
FRAGMENT_COUNT: 4
  ORIGINAL[0]: hashp -> isshared
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hashp -> tabname
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: tabname
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771261
FRAGMENT_COUNT: 5
  ORIGINAL[0]: flags & 0x040
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 & 0x040
  ORIGINAL[1]: hashp -> hcxt = CurrentDynaHashCxt
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 = VAR3
  ORIGINAL[2]: hashp -> hcxt
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: <global> CurrentDynaHashCxt
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: <global> VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640339
FRAGMENT_COUNT: 1
  ORIGINAL[0]: for (;;)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: for ( ; ; )

CENTER_NODE: 47244640298
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477242
FRAGMENT_COUNT: 12
  ORIGINAL[0]: *hctlv = (hashp -> hctl)
  TYPE[0]: CALL
  TOKENIZED[0]: *hctlv = ( VAR1 -> VAR2 )
  ORIGINAL[1]: hctlv -> num_partitions
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hctlv -> mutex
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hctlv -> mutex
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hctlv -> freeList
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hctlv -> num_partitions
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: hctlv -> mutex
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: !element_alloc(hashp,hctlv -> nelem_alloc)
  TYPE[7]: CALL
  TOKENIZED[7]: !element_alloc ( VAR1 , VAR2 -> VAR3 )
  ORIGINAL[8]: hctlv -> nelem_alloc
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: num_partitions
  TYPE[9]: FIELD_IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: hctlv
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: hctlv
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1

CENTER_NODE: 30064771627
FRAGMENT_COUNT: 6
  ORIGINAL[0]: (info -> dsize) * sizeof(HASHSEGMENT )
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 -> VAR2 ) * sizeof ( VAR3 )
  ORIGINAL[1]: info -> dsize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: sizeof(HASHSEGMENT )
  TYPE[2]: CALL
  TOKENIZED[2]: sizeof ( VAR1 )
  ORIGINAL[3]: dsize
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: info
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: HASHSEGMENT
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 47244640263
FRAGMENT_COUNT: 1
  ORIGINAL[0]: c >= 97 && c <= 122
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= 97 && VAR1 <= 122

CENTER_NODE: 30064772263
FRAGMENT_COUNT: 5
  ORIGINAL[0]: _start < _stop
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: *(_start++) = 0
  TYPE[1]: CALL
  TOKENIZED[1]: * ( _start++ ) = 0
  ORIGINAL[2]: *(_start++)
  TYPE[2]: CALL
  TOKENIZED[2]: * ( _start++ )
  ORIGINAL[3]: while (_start < _stop)
  TYPE[3]: CONTROL_STRUCTURE
  TOKENIZED[3]: while ( VAR1 < VAR2 )
  ORIGINAL[4]: _start
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477582
FRAGMENT_COUNT: 5
  ORIGINAL[0]: num_seq_scans - 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 - 1
  ORIGINAL[1]: i
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: <global> num_seq_scans
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: <global> VAR1
  ORIGINAL[3]: <global> num_seq_scans
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: <global> VAR1
  ORIGINAL[4]: num_seq_scans
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771465
FRAGMENT_COUNT: 4
  ORIGINAL[0]: allocSize = (32 * 4)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = ( 32 * 4 )
  ORIGINAL[1]: 32 * 4
  TYPE[1]: CALL
  TOKENIZED[1]: 32 * 4
  ORIGINAL[2]: allocSize
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: allocSize
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477227
FRAGMENT_COUNT: 14
  ORIGINAL[0]: hashp -> hctl
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hashp -> frozen
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hashp -> sshift
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hashp -> ssize
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hashp -> match
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: hashp -> keysize
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: hashp -> tabname
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: currBucket == ((void *)0)
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 == ( ( void * ) 0 )
  ORIGINAL[8]: action == HASH_ENTER_NULL
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 == VAR2
  ORIGINAL[9]: currBucket -> hashvalue = hashvalue
  TYPE[9]: CALL
  TOKENIZED[9]: VAR1 -> VAR2 = VAR2
  ORIGINAL[10]: hashp -> keycopy
  TYPE[10]: CALL
  TOKENIZED[10]: VAR1 -> VAR2
  ORIGINAL[11]: keycopy
  TYPE[11]: FIELD_IDENTIFIER
  TOKENIZED[11]: VAR1
  ORIGINAL[12]: hashp
  TYPE[12]: IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: hashp
  TYPE[13]: IDENTIFIER
  TOKENIZED[13]: VAR1

CENTER_NODE: 30064772043
FRAGMENT_COUNT: 5
  ORIGINAL[0]: !status -> hashp -> frozen
  TYPE[0]: CALL
  TOKENIZED[0]: !status -> VAR1 -> VAR2
  ORIGINAL[1]: status -> hashp -> frozen
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[2]: status -> hashp
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: status -> hashp
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: frozen
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640268
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771670
FRAGMENT_COUNT: 6
  ORIGINAL[0]: hash_search_with_hash_value(hashp,keyPtr,((hashp -> hash)(keyPtr,hashp -> keysize)),action,foundPtr)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , ( ( VAR1 -> VAR3 ) ( VAR2 , VAR1 -> VAR4 ) ) , VAR5 , VAR6 )
  ORIGINAL[1]: (hashp -> hash)(keyPtr,hashp -> keysize)
  TYPE[1]: CALL
  TOKENIZED[1]: ( VAR1 -> VAR2 ) ( VAR3 , VAR1 -> VAR4 )
  ORIGINAL[2]: hashp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: keyPtr
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: action
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: foundPtr
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477011
FRAGMENT_COUNT: 8
  ORIGINAL[0]: nsegs = next_pow2_int(nsegs)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = FUN1 ( VAR1 )
  ORIGINAL[1]: nsegs > hctl -> dsize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 > VAR2 -> VAR3
  ORIGINAL[2]: hctl -> dsize
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: nsegs
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: nsegs
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hctl
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: nsegs
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: nsegs
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 30064771994
FRAGMENT_COUNT: 3
  ORIGINAL[0]: curBucket > max_bucket
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > VAR2
  ORIGINAL[1]: hash_seq_term(status)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )
  ORIGINAL[2]: status
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477090
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (hashp -> hash)(keyPtr,hashp -> keysize)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 -> VAR2 ) ( VAR3 , VAR1 -> VAR4 )
  ORIGINAL[1]: hashp -> hash
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hashp -> keysize
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: keyPtr
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772456
FRAGMENT_COUNT: 1
  ORIGINAL[0]: *seq_scan_tables[100]
  TYPE[0]: CALL
  TOKENIZED[0]: *seq_scan_tables [ 100 ]

CENTER_NODE: 47244640341
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772203
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (((intptr_t )_vstart) & sizeof(long ) - 1) == 0 && (_len & sizeof(long ) - 1) == 0 && _val == 0 && _len <= 1024 && 1024 != 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( ( VAR1 ) VAR2 ) & sizeof ( long ) - 1 ) == 0 && ( VAR3 & sizeof ( long ) - 1 ) == 0 && VAR4 == 0 && VAR3 <= 1024 && 1024 != 0
  ORIGINAL[1]: *_start = (long *)_vstart
  TYPE[1]: CALL
  TOKENIZED[1]: *_start = ( long * ) VAR1
  ORIGINAL[2]: (long *)_vstart
  TYPE[2]: CALL
  TOKENIZED[2]: ( long * ) VAR1
  ORIGINAL[3]: _start
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: _stop
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771951
FRAGMENT_COUNT: 7
  ORIGINAL[0]: status -> hashp = hashp
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = VAR2
  ORIGINAL[1]: status -> hashp
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: hashp
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: status
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: status
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: status
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

