# Tokenized code fragments for 152179-v1.0.0-bad
# Total center nodes processed: 86
# Total code fragments found: 376

CENTER_NODE: 30064771276
FRAGMENT_COUNT: 4
  ORIGINAL[0]: string_first_non_whitespace(str -> data,str -> len)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , VAR1 -> VAR3 )
  ORIGINAL[1]: str -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640310
FRAGMENT_COUNT: 4
  ORIGINAL[0]: i = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = 0
  ORIGINAL[1]: i < list -> nelts
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 < VAR2 -> VAR3
  ORIGINAL[2]: i++
  TYPE[2]: CALL
  TOKENIZED[2]: i++
  ORIGINAL[3]: for (i = 0;i < list -> nelts;i++)
  TYPE[3]: CONTROL_STRUCTURE
  TOKENIZED[3]: for ( VAR1 = 0 ; VAR1 < VAR2 -> VAR3 ; i++ )

CENTER_NODE: 30064771418
FRAGMENT_COUNT: 3
  ORIGINAL[0]: duumviral_megalocephalous = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = 0
  ORIGINAL[1]: duumviral_megalocephalous
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: <global> ekalaka_raadzaal
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: <global> VAR1

CENTER_NODE: 68719476974
FRAGMENT_COUNT: 5
  ORIGINAL[0]: *strbuf = svn_stringbuf_create_ensure(size,pool)
  TYPE[0]: CALL
  TOKENIZED[0]: *strbuf = FUN1 ( VAR1 , VAR2 )
  ORIGINAL[1]: strbuf -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: data
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: strbuf
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: strbuf
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476771
FRAGMENT_COUNT: 6
  ORIGINAL[0]: filepath != NULL
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != VAR2
  ORIGINAL[1]: sprintf(filepath, \
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , \
  ORIGINAL[2]: filepath
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: filepath
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: dirpath
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: filepath
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719476818
FRAGMENT_COUNT: 6
  ORIGINAL[0]: minimum_size >  *size
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > *size
  ORIGINAL[1]: *size
  TYPE[1]: CALL
  TOKENIZED[1]: *size
  ORIGINAL[2]: membuf_create(data,size,new_size,pool)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , VAR2 , VAR3 , VAR4 )
  ORIGINAL[3]: data
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: size
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: new_size
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771272
FRAGMENT_COUNT: 5
  ORIGINAL[0]: string_compare(str1 -> data,str2 -> data,str1 -> len,str2 -> len)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , VAR3 -> VAR2 , VAR1 -> VAR4 , VAR3 -> VAR4 )
  ORIGINAL[1]: str1 -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str1
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: str2
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477027
FRAGMENT_COUNT: 5
  ORIGINAL[0]: str -> len > 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 > 0
  ORIGINAL[1]: str -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: str -> data
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: data
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: str
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772086
FRAGMENT_COUNT: 3
  ORIGINAL[0]: ++stonesoup_global_variable
  TYPE[0]: CALL
  TOKENIZED[0]: ++stonesoup_global_variable
  ORIGINAL[1]: <global> stonesoup_global_variable
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: <global> VAR1
  ORIGINAL[2]: terraciform_lucan
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771775
FRAGMENT_COUNT: 9
  ORIGINAL[0]: *__errno_location() == 22 || endptr == str || str[0] == '\\0' || ( *endptr) != '\\0'
  TYPE[0]: CALL
  TOKENIZED[0]: *__errno_location ( ) == 22 || VAR1 == VAR2 || VAR2 [ 0 ] == '\\0' || ( *endptr ) != '\\0'
  ORIGINAL[1]: *__errno_location() == 34 && (val == - 9223372036854775807L - 1 || val == 9223372036854775807L)
  TYPE[1]: CALL
  TOKENIZED[1]: *__errno_location ( ) == 34 && ( VAR1 == - 9223372036854775807L - 1 || VAR1 == 9223372036854775807L )
  ORIGINAL[2]: *__errno_location() == 34
  TYPE[2]: CALL
  TOKENIZED[2]: *__errno_location ( ) == 34
  ORIGINAL[3]: *__errno_location()
  TYPE[3]: CALL
  TOKENIZED[3]: *__errno_location ( )
  ORIGINAL[4]: val == - 9223372036854775807L - 1 || val == 9223372036854775807L
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 == - 9223372036854775807L - 1 || VAR1 == 9223372036854775807L
  ORIGINAL[5]: val == - 9223372036854775807L - 1
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 == - 9223372036854775807L - 1
  ORIGINAL[6]: - 9223372036854775807L - 1
  TYPE[6]: CALL
  TOKENIZED[6]: - 9223372036854775807L - 1
  ORIGINAL[7]: - 9223372036854775807L
  TYPE[7]: CALL
  TOKENIZED[7]: - 9223372036854775807L
  ORIGINAL[8]: val
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 30064772090
FRAGMENT_COUNT: 3
  ORIGINAL[0]: ++stonesoup_global_variable
  TYPE[0]: CALL
  TOKENIZED[0]: ++stonesoup_global_variable
  ORIGINAL[1]: <global> stonesoup_global_variable
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: <global> VAR1
  ORIGINAL[2]: tuftier_chider
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719476881
FRAGMENT_COUNT: 3
  ORIGINAL[0]: i != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != 0
  ORIGINAL[1]: len
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: len
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771507
FRAGMENT_COUNT: 5
  ORIGINAL[0]: memmove((str -> data + pos),(str -> data + pos + count),str -> len - pos - count + 1)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( ( VAR1 -> VAR2 + VAR3 ) , ( VAR1 -> VAR2 + VAR3 + VAR4 ) , VAR1 -> VAR5 - VAR3 - VAR4 + 1 )
  ORIGINAL[1]: str -> data + pos
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 + VAR3
  ORIGINAL[2]: str -> data + pos + count
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 + VAR3 + VAR4
  ORIGINAL[3]: str -> len - pos - count + 1
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2 - VAR3 - VAR4 + 1
  ORIGINAL[4]: str -> len - pos - count
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2 - VAR3 - VAR4

CENTER_NODE: 30064771181
FRAGMENT_COUNT: 5
  ORIGINAL[0]: membuf -> data && old_data && old_data != (membuf -> data)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 && VAR3 && VAR3 != ( VAR1 -> VAR2 )
  ORIGINAL[1]: membuf -> data && old_data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 && VAR3
  ORIGINAL[2]: old_data != (membuf -> data)
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 != ( VAR2 -> VAR3 )
  ORIGINAL[3]: membuf -> data
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: old_data
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476824
FRAGMENT_COUNT: 4
  ORIGINAL[0]: membuf_create(&membuf -> data,&membuf -> size,size,pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( &membuf -> VAR1 , &membuf -> VAR2 , VAR2 , VAR3 )
  ORIGINAL[1]: size
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: pool
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: pool
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477281
FRAGMENT_COUNT: 10
  ORIGINAL[0]: i = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = 0
  ORIGINAL[1]: i < list -> nelts
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 < VAR2 -> VAR3
  ORIGINAL[2]: list -> nelts
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: i++
  TYPE[3]: CALL
  TOKENIZED[3]: i++
  ORIGINAL[4]: apr_fnmatch(this_pattern,str,0) == 0
  TYPE[4]: CALL
  TOKENIZED[4]: FUN1 ( VAR1 , VAR2 , 0 ) == 0
  ORIGINAL[5]: i
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: i
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: list
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: i
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: i
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 30064771933
FRAGMENT_COUNT: 7
  ORIGINAL[0]: i > 3
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > 3
  ORIGINAL[1]: memmove((&buffer[i - 2]),(&buffer[i - 3]),length - i + 3)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( ( &buffer [ VAR1 - 2 ] ) , ( &buffer [ VAR1 - 3 ] ) , VAR2 - VAR1 + 3 )
  ORIGINAL[2]: &buffer[i - 2]
  TYPE[2]: CALL
  TOKENIZED[2]: &buffer [ VAR1 - 2 ]
  ORIGINAL[3]: &buffer[i - 3]
  TYPE[3]: CALL
  TOKENIZED[3]: &buffer [ VAR1 - 3 ]
  ORIGINAL[4]: buffer[i - 3]
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 [ VAR2 - 3 ]
  ORIGINAL[5]: length - i + 3
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 - VAR2 + 3
  ORIGINAL[6]: length
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 30064771382
FRAGMENT_COUNT: 9
  ORIGINAL[0]: membuf_ensure(&mem,&str -> blocksize,minimum_size,str -> pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( &mem , &str -> VAR1 , VAR2 , VAR3 -> VAR4 )
  ORIGINAL[1]: str -> pool
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: pool
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: str
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: str
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: str
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: str
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: str
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 47244640271
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771841
FRAGMENT_COUNT: 5
  ORIGINAL[0]: svn_cstring_strtoi64(&val,str,(- 2147483647 - 1),2147483647,10)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( &val , VAR1 , ( - 2147483647 - 1 ) , 2147483647 , 10 )
  ORIGINAL[1]: &val
  TYPE[1]: CALL
  TOKENIZED[1]: &val
  ORIGINAL[2]: val
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: val
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771599
FRAGMENT_COUNT: 5
  ORIGINAL[0]: string_compare(str1 -> data,(str2 -> data),str1 -> len,str2 -> len)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , ( VAR3 -> VAR2 ) , VAR1 -> VAR4 , VAR3 -> VAR4 )
  ORIGINAL[1]: str1 -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: data
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str1
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: str2
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772082
FRAGMENT_COUNT: 3
  ORIGINAL[0]: ++stonesoup_global_variable
  TYPE[0]: CALL
  TOKENIZED[0]: ++stonesoup_global_variable
  ORIGINAL[1]: <global> stonesoup_global_variable
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: <global> VAR1
  ORIGINAL[2]: gobiiformes_gutsy
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771321
FRAGMENT_COUNT: 4
  ORIGINAL[0]: svn_stringbuf_ncreate(str -> data,str -> len,pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , VAR1 -> VAR3 , VAR4 )
  ORIGINAL[1]: str -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: str -> len
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: pool
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771281
FRAGMENT_COUNT: 3
  ORIGINAL[0]: (svn_string_t *)(&strbuf -> data)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 * ) ( &strbuf -> VAR2 )
  ORIGINAL[1]: &strbuf -> data
  TYPE[1]: CALL
  TOKENIZED[1]: &strbuf -> VAR1
  ORIGINAL[2]: strbuf -> data
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2

CENTER_NODE: 68719476848
FRAGMENT_COUNT: 5
  ORIGINAL[0]: *const _m_b_f_ = membuf
  TYPE[0]: CALL
  TOKENIZED[0]: *const VAR1 = VAR2
  ORIGINAL[1]: _m_b_f_ -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: data
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: _m_b_f_
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: _m_b_f_
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476933
FRAGMENT_COUNT: 5
  ORIGINAL[0]: svn_string_createv(pool,fmt,ap)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , VAR3 )
  ORIGINAL[1]: fmt
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: pool
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: fmt
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: ap
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771163
FRAGMENT_COUNT: 4
  ORIGINAL[0]: membuf_ensure(&membuf -> data,&membuf -> size,size,membuf -> pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( &membuf -> VAR1 , &membuf -> VAR2 , VAR2 , VAR3 -> VAR4 )
  ORIGINAL[1]: &membuf -> data
  TYPE[1]: CALL
  TOKENIZED[1]: &membuf -> VAR1
  ORIGINAL[2]: membuf -> data
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: membuf
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771544
FRAGMENT_COUNT: 6
  ORIGINAL[0]: old_count != new_count
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != VAR2
  ORIGINAL[1]: str -> data + pos + new_count
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 + VAR3 + VAR4
  ORIGINAL[2]: str -> data + pos
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 + VAR3
  ORIGINAL[3]: str -> data
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: pos
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: new_count
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477664
FRAGMENT_COUNT: 3
  ORIGINAL[0]: ++stonesoup_global_variable
  TYPE[0]: CALL
  TOKENIZED[0]: ++stonesoup_global_variable
  ORIGINAL[1]: teletypesetter_sinalbin(vraisemblance_texline)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )
  ORIGINAL[2]: vraisemblance_texline
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477675
FRAGMENT_COUNT: 2
  ORIGINAL[0]: ++stonesoup_global_variable
  TYPE[0]: CALL
  TOKENIZED[0]: ++stonesoup_global_variable
  ORIGINAL[1]: <global> stonesoup_global_variable
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: <global> VAR1

CENTER_NODE: 30064771254
FRAGMENT_COUNT: 5
  ORIGINAL[0]: svn_string_ncreate((strbuf -> data),strbuf -> len,pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( ( VAR1 -> VAR2 ) , VAR1 -> VAR3 , VAR4 )
  ORIGINAL[1]: strbuf -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: data
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: strbuf
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: strbuf
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771564
FRAGMENT_COUNT: 5
  ORIGINAL[0]: string_compare((str1 -> data),(str2 -> data),str1 -> len,str2 -> len)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( ( VAR1 -> VAR2 ) , ( VAR3 -> VAR2 ) , VAR1 -> VAR4 , VAR3 -> VAR4 )
  ORIGINAL[1]: str2 -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: data
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str2
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: str1
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477008
FRAGMENT_COUNT: 6
  ORIGINAL[0]: __builtin_va_start(ap,fmt)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[1]: str = svn_stringbuf_createv(pool,fmt,ap)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = FUN1 ( VAR2 , VAR3 , VAR4 )
  ORIGINAL[2]: svn_stringbuf_createv(pool,fmt,ap)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , VAR2 , VAR3 )
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: pool
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: str
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771484
FRAGMENT_COUNT: 5
  ORIGINAL[0]: bytes + count > (str -> data) && bytes < (str -> data + str -> blocksize)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 + VAR2 > ( VAR3 -> VAR4 ) && VAR1 < ( VAR3 -> VAR4 + VAR3 -> VAR5 )
  ORIGINAL[1]: str -> len - pos + 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 - VAR3 + 1
  ORIGINAL[2]: str -> len - pos
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 - VAR3
  ORIGINAL[3]: str -> len
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: pos
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771251
FRAGMENT_COUNT: 4
  ORIGINAL[0]: svn_string_ncreate(cstring,strlen(cstring),pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , FUN2 ( VAR1 ) , VAR2 )
  ORIGINAL[1]: strlen(cstring)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )
  ORIGINAL[2]: cstring
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: pool
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772092
FRAGMENT_COUNT: 3
  ORIGINAL[0]: ++stonesoup_global_variable
  TYPE[0]: CALL
  TOKENIZED[0]: ++stonesoup_global_variable
  ORIGINAL[1]: <global> stonesoup_global_variable
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: <global> VAR1
  ORIGINAL[2]: canadianism_unsoling
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771340
FRAGMENT_COUNT: 6
  ORIGINAL[0]: new_string -> pool = pool
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = VAR2
  ORIGINAL[1]: new_string -> pool
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: pool
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: new_string
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: pool
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: new_string
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771373
FRAGMENT_COUNT: 2
  ORIGINAL[0]: str -> len == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == 0
  ORIGINAL[1]: str -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2

CENTER_NODE: 47244640270
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771705
FRAGMENT_COUNT: 3
  ORIGINAL[0]: count = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = 0
  ORIGINAL[1]: count
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: p
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477625
FRAGMENT_COUNT: 10
  ORIGINAL[0]: i <= slots
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 <= VAR2
  ORIGINAL[1]: ++i
  TYPE[1]: CALL
  TOKENIZED[1]: ++i
  ORIGINAL[2]: ( *pstr) == strb[i - 1]
  TYPE[2]: CALL
  TOKENIZED[2]: ( *pstr ) == VAR1 [ VAR2 - 1 ]
  ORIGINAL[3]: curr[i] = prev[i - 1] + 1
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 [ VAR2 ] = VAR3 [ VAR2 - 1 ] + 1
  ORIGINAL[4]: curr[i] = (curr[i - 1] > prev[i]?curr[i - 1] : prev[i])
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 [ VAR2 ] = ( VAR1 [ VAR2 - 1 ] > VAR3 [ VAR2 ] ?curr [ VAR2 - 1 ] : VAR3 [ VAR2 ] )
  ORIGINAL[5]: i
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: i
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: i
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: i
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: i
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 30064772089
FRAGMENT_COUNT: 2
  ORIGINAL[0]: elevating_clype(corseting_implausibility)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 )
  ORIGINAL[1]: corseting_implausibility
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1

CENTER_NODE: 47244640325
FRAGMENT_COUNT: 1
  ORIGINAL[0]: for (;;)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: for ( ; ; )

CENTER_NODE: 47244640374
FRAGMENT_COUNT: 1
  ORIGINAL[0]: else
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: else

CENTER_NODE: 68719476867
FRAGMENT_COUNT: 5
  ORIGINAL[0]: i < len
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: !(0 != (svn_ctype_table[(unsigned char )str[i]] & 0x0002))
  TYPE[1]: CALL
  TOKENIZED[1]: ! ( 0 != ( VAR1 [ ( unsigned char ) VAR2 [ VAR3 ] ] & 0x0002 ) )
  ORIGINAL[2]: i
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: len
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: len
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771454
FRAGMENT_COUNT: 5
  ORIGINAL[0]: svn_stringbuf_appendbytes(targetstr,(appendstr -> data),appendstr -> len)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , ( VAR2 -> VAR3 ) , VAR2 -> VAR4 )
  ORIGINAL[1]: appendstr -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: data
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: appendstr
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: appendstr
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476855
FRAGMENT_COUNT: 5
  ORIGINAL[0]: _m_b_f_ -> size
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: size
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: _m_b_f_
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: _s_z_
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: _m_b_f_
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771457
FRAGMENT_COUNT: 3
  ORIGINAL[0]: svn_stringbuf_appendbytes(targetstr,cstr,strlen(cstr))
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , FUN2 ( VAR2 ) )
  ORIGINAL[1]: strlen(cstr)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )
  ORIGINAL[2]: cstr
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477238
FRAGMENT_COUNT: 3
  ORIGINAL[0]: str -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: data
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: str
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719476889
FRAGMENT_COUNT: 6
  ORIGINAL[0]: *new_string
  TYPE[0]: CALL
  TOKENIZED[0]: *new_string
  ORIGINAL[1]: new_string -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: new_string -> len = size
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 = VAR3
  ORIGINAL[3]: new_string -> len
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: new_string
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: new_string
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477397
FRAGMENT_COUNT: 2
  ORIGINAL[0]: *n
  TYPE[0]: CALL
  TOKENIZED[0]: *n
  ORIGINAL[1]: n
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1

CENTER_NODE: 68719477356
FRAGMENT_COUNT: 6
  ORIGINAL[0]: a = ( *(str1++))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = ( * ( str1++ ) )
  ORIGINAL[1]: *(str1++)
  TYPE[1]: CALL
  TOKENIZED[1]: * ( str1++ )
  ORIGINAL[2]: cmp || !a || !b
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 || !a || !b
  ORIGINAL[3]: a
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: str1
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: a
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477226
FRAGMENT_COUNT: 5
  ORIGINAL[0]: str -> data += offset
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 += VAR3
  ORIGINAL[1]: str -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: offset
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: offset
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: offset
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477487
FRAGMENT_COUNT: 5
  ORIGINAL[0]: number >= 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= 0
  ORIGINAL[1]: *dest = 45
  TYPE[1]: CALL
  TOKENIZED[1]: *dest = 45
  ORIGINAL[2]: *dest
  TYPE[2]: CALL
  TOKENIZED[2]: *dest
  ORIGINAL[3]: dest + 1
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 + 1
  ORIGINAL[4]: dest
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476937
FRAGMENT_COUNT: 3
  ORIGINAL[0]: str -> len
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: len
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: str
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771734
FRAGMENT_COUNT: 6
  ORIGINAL[0]: i < strings -> nelts
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2 -> VAR3
  ORIGINAL[1]: (const char **)(strings -> elts)
  TYPE[1]: CALL
  TOKENIZED[1]: ( const char ** ) ( VAR1 -> VAR2 )
  ORIGINAL[2]: strings -> elts
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: elts
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: strings
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: strings
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477214
FRAGMENT_COUNT: 3
  ORIGINAL[0]: original_string -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: data
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: original_string
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477322
FRAGMENT_COUNT: 4
  ORIGINAL[0]: next == ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 == ( ( void * ) 0 )
  ORIGINAL[1]: (char *)next
  TYPE[1]: CALL
  TOKENIZED[1]: ( char * ) VAR1
  ORIGINAL[2]: next
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: next
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771873
FRAGMENT_COUNT: 4
  ORIGINAL[0]: number >= 100000000
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= 100000000
  ORIGINAL[1]: number /= 100000000
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 /= 100000000
  ORIGINAL[2]: number
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: target
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719476964
FRAGMENT_COUNT: 7
  ORIGINAL[0]: new_string -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: *new_string
  TYPE[1]: CALL
  TOKENIZED[1]: *new_string
  ORIGINAL[2]: new_string -> data[0] = '\\0'
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 [ 0 ] = '\\0'
  ORIGINAL[3]: new_string -> len
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: len
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: new_string
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: new_string
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 68719477660
FRAGMENT_COUNT: 3
  ORIGINAL[0]: ++stonesoup_global_variable
  TYPE[0]: CALL
  TOKENIZED[0]: ++stonesoup_global_variable
  ORIGINAL[1]: monophyletism_viers(naara_gippers)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )
  ORIGINAL[2]: naara_gippers
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477391
FRAGMENT_COUNT: 3
  ORIGINAL[0]: svn_cstring_strtoui64(n,str,0,18446744073709551615UL,10)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , 0 , 18446744073709551615UL , 10 )
  ORIGINAL[1]: n
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: str
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477029
FRAGMENT_COUNT: 5
  ORIGINAL[0]: nbytes > str -> len
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > VAR2 -> VAR3
  ORIGINAL[1]: str -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: nbytes
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: nbytes
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477274
FRAGMENT_COUNT: 6
  ORIGINAL[0]: *a = apr_array_make(pool,5,(sizeof(input)))
  TYPE[0]: CALL
  TOKENIZED[0]: *a = FUN1 ( VAR1 , 5 , ( sizeof ( VAR2 ) ) )
  ORIGINAL[1]: svn_cstring_split_append(a,input,sep_chars,chop_whitespace,pool)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 , VAR3 , VAR4 , VAR5 )
  ORIGINAL[2]: a
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: a
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: input
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: a
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477538
FRAGMENT_COUNT: 3
  ORIGINAL[0]: svn_string__similarity((&stringa),(&stringb),buffer,rlcs)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( ( &stringa ) , ( &stringb ) , VAR1 , VAR2 )
  ORIGINAL[1]: buffer
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: rlcs
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771268
FRAGMENT_COUNT: 5
  ORIGINAL[0]: svn_string_ncreate(original_string -> data,original_string -> len,pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , VAR1 -> VAR3 , VAR4 )
  ORIGINAL[1]: original_string -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: original_string
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: pool
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771277
FRAGMENT_COUNT: 4
  ORIGINAL[0]: find_char_backward(str -> data,str -> len,ch)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , VAR1 -> VAR3 , VAR4 )
  ORIGINAL[1]: str -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: str -> len
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: ch
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 47244640333
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771954
FRAGMENT_COUNT: 6
  ORIGINAL[0]: number < 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < 0
  ORIGINAL[1]: buffer[0] = 45
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ 0 ] = 45
  ORIGINAL[2]: buffer[0]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ 0 ]
  ORIGINAL[3]: buffer
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: buffer
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: buffer
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477412
FRAGMENT_COUNT: 5
  ORIGINAL[0]: *__errno_location() == 34 && (val == - 9223372036854775807L - 1 || val == 9223372036854775807L)
  TYPE[0]: CALL
  TOKENIZED[0]: *__errno_location ( ) == 34 && ( VAR1 == - 9223372036854775807L - 1 || VAR1 == 9223372036854775807L )
  ORIGINAL[1]: val < minval
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 < VAR2
  ORIGINAL[2]: val
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: minval
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: minval
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477024
FRAGMENT_COUNT: 6
  ORIGINAL[0]: memcpy((str -> data),value,amt + 1)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( ( VAR1 -> VAR2 ) , VAR3 , VAR4 + 1 )
  ORIGINAL[1]: str -> data
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: str -> len
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: len
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: str
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: str
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771142
FRAGMENT_COUNT: 4
  ORIGINAL[0]: *size = minimum_size
  TYPE[0]: CALL
  TOKENIZED[0]: *size = VAR1
  ORIGINAL[1]: *size
  TYPE[1]: CALL
  TOKENIZED[1]: *size
  ORIGINAL[2]: size
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: minimum_size
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477016
FRAGMENT_COUNT: 4
  ORIGINAL[0]: str -> len
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: len
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: c
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: str
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477421
FRAGMENT_COUNT: 3
  ORIGINAL[0]: svn_cstring_strtoi64(n,str,- 9223372036854775807L - 1,9223372036854775807L,10)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , - 9223372036854775807L - 1 , 9223372036854775807L , 10 )
  ORIGINAL[1]: n
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: str
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771231
FRAGMENT_COUNT: 5
  ORIGINAL[0]: new_string -> len = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = 0
  ORIGINAL[1]: new_string -> len
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: len
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: new_string
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: new_string
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771283
FRAGMENT_COUNT: 2
  ORIGINAL[0]: svn_stringbuf_create_ensure(0,pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( 0 , VAR1 )
  ORIGINAL[1]: pool
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1

CENTER_NODE: 68719477514
FRAGMENT_COUNT: 4
  ORIGINAL[0]: apr_pstrdup(pool,buffer)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[1]: buffer
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: pool
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: buffer
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771239
FRAGMENT_COUNT: 5
  ORIGINAL[0]: data = ((char *)mem) + sizeof(( *new_string))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = ( ( char * ) VAR2 ) + sizeof ( ( *new_string ) )
  ORIGINAL[1]: ((char *)mem) + sizeof(( *new_string))
  TYPE[1]: CALL
  TOKENIZED[1]: ( ( char * ) VAR1 ) + sizeof ( ( *new_string ) )
  ORIGINAL[2]: (char *)mem
  TYPE[2]: CALL
  TOKENIZED[2]: ( char * ) VAR1
  ORIGINAL[3]: sizeof(( *new_string))
  TYPE[3]: CALL
  TOKENIZED[3]: sizeof ( ( *new_string ) )
  ORIGINAL[4]: data
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772094
FRAGMENT_COUNT: 3
  ORIGINAL[0]: ++stonesoup_global_variable
  TYPE[0]: CALL
  TOKENIZED[0]: ++stonesoup_global_variable
  ORIGINAL[1]: <global> stonesoup_global_variable
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: <global> VAR1
  ORIGINAL[2]: palafitte_vascla
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640329
FRAGMENT_COUNT: 0

CENTER_NODE: 68719476922
FRAGMENT_COUNT: 5
  ORIGINAL[0]: *data = apr_pvsprintf(pool,fmt,ap)
  TYPE[0]: CALL
  TOKENIZED[0]: *data = FUN1 ( VAR1 , VAR2 , VAR3 )
  ORIGINAL[1]: apr_pvsprintf(pool,fmt,ap)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 , VAR3 )
  ORIGINAL[2]: data
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: pool
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: data
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477221
FRAGMENT_COUNT: 3
  ORIGINAL[0]: str -> data
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: data
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: str
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771198
FRAGMENT_COUNT: 8
  ORIGINAL[0]: len1 != len2
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != VAR2
  ORIGINAL[1]: memcmp(str1,str2,len1) == 0
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 , VAR3 ) == 0
  ORIGINAL[2]: memcmp(str1,str2,len1)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , VAR2 , VAR3 )
  ORIGINAL[3]: len1
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: len2
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: str1
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: str2
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: len1
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 68719476982
FRAGMENT_COUNT: 3
  ORIGINAL[0]: svn_stringbuf_ncreate(cstring,strlen(cstring),pool)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , FUN2 ( VAR1 ) , VAR2 )
  ORIGINAL[1]: cstring
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: cstring
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771619
FRAGMENT_COUNT: 12
  ORIGINAL[0]: e >= p && 0 != (svn_ctype_table[(unsigned char )( *e)] & 0x0002)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= VAR2 && 0 != ( VAR3 [ ( unsigned char ) ( *e ) ] & 0x0002 )
  ORIGINAL[1]: e >= p
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 >= VAR2
  ORIGINAL[2]: 0 != (svn_ctype_table[(unsigned char )( *e)] & 0x0002)
  TYPE[2]: CALL
  TOKENIZED[2]: 0 != ( VAR1 [ ( unsigned char ) ( *e ) ] & 0x0002 )
  ORIGINAL[3]: svn_ctype_table[(unsigned char )( *e)] & 0x0002
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 [ ( unsigned char ) ( *e ) ] & 0x0002
  ORIGINAL[4]: svn_ctype_table[(unsigned char )( *e)]
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 [ ( unsigned char ) ( *e ) ]
  ORIGINAL[5]: (unsigned char )( *e)
  TYPE[5]: CALL
  TOKENIZED[5]: ( unsigned char ) ( *e )
  ORIGINAL[6]: *e
  TYPE[6]: CALL
  TOKENIZED[6]: *e
  ORIGINAL[7]: chop_whitespace
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: e
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: p
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: svn_ctype_table
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1
  ORIGINAL[11]: e
  TYPE[11]: IDENTIFIER
  TOKENIZED[11]: VAR1

CENTER_NODE: 30064771405
FRAGMENT_COUNT: 3
  ORIGINAL[0]: str -> blocksize > old_len + 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 > VAR3 + 1
  ORIGINAL[1]: dest[old_len + 1] = '\\0'
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ VAR2 + 1 ] = '\\0'
  ORIGINAL[2]: dest[old_len + 1]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ VAR2 + 1 ]

