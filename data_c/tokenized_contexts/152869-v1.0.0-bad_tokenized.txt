# Tokenized code fragments for 152869-v1.0.0-bad
# Total center nodes processed: 26
# Total code fragments found: 165

CENTER_NODE: 68719476821
FRAGMENT_COUNT: 5
  ORIGINAL[0]: c >= 97
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= 97
  ORIGINAL[1]: c
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: c
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: c
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: c
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771414
FRAGMENT_COUNT: 4
  ORIGINAL[0]: *key = (const conversation_key *)v
  TYPE[0]: CALL
  TOKENIZED[0]: *key = ( const VAR1 * ) VAR2
  ORIGINAL[1]: (const conversation_key *)v
  TYPE[1]: CALL
  TOKENIZED[1]: ( const VAR1 * ) VAR2
  ORIGINAL[2]: key
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hash_val
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064772207
FRAGMENT_COUNT: 4
  ORIGINAL[0]: item = g_slist_find_custom(conv -> data_list,((gpointer *)(&temp)),p_compare)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = FUN1 ( VAR2 -> VAR3 , ( ( VAR4 * ) ( &temp ) ) , VAR5 )
  ORIGINAL[1]: g_slist_find_custom(conv -> data_list,((gpointer *)(&temp)),p_compare)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 -> VAR2 , ( ( VAR3 * ) ( &temp ) ) , VAR4 )
  ORIGINAL[2]: item
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: item
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719477554
FRAGMENT_COUNT: 3
  ORIGINAL[0]: conversation -> dissector_handle = handle
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = VAR3
  ORIGINAL[1]: conversation -> dissector_handle
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: handle
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064772300
FRAGMENT_COUNT: 5
  ORIGINAL[0]: stonesoup_oc_i < strlen(stonesoup_buff)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < FUN1 ( VAR2 )
  ORIGINAL[1]: ++stonesoup_oc_i
  TYPE[1]: CALL
  TOKENIZED[1]: ++stonesoup_oc_i
  ORIGINAL[2]: for (;stonesoup_oc_i < strlen(stonesoup_buff);++stonesoup_oc_i)
  TYPE[2]: CONTROL_STRUCTURE
  TOKENIZED[2]: for ( ; VAR1 < FUN1 ( VAR2 ) ; ++stonesoup_oc_i )
  ORIGINAL[3]: stonesoup_oc_i
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: stonesoup_oc_i
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772198
FRAGMENT_COUNT: 5
  ORIGINAL[0]: item != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: p1 = ((conv_proto_data *)(item -> data))
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = ( ( VAR2 * ) ( VAR3 -> VAR4 ) )
  ORIGINAL[2]: (conv_proto_data *)(item -> data)
  TYPE[2]: CALL
  TOKENIZED[2]: ( VAR1 * ) ( VAR2 -> VAR3 )
  ORIGINAL[3]: item -> data
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: p1
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772182
FRAGMENT_COUNT: 5
  ORIGINAL[0]: conv -> data_list = g_slist_insert_sorted(conv -> data_list,((gpointer *)p1),p_compare)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = FUN1 ( VAR1 -> VAR2 , ( ( VAR3 * ) VAR4 ) , VAR5 )
  ORIGINAL[1]: conv -> data_list
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: g_slist_insert_sorted(conv -> data_list,((gpointer *)p1),p_compare)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 -> VAR2 , ( ( VAR3 * ) VAR4 ) , VAR5 )
  ORIGINAL[3]: conv -> data_list
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: (gpointer *)p1
  TYPE[4]: CALL
  TOKENIZED[4]: ( VAR1 * ) VAR2

CENTER_NODE: 68719477077
FRAGMENT_COUNT: 6
  ORIGINAL[0]: *conv = (conversation_t *)value
  TYPE[0]: CALL
  TOKENIZED[0]: *conv = ( VAR1 * ) VAR2
  ORIGINAL[1]: (conversation_t *)value
  TYPE[1]: CALL
  TOKENIZED[1]: ( VAR1 * ) VAR2
  ORIGINAL[2]: conv -> data_list
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: conv
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: conv
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: conv
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064772230
FRAGMENT_COUNT: 5
  ORIGINAL[0]: conversation -> dissector_handle == ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == ( ( void * ) 0 )
  ORIGINAL[1]: ret = call_dissector_only(conversation -> dissector_handle,tvb,pinfo,tree)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = FUN1 ( VAR2 -> VAR3 , VAR4 , VAR5 , VAR6 )
  ORIGINAL[2]: call_dissector_only(conversation -> dissector_handle,tvb,pinfo,tree)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 -> VAR2 , VAR3 , VAR4 , VAR5 )
  ORIGINAL[3]: ret
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: ret
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771583
FRAGMENT_COUNT: 15
  ORIGINAL[0]: &v1 -> addr2
  TYPE[0]: CALL
  TOKENIZED[0]: &v1 -> VAR1
  ORIGINAL[1]: v1 -> addr2
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: (&v1 -> addr2) -> type
  TYPE[2]: CALL
  TOKENIZED[2]: ( &v1 -> VAR1 ) -> VAR2
  ORIGINAL[3]: &v1 -> addr2
  TYPE[3]: CALL
  TOKENIZED[3]: &v1 -> VAR1
  ORIGINAL[4]: v1 -> addr2
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: (&v1 -> addr2) -> len == (&v2 -> addr2) -> len
  TYPE[5]: CALL
  TOKENIZED[5]: ( &v1 -> VAR1 ) -> VAR2 == ( &v2 -> VAR1 ) -> VAR2
  ORIGINAL[6]: (&v1 -> addr2) -> len
  TYPE[6]: CALL
  TOKENIZED[6]: ( &v1 -> VAR1 ) -> VAR2
  ORIGINAL[7]: &v1 -> addr2
  TYPE[7]: CALL
  TOKENIZED[7]: &v1 -> VAR1
  ORIGINAL[8]: v1 -> addr2
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: &v1 -> addr2
  TYPE[9]: CALL
  TOKENIZED[9]: &v1 -> VAR1
  ORIGINAL[10]: v1 -> addr2
  TYPE[10]: CALL
  TOKENIZED[10]: VAR1 -> VAR2
  ORIGINAL[11]: (&v1 -> addr2) -> len
  TYPE[11]: CALL
  TOKENIZED[11]: ( &v1 -> VAR1 ) -> VAR2
  ORIGINAL[12]: &v1 -> addr2
  TYPE[12]: CALL
  TOKENIZED[12]: &v1 -> VAR1
  ORIGINAL[13]: v1 -> addr2
  TYPE[13]: CALL
  TOKENIZED[13]: VAR1 -> VAR2
  ORIGINAL[14]: len
  TYPE[14]: FIELD_IDENTIFIER
  TOKENIZED[14]: VAR1

CENTER_NODE: 68719477098
FRAGMENT_COUNT: 3
  ORIGINAL[0]: conversation_hashtable_exact = g_hash_table_new(conversation_hash_exact,conversation_match_exact)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = FUN1 ( VAR2 , VAR3 )
  ORIGINAL[1]: g_hash_table_new(conversation_hash_exact,conversation_match_exact)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[2]: <global> conversation_hashtable_exact
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: <global> VAR1

CENTER_NODE: 47244640256
FRAGMENT_COUNT: 1
  ORIGINAL[0]: ss_tc_root != NULL
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != VAR2

CENTER_NODE: 68719476804
FRAGMENT_COUNT: 7
  ORIGINAL[0]: stonesoup_tainted_file != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != 0
  ORIGINAL[1]: fseek(stonesoup_tainted_file,0L,2)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , 0L , 2 )
  ORIGINAL[2]: stonesoup_lsize = ftell(stonesoup_tainted_file)
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = FUN1 ( VAR2 )
  ORIGINAL[3]: ftell(stonesoup_tainted_file)
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( VAR1 )
  ORIGINAL[4]: stonesoup_lsize
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: stonesoup_tainted_file
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: stonesoup_lsize
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 68719477136
FRAGMENT_COUNT: 8
  ORIGINAL[0]: conv -> key_ptr
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: conv -> setup_frame
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: ((void *)0) == prev
  TYPE[2]: CALL
  TOKENIZED[2]: ( ( void * ) 0 ) == VAR1
  ORIGINAL[3]: conv -> next = chain_head
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2 = VAR3
  ORIGINAL[4]: conv -> next
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: conv -> last
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: last
  TYPE[6]: FIELD_IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: conv
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 68719477303
FRAGMENT_COUNT: 3
  ORIGINAL[0]: key . port1 = port1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 . VAR2 = VAR2
  ORIGINAL[1]: key . port1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 . VAR2
  ORIGINAL[2]: port1
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771484
FRAGMENT_COUNT: 6
  ORIGINAL[0]: ADD_ADDRESS_TO_HASH_data = ((&key -> addr1) -> data)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = ( ( &key -> VAR2 ) -> VAR3 )
  ORIGINAL[1]: (&key -> addr1) -> data
  TYPE[1]: CALL
  TOKENIZED[1]: ( &key -> VAR1 ) -> VAR2
  ORIGINAL[2]: &key -> addr1
  TYPE[2]: CALL
  TOKENIZED[2]: &key -> VAR1
  ORIGINAL[3]: &key -> addr1
  TYPE[3]: CALL
  TOKENIZED[3]: &key -> VAR1
  ORIGINAL[4]: data
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: ADD_ADDRESS_TO_HASH_data
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771681
FRAGMENT_COUNT: 3
  ORIGINAL[0]: conversation_hashtable_exact != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: (void *)0
  TYPE[1]: CALL
  TOKENIZED[1]: ( void * ) 0
  ORIGINAL[2]: <global> conversation_hashtable_exact
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: <global> VAR1

CENTER_NODE: 68719477518
FRAGMENT_COUNT: 5
  ORIGINAL[0]: *bp = (const conv_proto_data *)b
  TYPE[0]: CALL
  TOKENIZED[0]: *bp = ( const VAR1 * ) VAR2
  ORIGINAL[1]: ap -> proto
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: proto
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: ap
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: ap
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640275
FRAGMENT_COUNT: 1
  ORIGINAL[0]: else
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: else

CENTER_NODE: 30064771794
FRAGMENT_COUNT: 5
  ORIGINAL[0]: conv -> latest_found == conv
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == VAR1
  ORIGINAL[1]: chain_head -> latest_found = conv -> latest_found
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 = VAR3 -> VAR2
  ORIGINAL[2]: chain_head -> latest_found
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: conv -> latest_found
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hashtable
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771612
FRAGMENT_COUNT: 5
  ORIGINAL[0]: affects_acas != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != 0
  ORIGINAL[1]: cancerin_stanhopes(misdiagnosis_postallantoic,bilabiate_unregressive)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 )
  ORIGINAL[2]: misdiagnosis_postallantoic
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: bilabiate_unregressive
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hash_val
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772248
FRAGMENT_COUNT: 11
  ORIGINAL[0]: find_conversation(pinfo -> fd -> num,(&pinfo -> src),(&pinfo -> dst),pinfo -> ptype,pinfo -> srcport,pinfo -> destport,0)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 -> VAR3 , ( &pinfo -> VAR4 ) , ( &pinfo -> VAR5 ) , VAR1 -> VAR6 , VAR1 -> VAR7 , VAR1 -> VAR8 , 0 )
  ORIGINAL[1]: pinfo -> destport
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: pinfo -> destport
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: destport
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: pinfo
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: pinfo
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: pinfo
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: pinfo
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: pinfo
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: pinfo
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: pinfo
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1

CENTER_NODE: 30064771323
FRAGMENT_COUNT: 25
  ORIGINAL[0]: ((&v1 -> addr2) -> type) == AT_NONE
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( &v1 -> VAR1 ) -> VAR2 ) == VAR3
  ORIGINAL[1]: (&v1 -> addr2) -> len == (&v2 -> addr2) -> len && memcmp((&v1 -> addr2) -> data,(&v2 -> addr2) -> data,((&v1 -> addr2) -> len)) == 0
  TYPE[1]: CALL
  TOKENIZED[1]: ( &v1 -> VAR1 ) -> VAR2 == ( &v2 -> VAR1 ) -> VAR2 && FUN1 ( ( &v1 -> VAR1 ) -> VAR3 , ( &v2 -> VAR1 ) -> VAR3 , ( ( &v1 -> VAR1 ) -> VAR2 ) ) == 0
  ORIGINAL[2]: (&v1 -> addr2) -> len == (&v2 -> addr2) -> len
  TYPE[2]: CALL
  TOKENIZED[2]: ( &v1 -> VAR1 ) -> VAR2 == ( &v2 -> VAR1 ) -> VAR2
  ORIGINAL[3]: (&v1 -> addr2) -> len
  TYPE[3]: CALL
  TOKENIZED[3]: ( &v1 -> VAR1 ) -> VAR2
  ORIGINAL[4]: (&v2 -> addr2) -> len
  TYPE[4]: CALL
  TOKENIZED[4]: ( &v2 -> VAR1 ) -> VAR2
  ORIGINAL[5]: memcmp((&v1 -> addr2) -> data,(&v2 -> addr2) -> data,((&v1 -> addr2) -> len)) == 0
  TYPE[5]: CALL
  TOKENIZED[5]: FUN1 ( ( &v1 -> VAR1 ) -> VAR2 , ( &v2 -> VAR1 ) -> VAR2 , ( ( &v1 -> VAR1 ) -> VAR3 ) ) == 0
  ORIGINAL[6]: memcmp((&v1 -> addr2) -> data,(&v2 -> addr2) -> data,((&v1 -> addr2) -> len))
  TYPE[6]: CALL
  TOKENIZED[6]: FUN1 ( ( &v1 -> VAR1 ) -> VAR2 , ( &v2 -> VAR1 ) -> VAR2 , ( ( &v1 -> VAR1 ) -> VAR3 ) )
  ORIGINAL[7]: (&v1 -> addr2) -> data
  TYPE[7]: CALL
  TOKENIZED[7]: ( &v1 -> VAR1 ) -> VAR2
  ORIGINAL[8]: &v1 -> addr2
  TYPE[8]: CALL
  TOKENIZED[8]: &v1 -> VAR1
  ORIGINAL[9]: v1 -> addr2
  TYPE[9]: CALL
  TOKENIZED[9]: VAR1 -> VAR2
  ORIGINAL[10]: (&v2 -> addr2) -> data
  TYPE[10]: CALL
  TOKENIZED[10]: ( &v2 -> VAR1 ) -> VAR2
  ORIGINAL[11]: &v2 -> addr2
  TYPE[11]: CALL
  TOKENIZED[11]: &v2 -> VAR1
  ORIGINAL[12]: v2 -> addr2
  TYPE[12]: CALL
  TOKENIZED[12]: VAR1 -> VAR2
  ORIGINAL[13]: (&v1 -> addr2) -> len
  TYPE[13]: CALL
  TOKENIZED[13]: ( &v1 -> VAR1 ) -> VAR2
  ORIGINAL[14]: &v1 -> addr2
  TYPE[14]: CALL
  TOKENIZED[14]: &v1 -> VAR1
  ORIGINAL[15]: v1 -> addr2
  TYPE[15]: CALL
  TOKENIZED[15]: VAR1 -> VAR2
  ORIGINAL[16]: addr2
  TYPE[16]: FIELD_IDENTIFIER
  TOKENIZED[16]: VAR1
  ORIGINAL[17]: data
  TYPE[17]: FIELD_IDENTIFIER
  TOKENIZED[17]: VAR1
  ORIGINAL[18]: addr2
  TYPE[18]: FIELD_IDENTIFIER
  TOKENIZED[18]: VAR1
  ORIGINAL[19]: data
  TYPE[19]: FIELD_IDENTIFIER
  TOKENIZED[19]: VAR1
  ORIGINAL[20]: addr2
  TYPE[20]: FIELD_IDENTIFIER
  TOKENIZED[20]: VAR1
  ORIGINAL[21]: len
  TYPE[21]: FIELD_IDENTIFIER
  TOKENIZED[21]: VAR1
  ORIGINAL[22]: v1
  TYPE[22]: IDENTIFIER
  TOKENIZED[22]: VAR1
  ORIGINAL[23]: v2
  TYPE[23]: IDENTIFIER
  TOKENIZED[23]: VAR1
  ORIGINAL[24]: v1
  TYPE[24]: IDENTIFIER
  TOKENIZED[24]: VAR1

CENTER_NODE: 30064771658
FRAGMENT_COUNT: 10
  ORIGINAL[0]: (&v2 -> addr1) -> type
  TYPE[0]: CALL
  TOKENIZED[0]: ( &v2 -> VAR1 ) -> VAR2
  ORIGINAL[1]: &v2 -> addr1
  TYPE[1]: CALL
  TOKENIZED[1]: &v2 -> VAR1
  ORIGINAL[2]: v2 -> addr1
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: ((&v1 -> addr1) -> type) == AT_NONE
  TYPE[3]: CALL
  TOKENIZED[3]: ( ( &v1 -> VAR1 ) -> VAR2 ) == VAR3
  ORIGINAL[4]: (&v2 -> addr1) -> len
  TYPE[4]: CALL
  TOKENIZED[4]: ( &v2 -> VAR1 ) -> VAR2
  ORIGINAL[5]: &v2 -> addr1
  TYPE[5]: CALL
  TOKENIZED[5]: &v2 -> VAR1
  ORIGINAL[6]: v2 -> addr1
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: &v2 -> addr1
  TYPE[7]: CALL
  TOKENIZED[7]: &v2 -> VAR1
  ORIGINAL[8]: v2 -> addr1
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: len
  TYPE[9]: FIELD_IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 30064771230
FRAGMENT_COUNT: 4
  ORIGINAL[0]: *key = (const conversation_key *)v
  TYPE[0]: CALL
  TOKENIZED[0]: *key = ( const VAR1 * ) VAR2
  ORIGINAL[1]: (const conversation_key *)v
  TYPE[1]: CALL
  TOKENIZED[1]: ( const VAR1 * ) VAR2
  ORIGINAL[2]: key
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hash_val
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 68719476972
FRAGMENT_COUNT: 11
  ORIGINAL[0]: v1 -> ptype
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: v1 -> port1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: v1 -> port2
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: v1 -> addr1
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: v1 -> addr1
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: (&v1 -> addr1) -> len == (&v2 -> addr1) -> len
  TYPE[5]: CALL
  TOKENIZED[5]: ( &v1 -> VAR1 ) -> VAR2 == ( &v2 -> VAR1 ) -> VAR2
  ORIGINAL[6]: v1 -> addr1
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: v1 -> addr1
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2
  ORIGINAL[8]: addr1
  TYPE[8]: FIELD_IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: v1
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1
  ORIGINAL[10]: v1
  TYPE[10]: IDENTIFIER
  TOKENIZED[10]: VAR1

