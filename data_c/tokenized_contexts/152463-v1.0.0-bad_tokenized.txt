# Tokenized code fragments for 152463-v1.0.0-bad
# Total center nodes processed: 32
# Total code fragments found: 195

CENTER_NODE: 30064771718
FRAGMENT_COUNT: 9
  ORIGINAL[0]: file -> eof && file -> avail_in == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 && VAR1 -> VAR3 == 0
  ORIGINAL[1]: file -> eof
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: file -> avail_in == 0
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 == 0
  ORIGINAL[3]: file -> avail_in
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: eof
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: avail_in
  TYPE[5]: FIELD_IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: file
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: file
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: file
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 30064771302
FRAGMENT_COUNT: 9
  ORIGINAL[0]: !item || item -> out < out_pos
  TYPE[0]: CALL
  TOKENIZED[0]: !item || VAR1 -> VAR2 < VAR3
  ORIGINAL[1]: val -> in = in_pos
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 = VAR3
  ORIGINAL[2]: val -> in
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: in
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: val
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: in_pos
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: val
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: val
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: val
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 68719476936
FRAGMENT_COUNT: 15
  ORIGINAL[0]: state -> raw_pos
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: state -> raw
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: state -> pos
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: state -> out
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: state -> avail_in
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: state -> next
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: state -> have
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2
  ORIGINAL[7]: state -> next_in
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2
  ORIGINAL[8]: state -> avail_in
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2
  ORIGINAL[9]: state -> have
  TYPE[9]: CALL
  TOKENIZED[9]: VAR1 -> VAR2
  ORIGINAL[10]: state -> avail_in
  TYPE[10]: CALL
  TOKENIZED[10]: VAR1 -> VAR2
  ORIGINAL[11]: avail_in
  TYPE[11]: FIELD_IDENTIFIER
  TOKENIZED[11]: VAR1
  ORIGINAL[12]: state
  TYPE[12]: IDENTIFIER
  TOKENIZED[12]: VAR1
  ORIGINAL[13]: state
  TYPE[13]: IDENTIFIER
  TOKENIZED[13]: VAR1
  ORIGINAL[14]: state
  TYPE[14]: IDENTIFIER
  TOKENIZED[14]: VAR1

CENTER_NODE: 68719477175
FRAGMENT_COUNT: 8
  ORIGINAL[0]: file -> have == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == 0
  ORIGINAL[1]: file -> have == 0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 == 0
  ORIGINAL[2]: file -> pos += n
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 += VAR3
  ORIGINAL[3]: left -= n
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -= VAR2
  ORIGINAL[4]: left
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: left
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: n
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: left
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 30064771210
FRAGMENT_COUNT: 5
  ORIGINAL[0]: ret = (read(state -> fd,(buf +  *have),(count -  *have)))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = ( FUN1 ( VAR2 -> VAR3 , ( VAR4 + *have ) , ( VAR5 - *have ) ) )
  ORIGINAL[1]: read(state -> fd,(buf +  *have),(count -  *have))
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 -> VAR2 , ( VAR3 + *have ) , ( VAR4 - *have ) )
  ORIGINAL[2]: *have < count
  TYPE[2]: CALL
  TOKENIZED[2]: *have < VAR1
  ORIGINAL[3]: ret
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: ret
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477086
FRAGMENT_COUNT: 3
  ORIGINAL[0]: stream -> fd
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: fd
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: stream
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719476868
FRAGMENT_COUNT: 9
  ORIGINAL[0]: state -> eof == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == 0
  ORIGINAL[1]: state -> eof
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: raw_read(state,state -> in,state -> size,((unsigned int *)(&state -> avail_in)))
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , VAR1 -> VAR2 , VAR1 -> VAR3 , ( ( unsigned int * ) ( &state -> VAR4 ) ) )
  ORIGINAL[3]: state -> avail_in
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: state -> in
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: state
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: state
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: state
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: state
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 30064771375
FRAGMENT_COUNT: 9
  ORIGINAL[0]: state -> compression = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = 0
  ORIGINAL[1]: state -> compression
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: compression
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: state
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: state
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: state
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: state
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: state
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: state
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 30064771252
FRAGMENT_COUNT: 1
  ORIGINAL[0]: window[32768]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ 32768 ]

CENTER_NODE: 68719476764
FRAGMENT_COUNT: 6
  ORIGINAL[0]: retval == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 == 0
  ORIGINAL[1]: size_filepath = strlen(dirpath) + strlen(\
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = FUN1 ( VAR2 ) + FUN1 ( \
  ORIGINAL[2]: strlen(dirpath) + strlen(\
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 ) + FUN1 ( \
  ORIGINAL[3]: size_filepath
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: dirpath
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: size_filepath
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477085
FRAGMENT_COUNT: 3
  ORIGINAL[0]: stream -> raw_pos
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: raw_pos
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: stream
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477013
FRAGMENT_COUNT: 3
  ORIGINAL[0]: stream -> fast_seek
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: fast_seek
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: stream
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477079
FRAGMENT_COUNT: 3
  ORIGINAL[0]: file_seek(file,delta,1,err)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , 1 , VAR3 )
  ORIGINAL[1]: file
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: delta
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771736
FRAGMENT_COUNT: 6
  ORIGINAL[0]: stream -> err = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = 0
  ORIGINAL[1]: stream -> err
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: err
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: stream
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: stream
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: stream
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477194
FRAGMENT_COUNT: 3
  ORIGINAL[0]: file -> fd
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: fd
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: file
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771762
FRAGMENT_COUNT: 3
  ORIGINAL[0]: g_free(file -> fast_seek_cur)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 )
  ORIGINAL[1]: file -> fast_seek_cur
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: file
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477008
FRAGMENT_COUNT: 6
  ORIGINAL[0]: (fd = open(path,0 | 0,0)) == - 1
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 = FUN1 ( VAR2 , 0 | 0 , 0 ) ) == - 1
  ORIGINAL[1]: ft = file_fdopen(fd)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = FUN1 ( VAR2 )
  ORIGINAL[2]: file_fdopen(fd)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 )
  ORIGINAL[3]: ft
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: fd
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: ft
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771754
FRAGMENT_COUNT: 2
  ORIGINAL[0]: (fd = open(path,0 | 0,0)) == - 1
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 = FUN1 ( VAR2 , 0 | 0 , 0 ) ) == - 1
  ORIGINAL[1]: !0
  TYPE[1]: CALL
  TOKENIZED[1]: !0

CENTER_NODE: 30064771595
FRAGMENT_COUNT: 3
  ORIGINAL[0]: stream -> is_compressed
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: is_compressed
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: stream
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771253
FRAGMENT_COUNT: 1
  ORIGINAL[0]: window[32768]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ 32768 ]

CENTER_NODE: 47244640322
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477119
FRAGMENT_COUNT: 7
  ORIGINAL[0]: file -> have
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: buf = (((char *)buf) + n)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = ( ( ( char * ) VAR1 ) + VAR2 )
  ORIGINAL[2]: got += n
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 += VAR2
  ORIGINAL[3]: got
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: got
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: n
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: got
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 68719476875
FRAGMENT_COUNT: 4
  ORIGINAL[0]: *smallest = ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: *smallest = ( ( void * ) 0 )
  ORIGINAL[1]: file -> fast_seek
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: fast_seek
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: file
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771175
FRAGMENT_COUNT: 10
  ORIGINAL[0]: src[i] != '\\0'
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ VAR2 ] != '\\0'
  ORIGINAL[1]: src[i]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ VAR2 ]
  ORIGINAL[2]: src[i] == ';'
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 [ VAR2 ] == ' ; '
  ORIGINAL[3]: i == 0 || src[i-1] != '\\\\'
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 == 0 || VAR2 [ i-1 ] != '\\\\'
  ORIGINAL[4]: i == 0
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 == 0
  ORIGINAL[5]: src[i-1] != '\\\\'
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 [ i-1 ] != '\\\\'
  ORIGINAL[6]: i++
  TYPE[6]: CALL
  TOKENIZED[6]: i++
  ORIGINAL[7]: src
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: i
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: i
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 30064771578
FRAGMENT_COUNT: 3
  ORIGINAL[0]: stream -> pos + ((stream -> seek?stream -> skip : 0))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 + ( ( VAR1 -> seek?stream -> VAR3 : 0 ) )
  ORIGINAL[1]: stream -> pos
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: stream -> seek?stream -> skip : 0
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> seek?stream -> VAR2 : 0

CENTER_NODE: 30064771155
FRAGMENT_COUNT: 4
  ORIGINAL[0]: b >= 'A'
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= 'A'
  ORIGINAL[1]: b -= '0'
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -= '0'
  ORIGINAL[2]: b
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: dst
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1

CENTER_NODE: 30064771652
FRAGMENT_COUNT: 3
  ORIGINAL[0]: ret < 1?- 1 : buf[0]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < 1?- 1 : VAR2 [ 0 ]
  ORIGINAL[1]: ret < 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 < 1
  ORIGINAL[2]: - 1
  TYPE[2]: CALL
  TOKENIZED[2]: - 1

CENTER_NODE: 30064771725
FRAGMENT_COUNT: 5
  ORIGINAL[0]: fh -> err != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 != 0
  ORIGINAL[1]: *err_info = (fh -> err_info == ((void *)0)?((void *)0) : g_strdup(fh -> err_info))
  TYPE[1]: CALL
  TOKENIZED[1]: *err_info = ( VAR1 -> VAR2 == ( ( void * ) 0 ) ? ( ( void * ) 0 ) : FUN1 ( VAR1 -> VAR2 ) )
  ORIGINAL[2]: *err_info
  TYPE[2]: CALL
  TOKENIZED[2]: *err_info
  ORIGINAL[3]: fh -> err_info == ((void *)0)?((void *)0) : g_strdup(fh -> err_info)
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2 == ( ( void * ) 0 ) ? ( ( void * ) 0 ) : FUN1 ( VAR1 -> VAR2 )
  ORIGINAL[4]: fh
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771562
FRAGMENT_COUNT: 2
  ORIGINAL[0]: gz_head(state) == - 1
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 ) == - 1
  ORIGINAL[1]: - 1
  TYPE[1]: CALL
  TOKENIZED[1]: - 1

CENTER_NODE: 30064771345
FRAGMENT_COUNT: 35
  ORIGINAL[0]: state -> have
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: n = (((gint64 )(state -> have)) > len?((unsigned int )len) : state -> have)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = ( ( ( VAR2 ) ( VAR3 -> VAR4 ) ) > len? ( ( unsigned int ) VAR5 ) : VAR3 -> VAR4 )
  ORIGINAL[2]: ((gint64 )(state -> have)) > len?((unsigned int )len) : state -> have
  TYPE[2]: CALL
  TOKENIZED[2]: ( ( VAR1 ) ( VAR2 -> VAR3 ) ) > len? ( ( unsigned int ) VAR4 ) : VAR2 -> VAR3
  ORIGINAL[3]: ((gint64 )(state -> have)) > len
  TYPE[3]: CALL
  TOKENIZED[3]: ( ( VAR1 ) ( VAR2 -> VAR3 ) ) > VAR4
  ORIGINAL[4]: (gint64 )(state -> have)
  TYPE[4]: CALL
  TOKENIZED[4]: ( VAR1 ) ( VAR2 -> VAR3 )
  ORIGINAL[5]: state -> have
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 -> VAR2
  ORIGINAL[6]: state -> have -= n
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 -> VAR2 -= VAR3
  ORIGINAL[7]: state -> have
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 -> VAR2
  ORIGINAL[8]: state -> next += n
  TYPE[8]: CALL
  TOKENIZED[8]: VAR1 -> VAR2 += VAR3
  ORIGINAL[9]: state -> next
  TYPE[9]: CALL
  TOKENIZED[9]: VAR1 -> VAR2
  ORIGINAL[10]: state -> pos += n
  TYPE[10]: CALL
  TOKENIZED[10]: VAR1 -> VAR2 += VAR3
  ORIGINAL[11]: state -> pos
  TYPE[11]: CALL
  TOKENIZED[11]: VAR1 -> VAR2
  ORIGINAL[12]: len -= n
  TYPE[12]: CALL
  TOKENIZED[12]: VAR1 -= VAR2
  ORIGINAL[13]: state -> err
  TYPE[13]: CALL
  TOKENIZED[13]: VAR1 -> VAR2
  ORIGINAL[14]: have
  TYPE[14]: FIELD_IDENTIFIER
  TOKENIZED[14]: VAR1
  ORIGINAL[15]: have
  TYPE[15]: FIELD_IDENTIFIER
  TOKENIZED[15]: VAR1
  ORIGINAL[16]: have
  TYPE[16]: FIELD_IDENTIFIER
  TOKENIZED[16]: VAR1
  ORIGINAL[17]: next
  TYPE[17]: FIELD_IDENTIFIER
  TOKENIZED[17]: VAR1
  ORIGINAL[18]: pos
  TYPE[18]: FIELD_IDENTIFIER
  TOKENIZED[18]: VAR1
  ORIGINAL[19]: err
  TYPE[19]: FIELD_IDENTIFIER
  TOKENIZED[19]: VAR1
  ORIGINAL[20]: len
  TYPE[20]: IDENTIFIER
  TOKENIZED[20]: VAR1
  ORIGINAL[21]: state
  TYPE[21]: IDENTIFIER
  TOKENIZED[21]: VAR1
  ORIGINAL[22]: n
  TYPE[22]: IDENTIFIER
  TOKENIZED[22]: VAR1
  ORIGINAL[23]: gint64
  TYPE[23]: IDENTIFIER
  TOKENIZED[23]: VAR1
  ORIGINAL[24]: state
  TYPE[24]: IDENTIFIER
  TOKENIZED[24]: VAR1
  ORIGINAL[25]: len
  TYPE[25]: IDENTIFIER
  TOKENIZED[25]: VAR1
  ORIGINAL[26]: state
  TYPE[26]: IDENTIFIER
  TOKENIZED[26]: VAR1
  ORIGINAL[27]: n
  TYPE[27]: IDENTIFIER
  TOKENIZED[27]: VAR1
  ORIGINAL[28]: state
  TYPE[28]: IDENTIFIER
  TOKENIZED[28]: VAR1
  ORIGINAL[29]: n
  TYPE[29]: IDENTIFIER
  TOKENIZED[29]: VAR1
  ORIGINAL[30]: state
  TYPE[30]: IDENTIFIER
  TOKENIZED[30]: VAR1
  ORIGINAL[31]: n
  TYPE[31]: IDENTIFIER
  TOKENIZED[31]: VAR1
  ORIGINAL[32]: len
  TYPE[32]: IDENTIFIER
  TOKENIZED[32]: VAR1
  ORIGINAL[33]: n
  TYPE[33]: IDENTIFIER
  TOKENIZED[33]: VAR1
  ORIGINAL[34]: state
  TYPE[34]: IDENTIFIER
  TOKENIZED[34]: VAR1

CENTER_NODE: 30064771511
FRAGMENT_COUNT: 10
  ORIGINAL[0]: offset < 0 && file -> next
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < 0 && VAR2 -> VAR3
  ORIGINAL[1]: -offset <= had
  TYPE[1]: CALL
  TOKENIZED[1]: -offset <= VAR1
  ORIGINAL[2]: (here = fast_seek_find(file,file -> pos + offset)) && (offset < 0 || offset > 1048576L || here -> compression == 1)
  TYPE[2]: CALL
  TOKENIZED[2]: ( VAR1 = FUN1 ( VAR2 , VAR2 -> VAR3 + VAR4 ) ) && ( VAR4 < 0 || VAR4 > 1048576L || VAR1 -> VAR5 == 1 )
  ORIGINAL[3]: here = fast_seek_find(file,file -> pos + offset)
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 = FUN1 ( VAR2 , VAR2 -> VAR3 + VAR4 )
  ORIGINAL[4]: fast_seek_find(file,file -> pos + offset)
  TYPE[4]: CALL
  TOKENIZED[4]: FUN1 ( VAR1 , VAR1 -> VAR2 + VAR3 )
  ORIGINAL[5]: offset < 0 || offset > 1048576L || here -> compression == 1
  TYPE[5]: CALL
  TOKENIZED[5]: VAR1 < 0 || VAR1 > 1048576L || VAR2 -> VAR3 == 1
  ORIGINAL[6]: offset < 0 || offset > 1048576L
  TYPE[6]: CALL
  TOKENIZED[6]: VAR1 < 0 || VAR1 > 1048576L
  ORIGINAL[7]: offset < 0
  TYPE[7]: CALL
  TOKENIZED[7]: VAR1 < 0
  ORIGINAL[8]: here
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: offset
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 30064771430
FRAGMENT_COUNT: 5
  ORIGINAL[0]: state == ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 == ( ( void * ) 0 )
  ORIGINAL[1]: state -> raw_pos = state -> start
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 = VAR1 -> VAR3
  ORIGINAL[2]: state -> raw_pos
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: state -> start
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: state
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

