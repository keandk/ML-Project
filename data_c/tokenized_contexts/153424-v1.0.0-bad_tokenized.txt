# Tokenized code fragments for 153424-v1.0.0-bad
# Total center nodes processed: 45
# Total code fragments found: 148

CENTER_NODE: 47244640378
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640386
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477504
FRAGMENT_COUNT: 5
  ORIGINAL[0]: limit < num
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: i++
  TYPE[1]: CALL
  TOKENIZED[1]: i++
  ORIGINAL[2]: limit <<= 1
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 <<= 1
  ORIGINAL[3]: limit
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: limit
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771619
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hashp != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: MemoryContextDelete(hashp -> hcxt)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 -> VAR2 )
  ORIGINAL[2]: hashp -> hcxt
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2

CENTER_NODE: 68719477572
FRAGMENT_COUNT: 6
  ORIGINAL[0]: seq_scan_level[i] >= nestDepth
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ VAR2 ] >= VAR3
  ORIGINAL[1]: seq_scan_level[i] = seq_scan_level[num_seq_scans - 1]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 [ VAR2 ] = VAR1 [ VAR3 - 1 ]
  ORIGINAL[2]: num_seq_scans--
  TYPE[2]: CALL
  TOKENIZED[2]: num_seq_scans--
  ORIGINAL[3]: <global> num_seq_scans
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: <global> VAR1
  ORIGINAL[4]: <global> num_seq_scans
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: <global> VAR1
  ORIGINAL[5]: <global> num_seq_scans
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: <global> VAR1

CENTER_NODE: 47244640262
FRAGMENT_COUNT: 2
  ORIGINAL[0]: ev == MG_AUTH
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 == VAR2
  ORIGINAL[1]: else
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: else

CENTER_NODE: 30064771400
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (((intptr_t )_vstart) & sizeof(long ) - 1) == 0 && (_len & sizeof(long ) - 1) == 0 && _val == 0 && _len <= 1024 && 1024 != 0
  TYPE[0]: CALL
  TOKENIZED[0]: ( ( ( VAR1 ) VAR2 ) & sizeof ( long ) - 1 ) == 0 && ( VAR3 & sizeof ( long ) - 1 ) == 0 && VAR4 == 0 && VAR3 <= 1024 && 1024 != 0
  ORIGINAL[1]: *_stop = (long *)(((char *)_start) + _len)
  TYPE[1]: CALL
  TOKENIZED[1]: *_stop = ( long * ) ( ( ( char * ) VAR1 ) + VAR2 )
  ORIGINAL[2]: (long *)(((char *)_start) + _len)
  TYPE[2]: CALL
  TOKENIZED[2]: ( long * ) ( ( ( char * ) VAR1 ) + VAR2 )
  ORIGINAL[3]: ((char *)_start) + _len
  TYPE[3]: CALL
  TOKENIZED[3]: ( ( char * ) VAR1 ) + VAR2
  ORIGINAL[4]: _stop
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640273
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640387
FRAGMENT_COUNT: 1
  ORIGINAL[0]: tas(&hctlv -> mutex)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( &hctlv -> VAR1 )

CENTER_NODE: 68719477294
FRAGMENT_COUNT: 3
  ORIGINAL[0]: status -> hashp
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hashp
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: status
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640373
FRAGMENT_COUNT: 0

CENTER_NODE: 47244640343
FRAGMENT_COUNT: 1
  ORIGINAL[0]: for (;;)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: for ( ; ; )

CENTER_NODE: 68719476969
FRAGMENT_COUNT: 5
  ORIGINAL[0]: elementSize = ((((intptr_t )(sizeof(HASHELEMENT ))) + (8 - 1) & ~((intptr_t )(8 - 1))) + (((intptr_t )entrysize) + (8 - 1) & ~((intptr_t )(8 - 1))))
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 = ( ( ( ( VAR2 ) ( sizeof ( VAR3 ) ) ) + ( 8 - 1 ) & ~ ( ( VAR2 ) ( 8 - 1 ) ) ) + ( ( ( VAR2 ) VAR4 ) + ( 8 - 1 ) & ~ ( ( VAR2 ) ( 8 - 1 ) ) ) )
  ORIGINAL[1]: allocSize = (32 * 4)
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = ( 32 * 4 )
  ORIGINAL[2]: 32 * 4
  TYPE[2]: CALL
  TOKENIZED[2]: 32 * 4
  ORIGINAL[3]: allocSize
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: allocSize
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771926
FRAGMENT_COUNT: 6
  ORIGINAL[0]: !hashp -> frozen
  TYPE[0]: CALL
  TOKENIZED[0]: !hashp -> VAR1
  ORIGINAL[1]: hashp -> frozen
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: frozen
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hashp
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064772462
FRAGMENT_COUNT: 1
  ORIGINAL[0]: *seq_scan_tables[100]
  TYPE[0]: CALL
  TOKENIZED[0]: *seq_scan_tables [ 100 ]

CENTER_NODE: 30064771113
FRAGMENT_COUNT: 4
  ORIGINAL[0]: mg_poll_server(stonesoup_server, 1000) == 0 && stonesoup_exit_flag == 1
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , 1000 ) == 0 && VAR2 == 1
  ORIGINAL[1]: mg_poll_server(stonesoup_server, 1000) == 0
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , 1000 ) == 0
  ORIGINAL[2]: stonesoup_exit_flag == 1
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 == 1
  ORIGINAL[3]: <global> stonesoup_exit_flag
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: <global> VAR1

CENTER_NODE: 68719477303
FRAGMENT_COUNT: 8
  ORIGINAL[0]: hashp -> tabname
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: !hashp -> frozen && has_seq_scans(hashp)
  TYPE[1]: CALL
  TOKENIZED[1]: !hashp -> VAR1 && FUN1 ( VAR2 )
  ORIGINAL[2]: hashp -> frozen
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: hashp -> tabname
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: hashp -> frozen
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: frozen
  TYPE[5]: FIELD_IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: hashp
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: hashp
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 47244640258
FRAGMENT_COUNT: 1
  ORIGINAL[0]: while (1)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: while ( 1 )

CENTER_NODE: 30064771623
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (hashp -> hash)(keyPtr,hashp -> keysize)
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 -> VAR2 ) ( VAR3 , VAR1 -> VAR4 )
  ORIGINAL[1]: hashp -> keysize
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: keysize
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: keyPtr
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: hashp
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772049
FRAGMENT_COUNT: 6
  ORIGINAL[0]: new_segnum >= hctl -> dsize
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= VAR2 -> VAR3
  ORIGINAL[1]: !dir_realloc(hashp)
  TYPE[1]: CALL
  TOKENIZED[1]: !dir_realloc ( VAR1 )
  ORIGINAL[2]: !(hashp -> dir[new_segnum] = seg_alloc(hashp))
  TYPE[2]: CALL
  TOKENIZED[2]: ! ( VAR1 -> VAR2 [ VAR3 ] = FUN1 ( VAR1 ) )
  ORIGINAL[3]: hashp -> dir[new_segnum] = seg_alloc(hashp)
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2 [ VAR3 ] = FUN1 ( VAR1 )
  ORIGINAL[4]: hashp -> dir[new_segnum]
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2 [ VAR3 ]
  ORIGINAL[5]: seg_alloc(hashp)
  TYPE[5]: CALL
  TOKENIZED[5]: FUN1 ( VAR1 )

CENTER_NODE: 30064771611
FRAGMENT_COUNT: 3
  ORIGINAL[0]: sizeof(HASHHDR ) + (info -> dsize) * sizeof(HASHSEGMENT )
  TYPE[0]: CALL
  TOKENIZED[0]: sizeof ( VAR1 ) + ( VAR2 -> VAR3 ) * sizeof ( VAR4 )
  ORIGINAL[1]: sizeof(HASHHDR )
  TYPE[1]: CALL
  TOKENIZED[1]: sizeof ( VAR1 )
  ORIGINAL[2]: (info -> dsize) * sizeof(HASHSEGMENT )
  TYPE[2]: CALL
  TOKENIZED[2]: ( VAR1 -> VAR2 ) * sizeof ( VAR3 )

CENTER_NODE: 68719477188
FRAGMENT_COUNT: 9
  ORIGINAL[0]: currBucket -> hashvalue
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: currBucket -> link
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: currBucket != ((void *)0)
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[3]: currBucket = get_hash_entry(hashp)
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 = FUN1 ( VAR2 )
  ORIGINAL[4]: currBucket == ((void *)0)
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 == ( ( void * ) 0 )
  ORIGINAL[5]: (void *)0
  TYPE[5]: CALL
  TOKENIZED[5]: ( void * ) 0
  ORIGINAL[6]: currBucket
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: currBucket
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: currBucket
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 30064772302
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hashp -> isshared
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: elog_finish(22,\
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( 22 , \
  ORIGINAL[2]: hashp -> tabname
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2

CENTER_NODE: 30064772316
FRAGMENT_COUNT: 2
  ORIGINAL[0]: 1L << my_log2(num)
  TYPE[0]: CALL
  TOKENIZED[0]: 1L << FUN1 ( VAR1 )
  ORIGINAL[1]: my_log2(num)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 )

CENTER_NODE: 47244640377
FRAGMENT_COUNT: 1
  ORIGINAL[0]: !segp
  TYPE[0]: CALL
  TOKENIZED[0]: !segp

CENTER_NODE: 30064771607
FRAGMENT_COUNT: 3
  ORIGINAL[0]: (nBuckets - 1) / 256
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 - 1 ) / 256
  ORIGINAL[1]: nBuckets - 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 - 1
  ORIGINAL[2]: nBuckets
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640268
FRAGMENT_COUNT: 1
  ORIGINAL[0]: c >= 97 && c <= 122
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 >= 97 && VAR1 <= 122

CENTER_NODE: 47244640349
FRAGMENT_COUNT: 1
  ORIGINAL[0]: hctlv -> num_partitions != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 != 0

CENTER_NODE: 68719477230
FRAGMENT_COUNT: 3
  ORIGINAL[0]: hashp -> hctl
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: hctl
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: hashp
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640345
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771970
FRAGMENT_COUNT: 5
  ORIGINAL[0]: curBucket > max_bucket
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > VAR2
  ORIGINAL[1]: segp = hashp -> dir[segment_num]
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = VAR2 -> VAR3 [ VAR4 ]
  ORIGINAL[2]: hashp -> dir[segment_num]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 [ VAR3 ]
  ORIGINAL[3]: segp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: curElem
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640265
FRAGMENT_COUNT: 1
  ORIGINAL[0]: while (1)
  TYPE[0]: CONTROL_STRUCTURE
  TOKENIZED[0]: while ( 1 )

CENTER_NODE: 47244640303
FRAGMENT_COUNT: 0

CENTER_NODE: 30064772394
FRAGMENT_COUNT: 5
  ORIGINAL[0]: glassfish_hanoi != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != 0
  ORIGINAL[1]: tellingly_genuinely = &aftertask_laxifolious
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = &aftertask_laxifolious
  ORIGINAL[2]: &aftertask_laxifolious
  TYPE[2]: CALL
  TOKENIZED[2]: &aftertask_laxifolious
  ORIGINAL[3]: tellingly_genuinely
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: interparietale_darwin
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771074
FRAGMENT_COUNT: 5
  ORIGINAL[0]: mg_vprintf_data((struct mg_connection*) stonesoup_printf_context, format, argptr)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( ( struct mg_connection* ) VAR1 , VAR2 , VAR3 )
  ORIGINAL[1]: (struct mg_connection*) stonesoup_printf_context
  TYPE[1]: CALL
  TOKENIZED[1]: ( struct mg_connection* ) VAR1
  ORIGINAL[2]: <global> stonesoup_printf_context
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: <global> VAR1
  ORIGINAL[3]: format
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: argptr
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064772436
FRAGMENT_COUNT: 7
  ORIGINAL[0]: strlen(christianite_stabilizator) < 20
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 ) < 20
  ORIGINAL[1]: tracepoint(stonesoup_trace, variable_signed_integral, \
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR2 , \
  ORIGINAL[2]: &stonesoup_opt_var
  TYPE[2]: CALL
  TOKENIZED[2]: &stonesoup_opt_var
  ORIGINAL[3]: stonesoup_trace
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: variable_signed_integral
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: stonesoup_opt_var
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: stonesoup_oc_i
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 47244640317
FRAGMENT_COUNT: 3
  ORIGINAL[0]: nDirEntries < nSegments
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < VAR2
  ORIGINAL[1]: nDirEntries <<= 1
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 <<= 1
  ORIGINAL[2]: while (nDirEntries < nSegments)
  TYPE[2]: CONTROL_STRUCTURE
  TOKENIZED[2]: while ( VAR1 < VAR2 )

CENTER_NODE: 68719476786
FRAGMENT_COUNT: 3
  ORIGINAL[0]: keysize - 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 - 1
  ORIGINAL[1]: key2
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: keysize
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477400
FRAGMENT_COUNT: 4
  ORIGINAL[0]: p != ((void *)0)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != ( ( void * ) 0 )
  ORIGINAL[1]: (intptr_t )_vstart
  TYPE[1]: CALL
  TOKENIZED[1]: ( VAR1 ) VAR2
  ORIGINAL[2]: sizeof(long )
  TYPE[2]: CALL
  TOKENIZED[2]: sizeof ( long )
  ORIGINAL[3]: long
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: long

CENTER_NODE: 30064771637
FRAGMENT_COUNT: 6
  ORIGINAL[0]: bucket > hctl -> max_bucket
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > VAR2 -> VAR3
  ORIGINAL[1]: bucket & hctl -> low_mask
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 & VAR2 -> VAR3
  ORIGINAL[2]: hctl -> low_mask
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: low_mask
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: bucket
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: hctl
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 68719477012
FRAGMENT_COUNT: 8
  ORIGINAL[0]: !hashp -> dir
  TYPE[0]: CALL
  TOKENIZED[0]: !hashp -> VAR1
  ORIGINAL[1]: !hashp -> dir
  TYPE[1]: CALL
  TOKENIZED[1]: !hashp -> VAR1
  ORIGINAL[2]: hctl -> nsegs < nsegs
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 < VAR2
  ORIGINAL[3]: hctl -> nsegs
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: *segp == ((void *)0)
  TYPE[4]: CALL
  TOKENIZED[4]: *segp == ( ( void * ) 0 )
  ORIGINAL[5]: nsegs
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: nsegs
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: nsegs
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1

CENTER_NODE: 30064772319
FRAGMENT_COUNT: 3
  ORIGINAL[0]: num > (2147483647 / 2)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 > ( 2147483647 / 2 )
  ORIGINAL[1]: 2147483647 / 2
  TYPE[1]: CALL
  TOKENIZED[1]: 2147483647 / 2
  ORIGINAL[2]: num
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640334
FRAGMENT_COUNT: 0

CENTER_NODE: 68719477090
FRAGMENT_COUNT: 5
  ORIGINAL[0]: hash_search_with_hash_value(hashp,keyPtr,((hashp -> hash)(keyPtr,hashp -> keysize)),action,foundPtr)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , ( ( VAR1 -> VAR3 ) ( VAR2 , VAR1 -> VAR4 ) ) , VAR5 , VAR6 )
  ORIGINAL[1]: hashp
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: keyPtr
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: hashp
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: keyPtr
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719476921
FRAGMENT_COUNT: 5
  ORIGINAL[0]: flags & 0x040 || nelem < (hctl -> nelem_alloc)
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 & 0x040 || VAR2 < ( VAR3 -> VAR4 )
  ORIGINAL[1]: (int )nelem
  TYPE[1]: CALL
  TOKENIZED[1]: ( int ) VAR1
  ORIGINAL[2]: nelem
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: nelem
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: nelem
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

