# Tokenized code fragments for 152264-v1.0.0-bad
# Total center nodes processed: 31
# Total code fragments found: 144

CENTER_NODE: 68719477282
FRAGMENT_COUNT: 3
  ORIGINAL[0]: stream -> err
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: err
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: stream
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 68719477290
FRAGMENT_COUNT: 5
  ORIGINAL[0]: (fd = open(path,0 | 0,0)) == - 1
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 = FUN1 ( VAR2 , 0 | 0 , 0 ) ) == - 1
  ORIGINAL[1]: file -> fd = fd
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 = VAR2
  ORIGINAL[2]: file -> fd
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: fd
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: fd
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771392
FRAGMENT_COUNT: 5
  ORIGINAL[0]: dehumidifies_uranometrical != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != 0
  ORIGINAL[1]: hypothermy_neoparaffin = displeases_turnup
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = VAR2
  ORIGINAL[2]: hypothermy_neoparaffin
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: displeases_turnup
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: cuerpo_moehringia
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771674
FRAGMENT_COUNT: 4
  ORIGINAL[0]: file -> seek
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: gz_skip(file,file -> skip) == - 1
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR1 -> VAR2 ) == - 1
  ORIGINAL[2]: gz_skip(file,file -> skip)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , VAR1 -> VAR2 )
  ORIGINAL[3]: - 1
  TYPE[3]: CALL
  TOKENIZED[3]: - 1

CENTER_NODE: 30064771796
FRAGMENT_COUNT: 5
  ORIGINAL[0]: fh -> err != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 != 0
  ORIGINAL[1]: *err_info = (fh -> err_info == ((void *)0)?((void *)0) : g_strdup(fh -> err_info))
  TYPE[1]: CALL
  TOKENIZED[1]: *err_info = ( VAR1 -> VAR2 == ( ( void * ) 0 ) ? ( ( void * ) 0 ) : FUN1 ( VAR1 -> VAR2 ) )
  ORIGINAL[2]: *err_info
  TYPE[2]: CALL
  TOKENIZED[2]: *err_info
  ORIGINAL[3]: fh -> err_info == ((void *)0)?((void *)0) : g_strdup(fh -> err_info)
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2 == ( ( void * ) 0 ) ? ( ( void * ) 0 ) : FUN1 ( VAR1 -> VAR2 )
  ORIGINAL[4]: fh
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771666
FRAGMENT_COUNT: 3
  ORIGINAL[0]: stream -> is_compressed
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: is_compressed
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: stream
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771816
FRAGMENT_COUNT: 5
  ORIGINAL[0]: file -> fd = - 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = - 1
  ORIGINAL[1]: file -> fd
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: - 1
  TYPE[2]: CALL
  TOKENIZED[2]: - 1
  ORIGINAL[3]: fd
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: file
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771653
FRAGMENT_COUNT: 5
  ORIGINAL[0]: stream -> seek?stream -> skip : 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> seek?stream -> VAR2 : 0
  ORIGINAL[1]: stream -> seek
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: stream -> skip
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: skip
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: stream
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771116
FRAGMENT_COUNT: 5
  ORIGINAL[0]: filepath != NULL
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 != VAR2
  ORIGINAL[1]: stonesoup_printf_context = fopen(filepath, \
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 = FUN1 ( VAR2 , \
  ORIGINAL[2]: fopen(filepath, \
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 , \
  ORIGINAL[3]: <global> stonesoup_printf_context
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: <global> VAR1
  ORIGINAL[4]: filepath
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771225
FRAGMENT_COUNT: 1
  ORIGINAL[0]: window[32768]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ 32768 ]

CENTER_NODE: 68719477250
FRAGMENT_COUNT: 7
  ORIGINAL[0]: file -> have == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == 0
  ORIGINAL[1]: file -> have == 0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 == 0
  ORIGINAL[2]: n = (file -> have > left?left : file -> have)
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 = ( VAR2 -> VAR3 > left?left : VAR2 -> VAR3 )
  ORIGINAL[3]: eol = ((unsigned char *)(memchr((file -> next),10,n)))
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 = ( ( unsigned char * ) ( FUN1 ( ( VAR2 -> VAR3 ) , 10 , VAR4 ) ) )
  ORIGINAL[4]: (unsigned char *)(memchr((file -> next),10,n))
  TYPE[4]: CALL
  TOKENIZED[4]: ( unsigned char * ) ( FUN1 ( ( VAR1 -> VAR2 ) , 10 , VAR3 ) )
  ORIGINAL[5]: eol
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: eol
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 30064771603
FRAGMENT_COUNT: 5
  ORIGINAL[0]: offset < 0 && file -> next
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 < 0 && VAR2 -> VAR3
  ORIGINAL[1]: -offset <= had
  TYPE[1]: CALL
  TOKENIZED[1]: -offset <= VAR1
  ORIGINAL[2]: lseek(file -> fd,off,0) == (- 1)
  TYPE[2]: CALL
  TOKENIZED[2]: FUN1 ( VAR1 -> VAR2 , VAR3 , 0 ) == ( - 1 )
  ORIGINAL[3]: lseek(file -> fd,off,0)
  TYPE[3]: CALL
  TOKENIZED[3]: FUN1 ( VAR1 -> VAR2 , VAR3 , 0 )
  ORIGINAL[4]: - 1
  TYPE[4]: CALL
  TOKENIZED[4]: - 1

CENTER_NODE: 47244640316
FRAGMENT_COUNT: 1
  ORIGINAL[0]: (fd = open(path,0 | 0,0)) == - 1
  TYPE[0]: CALL
  TOKENIZED[0]: ( VAR1 = FUN1 ( VAR2 , 0 | 0 , 0 ) ) == - 1

CENTER_NODE: 30064771830
FRAGMENT_COUNT: 10
  ORIGINAL[0]: file -> size
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: g_free((file -> out))
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( ( VAR1 -> VAR2 ) )
  ORIGINAL[2]: file -> out
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: out
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: file
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: file
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: file
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: file
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: file
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: file
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

CENTER_NODE: 30064771793
FRAGMENT_COUNT: 5
  ORIGINAL[0]: file -> eof && file -> avail_in == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 && VAR1 -> VAR3 == 0
  ORIGINAL[1]: file -> have == 0
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2 == 0
  ORIGINAL[2]: file -> have
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: have
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: file
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 30064771545
FRAGMENT_COUNT: 5
  ORIGINAL[0]: stream -> fast_seek = seek
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = VAR3
  ORIGINAL[1]: stream -> fast_seek
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: fast_seek
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: stream
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: seek
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 47244640319
FRAGMENT_COUNT: 0

CENTER_NODE: 30064771259
FRAGMENT_COUNT: 5
  ORIGINAL[0]: file -> fast_seek -> len != 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 -> VAR3 != 0
  ORIGINAL[1]: (struct fast_seek_point *)file -> fast_seek -> pdata[file -> fast_seek -> len - 1]
  TYPE[1]: CALL
  TOKENIZED[1]: ( struct VAR1 * ) VAR2 -> VAR3 -> VAR4 [ VAR2 -> VAR3 -> VAR5 - 1 ]
  ORIGINAL[2]: file -> fast_seek -> pdata[file -> fast_seek -> len - 1]
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 -> VAR3 [ VAR1 -> VAR2 -> VAR4 - 1 ]
  ORIGINAL[3]: file -> fast_seek -> pdata
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[4]: file -> fast_seek -> len - 1
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2 -> VAR3 - 1

CENTER_NODE: 68719476844
FRAGMENT_COUNT: 5
  ORIGINAL[0]: ret <= 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 <= 0
  ORIGINAL[1]: *have < count
  TYPE[1]: CALL
  TOKENIZED[1]: *have < VAR1
  ORIGINAL[2]: *have
  TYPE[2]: CALL
  TOKENIZED[2]: *have
  ORIGINAL[3]: count
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: count
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477176
FRAGMENT_COUNT: 3
  ORIGINAL[0]: stream -> raw_pos
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: raw_pos
  TYPE[1]: FIELD_IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: stream
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 47244640295
FRAGMENT_COUNT: 2
  ORIGINAL[0]: state -> eof && state -> avail_in == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 && VAR1 -> VAR3 == 0
  ORIGINAL[1]: else
  TYPE[1]: CONTROL_STRUCTURE
  TOKENIZED[1]: else

CENTER_NODE: 30064771224
FRAGMENT_COUNT: 1
  ORIGINAL[0]: window[32768]
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 [ 32768 ]

CENTER_NODE: 30064771234
FRAGMENT_COUNT: 7
  ORIGINAL[0]: !file -> fast_seek
  TYPE[0]: CALL
  TOKENIZED[0]: !file -> VAR1
  ORIGINAL[1]: file -> fast_seek
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: file -> fast_seek -> len
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 -> VAR3
  ORIGINAL[3]: file -> fast_seek
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2
  ORIGINAL[4]: fast_seek
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: len
  TYPE[5]: FIELD_IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: file
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 30064771291
FRAGMENT_COUNT: 6
  ORIGINAL[0]: state -> fast_seek
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: fast_seek_header(state,state -> raw_pos - (state -> avail_in) - (state -> have),state -> pos,1)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR1 -> VAR2 - ( VAR1 -> VAR3 ) - ( VAR1 -> VAR4 ) , VAR1 -> VAR5 , 1 )
  ORIGINAL[2]: state -> raw_pos - (state -> avail_in) - (state -> have)
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2 - ( VAR1 -> VAR3 ) - ( VAR1 -> VAR4 )
  ORIGINAL[3]: state -> raw_pos - (state -> avail_in)
  TYPE[3]: CALL
  TOKENIZED[3]: VAR1 -> VAR2 - ( VAR1 -> VAR3 )
  ORIGINAL[4]: state -> have
  TYPE[4]: CALL
  TOKENIZED[4]: VAR1 -> VAR2
  ORIGINAL[5]: state
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1

CENTER_NODE: 30064771640
FRAGMENT_COUNT: 9
  ORIGINAL[0]: state -> compression == 1
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == 1
  ORIGINAL[1]: raw_read(state,state -> out,state -> size,&state -> have)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR1 -> VAR2 , VAR1 -> VAR3 , &state -> VAR4 )
  ORIGINAL[2]: state -> size
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: size
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: state
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: state
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: state
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: state
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: state
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1

CENTER_NODE: 30064771161
FRAGMENT_COUNT: 5
  ORIGINAL[0]: *stonesoup_tainted_buff != 0
  TYPE[0]: CALL
  TOKENIZED[0]: *stonesoup_tainted_buff != 0
  ORIGINAL[1]: (*stonesoup_tainted_buff)[stonesoup_lsize] = '\\0'
  TYPE[1]: CALL
  TOKENIZED[1]: ( *stonesoup_tainted_buff ) [ VAR1 ] = '\\0'
  ORIGINAL[2]: (*stonesoup_tainted_buff)[stonesoup_lsize]
  TYPE[2]: CALL
  TOKENIZED[2]: ( *stonesoup_tainted_buff ) [ VAR1 ]
  ORIGINAL[3]: *stonesoup_tainted_buff
  TYPE[3]: CALL
  TOKENIZED[3]: *stonesoup_tainted_buff
  ORIGINAL[4]: stonesoup_lsize
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1

CENTER_NODE: 68719477171
FRAGMENT_COUNT: 3
  ORIGINAL[0]: file_seek(file,delta,1,err)
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 , VAR2 , 1 , VAR3 )
  ORIGINAL[1]: file
  TYPE[1]: IDENTIFIER
  TOKENIZED[1]: VAR1
  ORIGINAL[2]: delta
  TYPE[2]: IDENTIFIER
  TOKENIZED[2]: VAR1

CENTER_NODE: 30064771709
FRAGMENT_COUNT: 7
  ORIGINAL[0]: file -> err
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2
  ORIGINAL[1]: - 1
  TYPE[1]: CALL
  TOKENIZED[1]: - 1
  ORIGINAL[2]: file -> have
  TYPE[2]: CALL
  TOKENIZED[2]: VAR1 -> VAR2
  ORIGINAL[3]: err
  TYPE[3]: FIELD_IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: have
  TYPE[4]: FIELD_IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: file
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: file
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1

CENTER_NODE: 30064771658
FRAGMENT_COUNT: 3
  ORIGINAL[0]: fstat(stream -> fd,statb) == - 1
  TYPE[0]: CALL
  TOKENIZED[0]: FUN1 ( VAR1 -> VAR2 , VAR3 ) == - 1
  ORIGINAL[1]: fstat(stream -> fd,statb)
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 -> VAR2 , VAR3 )
  ORIGINAL[2]: - 1
  TYPE[2]: CALL
  TOKENIZED[2]: - 1

CENTER_NODE: 30064771216
FRAGMENT_COUNT: 4
  ORIGINAL[0]: state -> eof == 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 == 0
  ORIGINAL[1]: raw_read(state,state -> in,state -> size,((unsigned int *)(&state -> avail_in)))
  TYPE[1]: CALL
  TOKENIZED[1]: FUN1 ( VAR1 , VAR1 -> VAR2 , VAR1 -> VAR3 , ( ( unsigned int * ) ( &state -> VAR4 ) ) )
  ORIGINAL[2]: (unsigned int *)(&state -> avail_in)
  TYPE[2]: CALL
  TOKENIZED[2]: ( unsigned int * ) ( &state -> VAR1 )
  ORIGINAL[3]: &state -> avail_in
  TYPE[3]: CALL
  TOKENIZED[3]: &state -> VAR1

CENTER_NODE: 30064771345
FRAGMENT_COUNT: 10
  ORIGINAL[0]: state -> eof = 0
  TYPE[0]: CALL
  TOKENIZED[0]: VAR1 -> VAR2 = 0
  ORIGINAL[1]: state -> eof
  TYPE[1]: CALL
  TOKENIZED[1]: VAR1 -> VAR2
  ORIGINAL[2]: eof
  TYPE[2]: FIELD_IDENTIFIER
  TOKENIZED[2]: VAR1
  ORIGINAL[3]: state
  TYPE[3]: IDENTIFIER
  TOKENIZED[3]: VAR1
  ORIGINAL[4]: state
  TYPE[4]: IDENTIFIER
  TOKENIZED[4]: VAR1
  ORIGINAL[5]: state
  TYPE[5]: IDENTIFIER
  TOKENIZED[5]: VAR1
  ORIGINAL[6]: state
  TYPE[6]: IDENTIFIER
  TOKENIZED[6]: VAR1
  ORIGINAL[7]: state
  TYPE[7]: IDENTIFIER
  TOKENIZED[7]: VAR1
  ORIGINAL[8]: state
  TYPE[8]: IDENTIFIER
  TOKENIZED[8]: VAR1
  ORIGINAL[9]: state
  TYPE[9]: IDENTIFIER
  TOKENIZED[9]: VAR1

